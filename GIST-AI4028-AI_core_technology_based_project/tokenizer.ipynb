{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86635b30-0495-4980-b47c-5c79fdc23fce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a43bc1-f694-45de-94ee-7ab994e1fb71",
   "metadata": {},
   "source": [
    "source : https://github.com/naver/nlp-challenge/blob/master/missions/ner/data/train/train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f367110a-a2de-470b-b82e-5456bdae768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "239765f3-7ce1-4ee4-bff4-5859c2d6e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/train_data.txt', sep='\\t',  header=None)\n",
    "# data.columns = ['index' , 'content', 'tag']\n",
    "# data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe3ad94e-0cf2-4a88-b9d4-b8eb993fb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/indexingFile.csv', sep=',',  header=0)\n",
    "# data.columns = ['number', 'index' , 'content', 'tag']\n",
    "# data = data.drop(['number'], axis=1)\n",
    "# data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257cde25-217b-4654-b082-277973f7e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/indexingFile_527.csv', sep=',',  header=0)\n",
    "data.columns = ['number', 'index' , 'content', 'tag']\n",
    "data = data.drop(['number'], axis=1)\n",
    "data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4558a230-bea7-405a-8ffd-f0f18f84c5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>활동</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>잘할</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>같은</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>같은</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>스타는?.</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>것</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>갤러</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>리</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>리</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index content  tag  word_id\n",
       "0       1      강사    1        0\n",
       "1       2       로    0        1\n",
       "2       3      활동    0        2\n",
       "3       4      잘할    0        3\n",
       "4       5      같은    0        4\n",
       "5       1      일타    0        5\n",
       "6       2      강사    1        6\n",
       "7       3       로    0        7\n",
       "8       4      같은    0        8\n",
       "9       1      일타    0        9\n",
       "10      2      강사    1       10\n",
       "11      3      하면    0       11\n",
       "12      4   스타는?.    0       12\n",
       "13      1      일타    0       13\n",
       "14      2      강사    1       14\n",
       "15      3       로    0       15\n",
       "16      4      하면    0       16\n",
       "17      5       것    0       17\n",
       "18      1      커피    1       18\n",
       "19      1      커피    1       19\n",
       "20      2      갤러    0       20\n",
       "21      1      커피    1       21\n",
       "22      2       리    0       22\n",
       "23      1      커피    1       23\n",
       "24      2       리    0       24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169640f-9ec0-4e33-9ad6-a350a76fa19f",
   "metadata": {},
   "source": [
    "To split by a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e055717-ff75-4ad9-a9d7-9a499e37291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split = []\n",
    "tag_split = []\n",
    "word_id_split = []\n",
    "\n",
    "content_spliter = []\n",
    "tag_spliter = []\n",
    "word_spliter = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    content_spliter.append(data['content'].iloc[i])\n",
    "    tag_spliter.append(data['tag'].iloc[i])\n",
    "    word_spliter.append(data['word_id'].iloc[i])\n",
    "    if (i == len(data)-1) : # the last word in a data\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "    elif (data['index'].iloc[i+1] == 1) : # the last word in a sentence\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "        \n",
    "# sanity check\n",
    "len(content_split) == len(tag_split) == len(word_id_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce036f3-4143-4762-b38c-cf7d010c2f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb2ca207-01ea-49da-85bc-94cf99dc8c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c4f3a0-d96f-4e6e-b880-6f7dfee51552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7db0f-51e2-45c4-a0c8-42a82b740e6e",
   "metadata": {},
   "source": [
    "## vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e43ae-1916-469e-b257-ff0eea69bfea",
   "metadata": {},
   "source": [
    "konlpy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58d3e665-c23c-4894-8b02-7f293f4b734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 : ['자연어', '처리', '연구', '를', '기반', '으로', '인공', '지능', '모델', '을', '구성', '하고', '있습니다', '.']\n",
      "품사 태깅 : [('자연어', 'Noun'), ('처리', 'Noun'), ('연구', 'Noun'), ('를', 'Josa'), ('기반', 'Noun'), ('으로', 'Josa'), ('인공', 'Noun'), ('지능', 'Noun'), ('모델', 'Noun'), ('을', 'Josa'), ('구성', 'Noun'), ('하고', 'Josa'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n",
      "명사 추출 : ['자연어', '처리', '연구', '기반', '인공', '지능', '모델', '구성']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print('형태소 분석 :',okt.morphs(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))\n",
    "print('품사 태깅 :',okt.pos(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))\n",
    "print('명사 추출 :',okt.nouns(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d7244-104d-4b7a-980e-5c0be2225d03",
   "metadata": {},
   "source": [
    "To create the vocabulary corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfdb82f5-43b8-43d7-ba8e-d53bb646385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75559\n",
      "['세트', '해하', '안슬것', '별식', '좆', '급', '중', '놈', '이기', '개비']\n"
     ]
    }
   ],
   "source": [
    "# to select unique value in content_split\n",
    "unique_content_list = list(set([x for sublist in content_split for x in sublist]))\n",
    "\n",
    "# to select only string\n",
    "filtered = filter(lambda voca: isinstance(voca, str), unique_content_list)\n",
    "unique_filtered_content_list = list(filtered)\n",
    "\n",
    "# concatenation for tokenizer\n",
    "unique_filtered_content_str = ' '.join(unique_filtered_content_list)\n",
    "tokenizer_voca = okt.nouns(unique_filtered_content_str)\n",
    "print(len(tokenizer_voca))\n",
    "print(tokenizer_voca[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b001a38a-18b1-46c3-be83-d94c8a41b4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75559"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(okt.nouns(unique_filtered_content_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9e285e1-9ae6-436a-a745-106381ed2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_voca = okt.nouns(unique_filtered_content_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90a84c02-9dfe-438b-9fb3-fb2d9e408eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['세트', '해하', '안슬것', '별식', '좆', '급', '중', '놈', '이기', '개비']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_voca[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ddc1f31-d470-4386-83ab-9e775e6359f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21062\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "voca2idex = {gene: idx+5 for idx, gene in enumerate(tokenizer_voca)}\n",
    "voca2idex['<unk>']= 0 # unknown\n",
    "voca2idex['<pad>']= 1 # padding\n",
    "voca2idex['<bos>']= 2 # begining of sentence\n",
    "voca2idex['<eos>']= 3 # end of sentence\n",
    "voca2idex['<mask>']= 4 # masking\n",
    "\n",
    "# configuration parameters\n",
    "VOCA_SIZE = len(voca2idex)\n",
    "print(VOCA_SIZE)\n",
    "\n",
    "# index2voca\n",
    "idex2voca = {v:k for k,v in voca2idex.items()}\n",
    "#print(list(gene2idex.keys())[list(gene2idex.values()).index(16)]) \n",
    "print(f'{idex2voca[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ef923-ca47-49a4-a6f5-1ecfc6f03484",
   "metadata": {},
   "source": [
    "voca to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cf067db-7a30-4a93-99ad-1de0e61f0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60453, 0]\n"
     ]
    }
   ],
   "source": [
    "# seq_to_index function\n",
    "def sequence_to_index(string_list, voca2idex):\n",
    "    '''\n",
    "    INPUT\n",
    "        string_list : string list\n",
    "    OUTPUT\n",
    "        integer list\n",
    "    '''\n",
    "    converted_index = [voca2idex[word] if word in voca2idex.keys() else voca2idex['<unk>'] for word in string_list]\n",
    "    return converted_index\n",
    "\n",
    "# example\n",
    "print(sequence_to_index(['멋대로', '가나다라마'], voca2idex)) # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706838e-6441-4cd0-aa27-093c39c00fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tag integer encodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df882f50-42b0-4073-b104-83fc0c5672a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = data['tag'].unique()\n",
    "num_tags = len(tag_list)\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec5d01d3-5927-4253-83a3-33932f4af380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag2idex = {tag: idx for idx, tag in enumerate(tag_list)}\n",
    "# tag2idex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0353b1b-eb63-475e-a699-0143b8ef64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "idex2tag = {}\n",
    "idex2tag[0]='<unk>'\n",
    "idex2tag[1]='<pad>'\n",
    "idex2tag[2]='<bos>'\n",
    "idex2tag[3]='<eos>'\n",
    "idex2tag[4]='<mask>'\n",
    "idex2tag[5]='non'\n",
    "idex2tag[6]='coffe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8982602b-32fe-448e-b48e-046cdc5e5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idex = {'<unk>':0,'<pad>':1,'<bos>':2,'<eos>':3,'<mask>':4, 0:5, 1:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04068feb-b647-4121-8cfa-41994f33f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e30db683-8762-41c6-b7cc-53ad010430e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# seq_to_index function\n",
    "def tag_to_index(sequence, embedding_index):\n",
    "    '''\n",
    "    INPUT\n",
    "        sequence : string list\n",
    "    OUTPUT\n",
    "        integer list\n",
    "    '''\n",
    "    converted_index = [embedding_index[word] if word in embedding_index.keys() else embedding_index['<unk>'] for word in sequence]\n",
    "    return converted_index\n",
    "\n",
    "# # index2gene\n",
    "# idex2tag = {v:k for k,v in tag2idex.items()}\n",
    "# idex2tag[0]='[UNK]'\n",
    "# idex2tag[1]='[PAD]'\n",
    "# idex2tag[2]='[CLS]'\n",
    "# idex2tag[3]='[SEP]'\n",
    "# idex2tag[4]='[MASK]'\n",
    "# #print(list(gene2idex.keys())[list(gene2idex.values()).index(16)]) \n",
    "# print(f'{idex2tag[1]}')\n",
    "\n",
    "print(tag_to_index(['5', '6'], tag2idex)) # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d49e9d-fcc2-4a77-ad49-da4f72e199bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_to_index(['PER_B', 'DAT_B'], tag2idex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09e6b953-e25a-4f19-9e23-227dca8ff9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/voca2idex.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(voca2idex, fw)\n",
    "with open(\"data/idex2voca.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(idex2voca, fw)    \n",
    "with open(\"data/tag2idex.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag2idex, fw)\n",
    "with open(\"data/idex2tag.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(idex2tag, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abe8ed-cc47-430a-b75b-4555ef63080a",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2ec9f-9c9d-4851-a073-84a66ded2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_tokenizer = []\n",
    "# word_id_tokenizer = []\n",
    "# tag_id_tokenizer = []\n",
    "# label_ids_tokenizer = []\n",
    "# for i in data['word_id'][0:50]:\n",
    "#     # tokenizer\n",
    "#     content_tokenizer += tokenizer.encode(data['content'].iloc[i])[1:-1]\n",
    "#     word_id_tokenizer += [i]*(len(tokenizer.encode(data['content'].iloc[i]))-2)    \n",
    "#     label_ids_tokenizer += [tag2idex[data['tag'].iloc[i]]] + [-100] * (len(tokenizer.encode(data['content'].iloc[i]))-3)\n",
    " \n",
    "    \n",
    "# data_tokenizer = pd.DataFrame({'token': content_tokenizer, 'word_id': word_id_tokenizer, 'label_ids': label_ids_tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cc881e4-3586-4ede-8fed-b57ad93f676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = okt.morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16ade537-c346-42bd-915b-3c359a08e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def tokenizer_per_sentece(content_list, tag_list, word_id_list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        content_split_list : a list that represents a sentence \n",
    "            e.g. ['비토리오', '양일', '만에', '영사관', '감호', '용퇴,', '항룡', '압력설', '의심만', '가율']\n",
    "        tag_split_list\n",
    "            e.g. ['PER_B', 'DAT_B', '-', 'ORG_B', 'CVL_B', '-', '-', '-', '-', '-']\n",
    "        word_id_split_list\n",
    "            e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    OUTPUT: tokenzier results\n",
    "        content_tokenizer : a tokenized content list without special tokens\n",
    "        word_id_tokenizer : a word id list\n",
    "        tag_id_tokenizer : a tag id list that the only first content has a tag.\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    if not (len(content_list)==len(tag_list)==len(word_id_list)):\n",
    "        print(\"check the input data\")\n",
    "    \n",
    "    # tokenizer\n",
    "    content_tokenizer = []\n",
    "    word_id_tokenizer = []\n",
    "    tag_id_tokenizer = []\n",
    "    \n",
    "    # error handling : nan in input\n",
    "    if any(pd.isna(content_list)):\n",
    "        return [], [], []\n",
    "    \n",
    "    for i in range(len(word_id_list)):\n",
    "        # tokenizer\n",
    "        content_tokenizer += tokenizer(content_list[i]) # to remove special tokens [CLS] and [SEP]\n",
    "        word_id_tokenizer += [word_id_list[i]] * (len(tokenizer(content_list[i])))\n",
    "        tag_id_tokenizer += [tag2idex[tag_list[i]]] + [-100] * (len(tokenizer(content_list[i]))-1)\n",
    "    \n",
    "    # to convert string into index\n",
    "    content_tokenizer = sequence_to_index(content_tokenizer, voca2idex)\n",
    "    \n",
    "    # error handling : tokenizer does not work properly\n",
    "    if not (len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)):\n",
    "        # tokenzier does not work properly becuase of noise.\n",
    "        print(f'sanity check : {len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)}')\n",
    "        content_tokenizer = []\n",
    "        word_id_tokenizer = []\n",
    "        tag_id_tokenizer = []\n",
    "    return content_tokenizer, tag_id_tokenizer, word_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "862a6932-b526-4155-bfde-309698d1aa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강사', '로', '활동', '잘할', '같은']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc13f767-c940-47a6-85f6-9d6110b81400",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[0], tag_split[0], word_id_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d1dcb25-a57b-4538-935d-e9f70ff79450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13782, 75328, 67477, 0, 13189, 0]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ca68f23-42f4-488d-a9eb-3881571bfdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강사', '로', '활동', '<unk>', '할', '<unk>']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idex2voca[word] for word in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "655c3286-2f9d-41a9-b5b6-ef7645faf539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 5, 5, -100, 5]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c934426-6c98-40ca-b8ea-218ae4bdc40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc580a10-f552-4497-af4c-10b7fc6d9a01",
   "metadata": {},
   "source": [
    "error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eb65f64-7a3a-4413-a3ee-ad5606327a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff', '가장', '한', '추출', '콜드', '추출', '에서', '카페인', '변화']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af978862-503b-4c62-9a2d-99316c4c932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5d54aee-8240-4a10-b71c-71986d5e6ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[564651, 564652, 564653, 564654, 564655, 564656, 564657, 564658, 564659]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c13d796d-2d30-4548-ac4a-371a8e6b392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check : False\n"
     ]
    }
   ],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[172999], tag_split[172999], word_id_split[172999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ef4e90d-7ef7-40df-8f2d-59e83f7bd772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bd86f-1cf9-4239-a06d-7d45e56de7eb",
   "metadata": {},
   "source": [
    "Error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17561092-f699-4e65-a419-71b21e141dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[500120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b811f1dd-a10a-4778-894f-871b6dff03eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(content_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36b0d8df-e2a5-453f-a006-07173745cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(content_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8ebb2b2-bf1c-400c-a972-23bcefd9245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content_split[500120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b815fd9d-39d3-4a69-a8db-70e221dcae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[500120], tag_split[500120], word_id_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50f3d152-5ac9-4884-8e3f-057d8abe6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf0d53-d6a0-479e-9912-ead5da12825c",
   "metadata": {},
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b979e86c-718d-47f4-91c8-1d0496a5ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '1', 'coffee', nan, 'Mp']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[598602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eaaab4a3-6e55-4e1e-8c9a-94c6db698000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(content_split[598602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4864e32c-be4c-477c-86fc-3b834a2deec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(pd.isna(content_split[598602]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2faf3be4-a579-48d0-8508-1ee9303a4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[598602], tag_split[598602], word_id_split[598602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "490d5544-922d-4be8-8c23-b63f840e35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def tokenizer_all_data(content_split, tag_split, word_id_split):\n",
    "    # sanity check\n",
    "    if not (len(content_split)==len(tag_split)==len(word_id_split)):\n",
    "        print(\"check the input data\")\n",
    "        \n",
    "    content_split_token, tag_split_token, word_id_split_token = [], [], []\n",
    "    for i in tqdm(range(len((content_split)))):\n",
    "        #print(i)\n",
    "        content, tag, wordID = tokenizer_per_sentece(content_split[i], tag_split[i], word_id_split[i])\n",
    "        content_split_token.append(content)\n",
    "        tag_split_token.append(tag)\n",
    "        word_id_split_token.append(wordID)\n",
    "    return content_split_token, tag_split_token, word_id_split_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "882fd66c-6786-4d8f-b346-dd866f561df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 703464/703464 [59:20<00:00, 197.58it/s]\n"
     ]
    }
   ],
   "source": [
    "content_split_tok, tag_split_tok, word_id_split_tok = tokenizer_all_data(content_split, tag_split, word_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d22f00-793b-40a6-b3da-0112585eace2",
   "metadata": {},
   "source": [
    "record\n",
    "- 20220527 : KoBert\n",
    "- 20220531 : Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "007eac17-c8d8-4b27-9b92-8d7be7ca5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/content_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(content_split_tok, fw)\n",
    "with open(\"data/tag_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag_split_tok, fw)\n",
    "with open(\"data/word_id_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(word_id_split_tok, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6bc6d21a-c4b2-41c1-b5c9-9f55b9b4d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"data/content_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    content_split_tok = pickle.load(fr)\n",
    "with open(\"data/tag_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    tag_split_tok = pickle.load(fr)\n",
    "with open(\"data/word_id_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    word_id_split_tok = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be76ba18-da81-4b83-90cd-f2f906fd33b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강', '사', '하면', '잘', '할', '스타', '는', '?', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "[tokenizer.decode(token) for token in content_split_tok[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c6326-8efa-4a06-9337-eb937ecf29f0",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c537798-9e6b-4efd-82cd-ba9db2d0bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n",
      "6.924360502429632\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x) for x in content_split_tok]))\n",
    "print(sum([len(x) for x in content_split_tok])/len(content_split_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c20e8cb6-4d94-443a-8d74-17cd72c5f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_tensor(token_list, max_len=512):\n",
    "    '''\n",
    "    This function get a list of token lists and converts a input list into a list in size of max_len including special tokens.\n",
    "    \n",
    "    INPUT\n",
    "        token_list : a list of token lists\n",
    "            e.g. [[1,2,3,4], [5,6,7,8], [9, 10, 11, 12, 13, 14, 15]]\n",
    "        max_len : int\n",
    "    OUTPUT\n",
    "        token_ls : a list of lists in size of max_len with special tokens. The last token that does not fit the size would be dropped.\n",
    "            e.g. [[0, 1, 2, 3, 4, 0], [0, 5, 6, 7, 8, 0], [0, 9, 10, 11, 12, 0]]\n",
    "    '''\n",
    "    # setting\n",
    "    max_len = max_len-2 # to consider the special tokens\n",
    "    pad = 1 # an index of special token\n",
    "    bos = 2 # an index of special token\n",
    "    eos = 3 # an index of special token\n",
    "    token_output_ls = []\n",
    "    token_ls = []\n",
    "    #token_ls_iter_position = 0 # index of the last element in token_ls\n",
    "    \n",
    "    for index, tokens in enumerate(token_list):\n",
    "        n_token = len(tokens)\n",
    "        \n",
    "        if len(token_ls) + n_token <= max_len :\n",
    "            token_ls += tokens\n",
    "            #token_ls_iter_position += len(token_ls)\n",
    "        else : \n",
    "            cut_position = max_len - len(token_ls)\n",
    "            #assert cut_position >= 0\n",
    "            token_ls += tokens[:cut_position]\n",
    "\n",
    "            # to collect the current token_ls\n",
    "            token_ls = [bos] + token_ls + [eos] # to add special tokens\n",
    "\n",
    "            if not len(token_ls)-2 == max_len : \n",
    "                print(f'index: {index}')\n",
    "                print(f'cut_position: {cut_position}')\n",
    "                print(len(token_ls))\n",
    "            assert len(token_ls)-2 == max_len\n",
    "            token_output_ls.append(token_ls)\n",
    "\n",
    "            # to update token_ls for next iteration\n",
    "            token_ls = tokens[cut_position:]\n",
    "            #token_ls_iter_position = len(token_ls) - 1\n",
    "            \n",
    "            # if token_ls is longer than max_len, it requires an additional cleavage\n",
    "            while (len(token_ls) > max_len):\n",
    "                token_output_ls.append([bos] + token_ls[:max_len] + [eos])\n",
    "                token_ls = tokens[max_len:]\n",
    "    return token_output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "17a9165b-7b67-4a9c-bce9-6cf6bb7448ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_input = prep_for_tensor(content_split_tok, 512)\n",
    "tag_input = prep_for_tensor(tag_split_tok, 512)\n",
    "word_id_input = prep_for_tensor(word_id_split_tok, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7caa75a9-bfca-4b90-a1f0-903120f2be71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[807, 6493, 517, 6079, 5141, 3942, 7836, 833]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8db6cbd3-bf5f-4080-9a29-5c17ba8a8d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4656]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split_tok[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41eb3acf-fa43-40e7-8017-1b262f8e432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_padding(token_list, max_len=512):\n",
    "#     '''\n",
    "#     INPUT\n",
    "#         token_list : list\n",
    "#             e.g. [[1,2,3,4], [5,6,7,8]]\n",
    "#         max_len : int\n",
    "#     OUTPUT\n",
    "#         token_ls : list\n",
    "#     '''\n",
    "#     import copy\n",
    "#     token_ls = copy.deepcopy(token_list)\n",
    "#     max_len = max_len-2 # to consider the special tokens\n",
    "#     for index, tokens in enumerate(token_ls):\n",
    "#         n_token = len(tokens)\n",
    "#         pad = 1 # an index of special token\n",
    "#         bos = 2 # an index of special token\n",
    "#         eos = 3 # an index of special token\n",
    "#         # add padding if the length is shorter than max_len\n",
    "#         if n_token < max_len:\n",
    "#             token_ls[index] += [pad] * (max_len - n_token) # 부족한 만큼 padding을 추가함\n",
    "        \n",
    "#         # if a sentence is longer than max_len, cut a sentence.\n",
    "#         elif n_token > max_len:\n",
    "#             token_ls[index] = tokens[:max_len]\n",
    "        \n",
    "#         # to add begining token and end token\n",
    "#         token_ls[index] = [bos] + token_ls[index] + [eos]\n",
    "#     return token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffc0e21f-503d-4f3a-9c67-29685385dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_input = add_padding(content_split_tok, 512)\n",
    "# tag_input = add_padding(tag_split_tok, 512)\n",
    "# word_id_input = add_padding(word_id_split_tok, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c085f602-9ae7-45b3-9383-61513606e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_input[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24c7c495-8728-4769-85ea-23660fa6978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/content_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(content_input, fw)\n",
    "with open(\"./data/tag_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag_input, fw)\n",
    "with open(\"./data/word_id_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(word_id_input, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd8725-7680-4a67-ac09-60e2b7ca5c36",
   "metadata": {},
   "source": [
    "# To load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca19781-ac09-4046-8404-49f7daa578f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"./data/voca2idex.pickle\", \"rb\") as fr:\n",
    "    voca2idex = pickle.load(fr)\n",
    "with open(\"./data/idex2voca.pickle\", \"rb\") as fr:\n",
    "    idex2voca = pickle.load(fr)\n",
    "with open(\"./data/tag2idex.pickle\", \"rb\") as fr:\n",
    "    tag2idex = pickle.load(fr)\n",
    "with open(\"./data/idex2tag.pickle\", \"rb\") as fr:\n",
    "    idex2tag = pickle.load(fr)\n",
    "    \n",
    "# configuration parameters\n",
    "VOCA_SIZE = len(voca2idex)\n",
    "print(VOCA_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59620626-32e7-47ae-89a6-47c08fefeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"./data/content_input_20220531.pickle\", \"rb\") as fr:\n",
    "    content_input = pickle.load(fr)\n",
    "with open(\"./data/tag_input_20220531.pickle\", \"rb\") as fr:\n",
    "    tag_input = pickle.load(fr)\n",
    "with open(\"./data/word_id_input_20220531.pickle\", \"rb\") as fr:\n",
    "    word_id_input = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4941f5e-78bd-4952-82cd-c29ce146964e",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7869ee2c-a085-46cf-89cb-ef9ae9f3546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888c2d4c-fcbb-4190-b0a8-f57c49fcfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = torch.tensor(content_input)\n",
    "input_y = torch.tensor(tag_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a1e885-11dc-4ce5-a8d1-e83a8453ef3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 13782, 75328, 67477,     0, 13189,     0, 75210, 73263, 13782,\n",
       "        75328,     0, 75210, 73263, 13782,     0, 73916, 60672,     0, 75210,\n",
       "        73263, 13782, 75328,     0, 75486, 74865, 74865,     0, 74865, 74397,\n",
       "        74865, 74397, 74865, 74865, 64762, 68574, 74865, 74865,     0, 74939,\n",
       "            0, 66726, 66726, 66726, 66726, 35799, 74778, 74778, 74874, 35799,\n",
       "        74778, 74874, 74778, 75206, 66063, 70938, 68762, 74703, 74623, 75206,\n",
       "        66063, 70938, 75494, 68762, 74703, 74623, 75206, 66063, 70938, 68762,\n",
       "        74703, 74623, 75197, 75206, 66063, 70938, 68762, 74703, 74623, 72157,\n",
       "        74865, 44274,     0, 72157, 74865, 44274, 72157, 75260, 74865, 44274,\n",
       "        72157, 43623, 74865, 44274,     0, 71938,     0,     0, 44972, 53063,\n",
       "            0,     0,     0, 44972,     0, 75112,     0,     0,     0, 44972,\n",
       "            0, 44972, 75112, 75112, 74865, 74865, 74865,     0, 74865, 74517,\n",
       "        72509,     0, 75256, 72509, 67432, 74517, 72509, 75256, 72509, 67432,\n",
       "            0,     0,     0, 74517, 72509, 75256, 72509, 67432, 74517, 72509,\n",
       "            0, 75256, 72509, 67432,     0,     0,     0, 72510,     0, 75241,\n",
       "        72510,     0, 75241, 75328,  1307, 75102, 72510,     0, 75241, 44555,\n",
       "        72510,     0, 75241,     0, 70962, 47694, 70962, 47694, 70962, 70962,\n",
       "        47694,     0,     0,     0,     0,     0,     0, 74599, 75014, 73793,\n",
       "            0,     0, 75105, 66877, 64078, 75461, 43044, 66877, 64078, 75461,\n",
       "        43044, 55651, 66877, 64078, 75461, 43044,     0, 55651, 66877, 64078,\n",
       "            0, 75461, 43044,     0, 71645, 75408, 71645, 75408, 61681, 71546,\n",
       "        71546, 71083, 61681, 74110,     0,     0,     0, 74759, 60508,     0,\n",
       "            0, 74759, 60508,     0, 75210, 47072, 74865, 75157, 40654, 75210,\n",
       "        47072, 74865, 75157, 40654, 75210, 47072, 74865, 75157, 40654,     0,\n",
       "        75210, 47072, 74865, 75157, 40654,     0,     0, 58039, 66840, 58039,\n",
       "            0, 68068,     0,     0,     0, 13353, 74366,     0, 13353, 74366,\n",
       "            0,     0, 13353, 74366,     0,     0, 13353, 74366, 43494, 75225,\n",
       "        69571,     0,  4840,  4840, 75560,     0,     0,  4840,     0,  4840,\n",
       "        71648,  2405, 59161,     0,     0, 71648,  2405, 59161, 71648,  2405,\n",
       "        59161, 71648,  2405, 59161,     0,     0, 72157, 72157, 72157, 16672,\n",
       "        72157, 29676,   329, 43044,     0, 29676,   329, 29676,   329,     0,\n",
       "        29676,   329, 75279, 43044,     0, 72157, 35799, 75348, 75357, 47694,\n",
       "        72157, 43592, 69775, 75357, 72157, 35799, 75348, 75357, 73793,     0,\n",
       "        72157, 75357, 47694,     0, 19985,     0,     0,     0, 59161, 59161,\n",
       "        67622, 59161, 75112, 75107, 74831, 75107, 59161, 75112, 75107, 74831,\n",
       "        75107, 75484,     0, 73087,     0,     0, 74865,     0, 12611,     0,\n",
       "        74865, 12611, 74865, 74356, 75289, 75348, 43592, 12611, 74865,     0,\n",
       "        74356, 75289, 75348, 43592, 12611, 72157, 41829, 72157, 41829, 72157,\n",
       "        41829, 72157,     0, 41829,     0,     0,  4244,     0, 75380, 62190,\n",
       "        72289, 21264,  4244, 75380, 62190, 72289, 21264, 75328,  4244, 72289,\n",
       "        72168, 75289, 21264, 75328,  4244, 72289, 72168, 75289,     0, 46150,\n",
       "            0,     0, 46150, 69674, 74049,     0, 46150, 57775,     0, 26727,\n",
       "        29676,     0, 74049,     0, 29676,     0, 29676,     0, 74299, 29676,\n",
       "            0, 55247,     0,  4611, 50935,  7692, 50935,  7692, 50935,     0,\n",
       "         7692,  4611, 50935,     0,     0,     0, 43592,     0,     0,     0,\n",
       "        43592, 75044,     0,     0,     0, 43592, 75044,     0,     0,     0,\n",
       "        75044, 75229, 75540,     0, 74865, 46090, 65960,     0,     0, 74865,\n",
       "        46090, 65960,     0,     0,     0,     0, 74865, 74438, 46090, 65960,\n",
       "            0,     0, 19397, 74959, 74865,     0, 46090, 65960,     0,     0,\n",
       "            0,     0, 72157, 37827, 74441, 72157, 37827, 74441, 74918, 72157,\n",
       "        37827, 74441, 72157, 37827, 74441, 56161,  7936, 19397, 56161,  7936,\n",
       "        19397,     3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890461db-675e-4cb4-8e97-753a272fae9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    5,    6,    6, -100,    6,    6, -100,    6,    6, -100,    6,\n",
       "           5,    6, -100,    5, -100, -100, -100,    5,    6,    5,    6,    5,\n",
       "           5,    6,    5,    5,    5, -100,    6,    5,    5,    5,    6,    6,\n",
       "           5, -100,    6,    6,    5, -100,    6,    6,    5, -100,    6,    6,\n",
       "           5, -100, -100,    5,    6,    5,    6,    5,    5,    6,    6,    5,\n",
       "           6,    6,    6,    5,    6,    5,    5,    6,    6,    5,    5,    6,\n",
       "           5,    5,    6,    6,    6,    5, -100, -100,    6,    6,    5,    5,\n",
       "        -100, -100,    6,    6,    5,    5, -100, -100,    5,    6,    6,    5,\n",
       "           5, -100, -100,    6,    6,    6,    6,    6,    5,    6,    6,    6,\n",
       "           6,    6,    6,    6,    6,    5, -100,    6,    6,    6,    6,    5,\n",
       "        -100,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "           5,    6,    5, -100,    5,    5, -100,    5,    6,    5, -100,    5,\n",
       "           6,    5,    5,    5,    6,    5,    6, -100,    6, -100,    6, -100,\n",
       "           6, -100,    5,    6,    6,    6,    5,    5,    6,    6,    5, -100,\n",
       "           6,    6,    6,    6,    6,    6,    6,    6,    5,    6,    6,    6,\n",
       "           6,    6,    6,    6,    5,    6,    6,    6,    6,    5,    6,    6,\n",
       "           5,    6,    6,    5,    5,    5, -100,    6,    6,    6,    6, -100,\n",
       "           6,    6,    6,    6,    6, -100,    6,    6,    6,    6,    5,    6,\n",
       "        -100,    6,    6,    5,    5, -100,    6,    6,    6, -100,    5,    6,\n",
       "           6, -100,    6,    6,    6, -100,    5,    5,    6,    6,    6, -100,\n",
       "           6,    6,    6, -100,    5,    6,    6,    5, -100, -100,    6,    6,\n",
       "           6,    6,    6,    6,    5, -100,    6,    6,    5, -100,    5, -100,\n",
       "        -100,    5, -100,    5,    5, -100, -100,    5, -100,    6, -100,    6,\n",
       "           5,    6,    5,    6, -100,    6,    6,    6, -100,    6,    6,    6,\n",
       "        -100,    6,    6,    5,    6, -100,    6,    6, -100,    5,    5,    5,\n",
       "           6,    5, -100,    6, -100,    6,    5, -100,    6, -100,    5,    6,\n",
       "           5, -100,    6,    5, -100,    6,    6,    5,    6,    5,    6,    5,\n",
       "        -100,    6,    6,    5, -100,    6,    6,    5, -100,    5, -100, -100,\n",
       "           6,    5,    5, -100, -100,    5, -100,    6,    5,    5, -100,    6,\n",
       "           5,    5,    5, -100,    6,    6, -100,    6,    6, -100,    6,    6,\n",
       "        -100,    5,    6,    6, -100,    6,    6,    6,    5,    6,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5, -100,    5, -100,    5, -100,    5,\n",
       "        -100,    6,    5,    6,    6,    5,    6,    6,    6,    5,    6,    5,\n",
       "           5,    6,    5,    6,    6,    6,    5,    6,    6,    5,    6,    5,\n",
       "           6,    6,    6,    6,    6,    5,    6,    6,    6, -100,    6,    6,\n",
       "        -100,    6,    5,    6,    6, -100,    6,    6, -100,    6,    5, -100,\n",
       "           6,    6, -100,    6,    6, -100,    6,    5,    6,    6, -100,    5,\n",
       "           6,    6, -100,    6,    5,    5,    5,    5,    5,    6,    6,    6,\n",
       "           6,    6,    6,    5,    5,    5,    6,    5,    6,    6,    5,    6,\n",
       "           6,    6,    5,    5,    5, -100,    5, -100,    6,    6,    5,    6,\n",
       "           6,    6,    6,    5,    6,    5, -100, -100,    6,    5, -100,    5,\n",
       "        -100,    5,    5,    5, -100,    5,    5,    5,    5, -100,    5, -100,\n",
       "           6,    6, -100,    6,    5,    6,    6,    3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_y[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9db35-edcb-41ec-810c-8e93f3f100b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261626-9cbd-45db-92f7-283cfd3cb4da",
   "metadata": {},
   "source": [
    "source : https://github.com/naver/nlp-challenge/blob/master/missions/ner/data/train/train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152bf34d-8277-4da4-8365-1956cfa86df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d5a5f1-46d7-438b-aee0-10d932d04568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/train_data.txt', sep='\\t',  header=None)\n",
    "# data.columns = ['index' , 'content', 'tag']\n",
    "# data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e4de44-03d3-48a3-b7b0-ce94b6a23aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/indexingFile.csv', sep=',',  header=0)\n",
    "# data.columns = ['number', 'index' , 'content', 'tag']\n",
    "# data = data.drop(['number'], axis=1)\n",
    "# data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1c26e6e-6d15-4fb6-a424-7f10d6f869ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/indexingFile_527.csv', sep=',',  header=0)\n",
    "data.columns = ['number', 'index' , 'content', 'tag']\n",
    "data = data.drop(['number'], axis=1)\n",
    "data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4265c4cf-0221-4daa-a16d-8fc97e7ed5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>활동</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>잘할</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>같은</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>같은</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>스타는?.</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>일타</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>로</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>것</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>갤러</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>리</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>리</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index content  tag  word_id\n",
       "0       1      강사    1        0\n",
       "1       2       로    0        1\n",
       "2       3      활동    0        2\n",
       "3       4      잘할    0        3\n",
       "4       5      같은    0        4\n",
       "5       1      일타    0        5\n",
       "6       2      강사    1        6\n",
       "7       3       로    0        7\n",
       "8       4      같은    0        8\n",
       "9       1      일타    0        9\n",
       "10      2      강사    1       10\n",
       "11      3      하면    0       11\n",
       "12      4   스타는?.    0       12\n",
       "13      1      일타    0       13\n",
       "14      2      강사    1       14\n",
       "15      3       로    0       15\n",
       "16      4      하면    0       16\n",
       "17      5       것    0       17\n",
       "18      1      커피    1       18\n",
       "19      1      커피    1       19\n",
       "20      2      갤러    0       20\n",
       "21      1      커피    1       21\n",
       "22      2       리    0       22\n",
       "23      1      커피    1       23\n",
       "24      2       리    0       24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccffdef-7347-4cb1-94be-9668b0e72fee",
   "metadata": {},
   "source": [
    "To split by a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699768a7-172e-4f7a-aefd-7e23ea3f449b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split = []\n",
    "tag_split = []\n",
    "word_id_split = []\n",
    "\n",
    "content_spliter = []\n",
    "tag_spliter = []\n",
    "word_spliter = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    content_spliter.append(data['content'].iloc[i])\n",
    "    tag_spliter.append(data['tag'].iloc[i])\n",
    "    word_spliter.append(data['word_id'].iloc[i])\n",
    "    if (i == len(data)-1) : # the last word in a data\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "    elif (data['index'].iloc[i+1] == 1) : # the last word in a sentence\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "        \n",
    "# sanity check\n",
    "len(content_split) == len(tag_split) == len(word_id_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f362b5a-d62a-4909-8c77-4427a1c98169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4fe9a9-ef2a-4a0e-a0e5-e9d65103ed59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8cfa82-5e4e-457e-9398-5b8bd821a0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703464"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6fa9c-68a0-4910-a3d3-336240326415",
   "metadata": {},
   "source": [
    "## vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f5f7e-787f-4d04-8277-a3dc42ee6983",
   "metadata": {},
   "source": [
    "konlpy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42b232e4-9285-43db-95c3-fe37d929e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 : ['자연어', '처리', '연구', '를', '기반', '으로', '인공', '지능', '모델', '을', '구성', '하고', '있습니다', '.']\n",
      "품사 태깅 : [('자연어', 'Noun'), ('처리', 'Noun'), ('연구', 'Noun'), ('를', 'Josa'), ('기반', 'Noun'), ('으로', 'Josa'), ('인공', 'Noun'), ('지능', 'Noun'), ('모델', 'Noun'), ('을', 'Josa'), ('구성', 'Noun'), ('하고', 'Josa'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n",
      "명사 추출 : ['자연어', '처리', '연구', '기반', '인공', '지능', '모델', '구성']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print('형태소 분석 :',okt.morphs(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))\n",
    "print('품사 태깅 :',okt.pos(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))\n",
    "print('명사 추출 :',okt.nouns(\"자연어 처리 연구를 기반으로 인공지능 모델을 구성하고 있습니다.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce1160-9e61-454f-9ee9-16066e2d37bb",
   "metadata": {},
   "source": [
    "To create the vocabulary corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1751bb01-a969-40b2-973b-3c2ff85f2321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75559\n",
      "['세트', '해하', '안슬것', '별식', '좆', '급', '중', '놈', '이기', '개비']\n"
     ]
    }
   ],
   "source": [
    "# to select unique value in content_split\n",
    "unique_content_list = list(set([x for sublist in content_split for x in sublist]))\n",
    "\n",
    "# to select only string\n",
    "filtered = filter(lambda voca: isinstance(voca, str), unique_content_list)\n",
    "unique_filtered_content_list = list(filtered)\n",
    "\n",
    "# concatenation for tokenizer\n",
    "unique_filtered_content_str = ' '.join(unique_filtered_content_list)\n",
    "tokenizer_voca = okt.nouns(unique_filtered_content_str)\n",
    "print(len(tokenizer_voca))\n",
    "print(tokenizer_voca[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f3f710b-45a7-4ed5-94db-b2389a69ab55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75559"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(okt.nouns(unique_filtered_content_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8790d8d2-a337-4a6f-b647-29c84dc4cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_voca = okt.nouns(unique_filtered_content_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9c16596-d299-4755-ab35-e225fdf211a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['세트', '해하', '안슬것', '별식', '좆', '급', '중', '놈', '이기', '개비']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_voca[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bf9f4e3-5084-4a67-a6a4-6fa982426fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21062\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "voca2idex = {gene: idx+5 for idx, gene in enumerate(tokenizer_voca)}\n",
    "voca2idex['<unk>']= 0 # unknown\n",
    "voca2idex['<pad>']= 1 # padding\n",
    "voca2idex['<bos>']= 2 # begining of sentence\n",
    "voca2idex['<eos>']= 3 # end of sentence\n",
    "voca2idex['<mask>']= 4 # masking\n",
    "\n",
    "# configuration parameters\n",
    "VOCA_SIZE = len(voca2idex)\n",
    "print(VOCA_SIZE)\n",
    "\n",
    "# index2voca\n",
    "idex2voca = {v:k for k,v in voca2idex.items()}\n",
    "#print(list(gene2idex.keys())[list(gene2idex.values()).index(16)]) \n",
    "print(f'{idex2voca[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff988584-5161-4ec9-a3f2-89b204ed2484",
   "metadata": {},
   "source": [
    "voca to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a42ea0-791d-4f67-90f4-93536f15a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60453, 0]\n"
     ]
    }
   ],
   "source": [
    "# seq_to_index function\n",
    "def sequence_to_index(string_list, voca2idex):\n",
    "    '''\n",
    "    INPUT\n",
    "        string_list : string list\n",
    "    OUTPUT\n",
    "        integer list\n",
    "    '''\n",
    "    converted_index = [voca2idex[word] if word in voca2idex.keys() else voca2idex['<unk>'] for word in string_list]\n",
    "    return converted_index\n",
    "\n",
    "# example\n",
    "print(sequence_to_index(['멋대로', '가나다라마'], voca2idex)) # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744599e-03a6-4506-b67a-5fc0b18f45a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tag integer encodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c543bdc-3762-4b90-ae9a-e7578f765c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = data['tag'].unique()\n",
    "num_tags = len(tag_list)\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5162b295-d5c7-482a-9835-522f20fed20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag2idex = {tag: idx for idx, tag in enumerate(tag_list)}\n",
    "# tag2idex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2352f49-b954-44bc-bfaa-6a9dd3dfa7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idex2tag = {}\n",
    "idex2tag[0]='<unk>'\n",
    "idex2tag[1]='<pad>'\n",
    "idex2tag[2]='<bos>'\n",
    "idex2tag[3]='<eos>'\n",
    "idex2tag[4]='<mask>'\n",
    "idex2tag[5]='non'\n",
    "idex2tag[6]='coffe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "754af5fc-3b37-4612-93b1-5932f0fc9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idex = {'<unk>':0,'<pad>':1,'<bos>':2,'<eos>':3,'<mask>':4, 0:5, 1:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32ff6c33-4f52-4023-a109-71f587fe5361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "870e20f5-32c2-41a2-b3b2-924c474c45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# seq_to_index function\n",
    "def tag_to_index(sequence, embedding_index):\n",
    "    '''\n",
    "    INPUT\n",
    "        sequence : string list\n",
    "    OUTPUT\n",
    "        integer list\n",
    "    '''\n",
    "    converted_index = [embedding_index[word] if word in embedding_index.keys() else embedding_index['<unk>'] for word in sequence]\n",
    "    return converted_index\n",
    "\n",
    "# # index2gene\n",
    "# idex2tag = {v:k for k,v in tag2idex.items()}\n",
    "# idex2tag[0]='[UNK]'\n",
    "# idex2tag[1]='[PAD]'\n",
    "# idex2tag[2]='[CLS]'\n",
    "# idex2tag[3]='[SEP]'\n",
    "# idex2tag[4]='[MASK]'\n",
    "# #print(list(gene2idex.keys())[list(gene2idex.values()).index(16)]) \n",
    "# print(f'{idex2tag[1]}')\n",
    "\n",
    "print(tag_to_index(['5', '6'], tag2idex)) # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da8296-d40d-491d-9baa-288be5beee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_to_index(['PER_B', 'DAT_B'], tag2idex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "002505f8-6af7-44a5-90c6-eee94228f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/voca2idex.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(voca2idex, fw)\n",
    "with open(\"data/idex2voca.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(idex2voca, fw)    \n",
    "with open(\"data/tag2idex.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag2idex, fw)\n",
    "with open(\"data/idex2tag.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(idex2tag, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae98973-c8bf-4101-8fe2-dab10e2cb893",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301276b-8485-401e-bcf4-4cc027da2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_tokenizer = []\n",
    "# word_id_tokenizer = []\n",
    "# tag_id_tokenizer = []\n",
    "# label_ids_tokenizer = []\n",
    "# for i in data['word_id'][0:50]:\n",
    "#     # tokenizer\n",
    "#     content_tokenizer += tokenizer.encode(data['content'].iloc[i])[1:-1]\n",
    "#     word_id_tokenizer += [i]*(len(tokenizer.encode(data['content'].iloc[i]))-2)    \n",
    "#     label_ids_tokenizer += [tag2idex[data['tag'].iloc[i]]] + [-100] * (len(tokenizer.encode(data['content'].iloc[i]))-3)\n",
    " \n",
    "    \n",
    "# data_tokenizer = pd.DataFrame({'token': content_tokenizer, 'word_id': word_id_tokenizer, 'label_ids': label_ids_tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29854636-13c1-4527-80d2-2e6dc09a20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = okt.morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7face7c0-8e23-4ec2-9b59-5bf3de2c7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def tokenizer_per_sentece(content_list, tag_list, word_id_list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        content_split_list : a list that represents a sentence \n",
    "            e.g. ['비토리오', '양일', '만에', '영사관', '감호', '용퇴,', '항룡', '압력설', '의심만', '가율']\n",
    "        tag_split_list\n",
    "            e.g. ['PER_B', 'DAT_B', '-', 'ORG_B', 'CVL_B', '-', '-', '-', '-', '-']\n",
    "        word_id_split_list\n",
    "            e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    OUTPUT: tokenzier results\n",
    "        content_tokenizer : a tokenized content list without special tokens\n",
    "        word_id_tokenizer : a word id list\n",
    "        tag_id_tokenizer : a tag id list that the only first content has a tag.\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    if not (len(content_list)==len(tag_list)==len(word_id_list)):\n",
    "        print(\"check the input data\")\n",
    "    \n",
    "    # tokenizer\n",
    "    content_tokenizer = []\n",
    "    word_id_tokenizer = []\n",
    "    tag_id_tokenizer = []\n",
    "    \n",
    "    # error handling : nan in input\n",
    "    if any(pd.isna(content_list)):\n",
    "        return [], [], []\n",
    "    \n",
    "    for i in range(len(word_id_list)):\n",
    "        # tokenizer\n",
    "        content_tokenizer += tokenizer(content_list[i]) # to remove special tokens [CLS] and [SEP]\n",
    "        word_id_tokenizer += [word_id_list[i]] * (len(tokenizer(content_list[i])))\n",
    "        tag_id_tokenizer += [tag2idex[tag_list[i]]] + [-100] * (len(tokenizer(content_list[i]))-1)\n",
    "    \n",
    "    # to convert string into index\n",
    "    content_tokenizer = sequence_to_index(content_tokenizer, voca2idex)\n",
    "    \n",
    "    # error handling : tokenizer does not work properly\n",
    "    if not (len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)):\n",
    "        # tokenzier does not work properly becuase of noise.\n",
    "        print(f'sanity check : {len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)}')\n",
    "        content_tokenizer = []\n",
    "        word_id_tokenizer = []\n",
    "        tag_id_tokenizer = []\n",
    "    return content_tokenizer, tag_id_tokenizer, word_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bd116df-2708-4ef8-9233-b85cf5ed2000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강사', '로', '활동', '잘할', '같은']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97f12ad4-1dea-461c-a940-27f4b037db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[0], tag_split[0], word_id_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b414a8c-7e33-4652-87e7-fc941aa4ed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13782, 75328, 67477, 0, 13189, 0]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74342c80-d21e-4b2c-b52f-9b03effdf5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강사', '로', '활동', '<unk>', '할', '<unk>']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idex2voca[word] for word in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f3b56a7-2d12-49f7-98be-61a7a2a99194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 5, 5, -100, 5]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b19e24d-364c-4b41-a574-b89a60d54e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c285c-ecba-4dee-8822-a30f03786b25",
   "metadata": {},
   "source": [
    "error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcc91120-73fe-480f-b915-cac30b3b055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff', '가장', '한', '추출', '콜드', '추출', '에서', '카페인', '변화']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5e54b8f-b447-42b3-987f-83ca8e27c6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9661fa4b-980e-4b99-bdf3-a75584804b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[564651, 564652, 564653, 564654, 564655, 564656, 564657, 564658, 564659]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id_split[172999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba6989d5-1157-40b0-a996-e5646d6ccf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check : False\n"
     ]
    }
   ],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[172999], tag_split[172999], word_id_split[172999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8061117-a124-4326-80cc-911482f0f662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e5a44-581f-4f64-9366-5bb7efed3af4",
   "metadata": {},
   "source": [
    "Error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "468a5fae-1de4-478b-8a7e-849fb5f08fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[500120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bc6c342-3f91-4ed7-baf0-2184a923647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(content_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77c45973-9660-42bb-809d-323501f53960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(content_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7aa52e0b-0e6e-4153-b3cb-a979f3811889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content_split[500120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffc75a97-8044-47e6-b7c4-4d3b5ee60a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[500120], tag_split[500120], word_id_split[500120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88701242-6f30-4aae-b9ad-30ed3d6e87dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694595a-884f-4556-98e0-f00406e995bf",
   "metadata": {},
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "423dc092-caac-4165-ae96-6f6b429333af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '1', 'coffee', nan, 'Mp']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split[598602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "060fcb40-b14c-4183-aeb4-0569ad12e818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(content_split[598602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6579d2b-d07c-48fa-9895-d87c3e31e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(pd.isna(content_split[598602]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46794f14-cc8b-48e7-a98b-d511c18c7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[598602], tag_split[598602], word_id_split[598602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90920008-e680-4c43-9d27-f75fde9a4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def tokenizer_all_data(content_split, tag_split, word_id_split):\n",
    "    # sanity check\n",
    "    if not (len(content_split)==len(tag_split)==len(word_id_split)):\n",
    "        print(\"check the input data\")\n",
    "        \n",
    "    content_split_token, tag_split_token, word_id_split_token = [], [], []\n",
    "    for i in tqdm(range(len((content_split)))):\n",
    "        #print(i)\n",
    "        content, tag, wordID = tokenizer_per_sentece(content_split[i], tag_split[i], word_id_split[i])\n",
    "        content_split_token.append(content)\n",
    "        tag_split_token.append(tag)\n",
    "        word_id_split_token.append(wordID)\n",
    "    return content_split_token, tag_split_token, word_id_split_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59c3a880-4f14-4954-842d-9124969a258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 703464/703464 [59:20<00:00, 197.58it/s]\n"
     ]
    }
   ],
   "source": [
    "content_split_tok, tag_split_tok, word_id_split_tok = tokenizer_all_data(content_split, tag_split, word_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3498240-0a4d-4fd8-b6dd-b8c71b7cf738",
   "metadata": {},
   "source": [
    "record\n",
    "- 20220527 : KoBert\n",
    "- 20220531 : Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "299c4c6c-fe6f-4135-a79d-ba43d7de315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/content_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(content_split_tok, fw)\n",
    "with open(\"data/tag_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag_split_tok, fw)\n",
    "with open(\"data/word_id_split_tok_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(word_id_split_tok, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e8b1c24-9a19-49df-b0e6-7fc3240fb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"data/content_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    content_split_tok = pickle.load(fr)\n",
    "with open(\"data/tag_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    tag_split_tok = pickle.load(fr)\n",
    "with open(\"data/word_id_split_tok_20220531.pickle\", \"rb\") as fr:\n",
    "    word_id_split_tok = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4189faa-ce10-4191-a745-71ac9875579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강', '사', '하면', '잘', '할', '스타', '는', '?', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "[tokenizer.decode(token) for token in content_split_tok[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae22c3-162a-424e-b5aa-ab39150a9720",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "184fa95c-b64f-42ba-92cc-180acaf4133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n",
      "6.924360502429632\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x) for x in content_split_tok]))\n",
    "print(sum([len(x) for x in content_split_tok])/len(content_split_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2728153-c5e6-4251-981b-70d26c4b094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_tensor(token_list, max_len=512):\n",
    "    '''\n",
    "    This function get a list of token lists and converts a input list into a list in size of max_len including special tokens.\n",
    "    \n",
    "    INPUT\n",
    "        token_list : a list of token lists\n",
    "            e.g. [[1,2,3,4], [5,6,7,8], [9, 10, 11, 12, 13, 14, 15]]\n",
    "        max_len : int\n",
    "    OUTPUT\n",
    "        token_ls : a list of lists in size of max_len with special tokens. The last token that does not fit the size would be dropped.\n",
    "            e.g. [[0, 1, 2, 3, 4, 0], [0, 5, 6, 7, 8, 0], [0, 9, 10, 11, 12, 0]]\n",
    "    '''\n",
    "    # setting\n",
    "    max_len = max_len-2 # to consider the special tokens\n",
    "    pad = 1 # an index of special token\n",
    "    bos = 2 # an index of special token\n",
    "    eos = 3 # an index of special token\n",
    "    token_output_ls = []\n",
    "    token_ls = []\n",
    "    #token_ls_iter_position = 0 # index of the last element in token_ls\n",
    "    \n",
    "    for index, tokens in enumerate(token_list):\n",
    "        n_token = len(tokens)\n",
    "        \n",
    "        if len(token_ls) + n_token <= max_len :\n",
    "            token_ls += tokens\n",
    "            #token_ls_iter_position += len(token_ls)\n",
    "        else : \n",
    "            cut_position = max_len - len(token_ls)\n",
    "            #assert cut_position >= 0\n",
    "            token_ls += tokens[:cut_position]\n",
    "\n",
    "            # to collect the current token_ls\n",
    "            token_ls = [bos] + token_ls + [eos] # to add special tokens\n",
    "\n",
    "            if not len(token_ls)-2 == max_len : \n",
    "                print(f'index: {index}')\n",
    "                print(f'cut_position: {cut_position}')\n",
    "                print(len(token_ls))\n",
    "            assert len(token_ls)-2 == max_len\n",
    "            token_output_ls.append(token_ls)\n",
    "\n",
    "            # to update token_ls for next iteration\n",
    "            token_ls = tokens[cut_position:]\n",
    "            #token_ls_iter_position = len(token_ls) - 1\n",
    "            \n",
    "            # if token_ls is longer than max_len, it requires an additional cleavage\n",
    "            while (len(token_ls) > max_len):\n",
    "                token_output_ls.append([bos] + token_ls[:max_len] + [eos])\n",
    "                token_ls = tokens[max_len:]\n",
    "    return token_output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "10767d22-2608-47d8-86ab-20031a71d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_input = prep_for_tensor(content_split_tok, 512)\n",
    "tag_input = prep_for_tensor(tag_split_tok, 512)\n",
    "word_id_input = prep_for_tensor(word_id_split_tok, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "288d7a60-178b-4797-a75c-40f11c6f4e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[807, 6493, 517, 6079, 5141, 3942, 7836, 833]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13819c43-23ab-49d8-a89c-41d1564df1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4656]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split_tok[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ec4f936-0dca-4b4d-94c0-bd9d4c760ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_padding(token_list, max_len=512):\n",
    "#     '''\n",
    "#     INPUT\n",
    "#         token_list : list\n",
    "#             e.g. [[1,2,3,4], [5,6,7,8]]\n",
    "#         max_len : int\n",
    "#     OUTPUT\n",
    "#         token_ls : list\n",
    "#     '''\n",
    "#     import copy\n",
    "#     token_ls = copy.deepcopy(token_list)\n",
    "#     max_len = max_len-2 # to consider the special tokens\n",
    "#     for index, tokens in enumerate(token_ls):\n",
    "#         n_token = len(tokens)\n",
    "#         pad = 1 # an index of special token\n",
    "#         bos = 2 # an index of special token\n",
    "#         eos = 3 # an index of special token\n",
    "#         # add padding if the length is shorter than max_len\n",
    "#         if n_token < max_len:\n",
    "#             token_ls[index] += [pad] * (max_len - n_token) # 부족한 만큼 padding을 추가함\n",
    "        \n",
    "#         # if a sentence is longer than max_len, cut a sentence.\n",
    "#         elif n_token > max_len:\n",
    "#             token_ls[index] = tokens[:max_len]\n",
    "        \n",
    "#         # to add begining token and end token\n",
    "#         token_ls[index] = [bos] + token_ls[index] + [eos]\n",
    "#     return token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76b16335-61db-44db-9e45-c12d94f35248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_input = add_padding(content_split_tok, 512)\n",
    "# tag_input = add_padding(tag_split_tok, 512)\n",
    "# word_id_input = add_padding(word_id_split_tok, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "243e0d30-0be9-4ea6-8a84-9d83c66b4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_input[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ae508eb5-a8f9-42ff-8abd-70dde8451bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/content_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(content_input, fw)\n",
    "with open(\"./data/tag_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(tag_input, fw)\n",
    "with open(\"./data/word_id_input_20220531.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(word_id_input, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1b5dc-001e-4f43-9e29-0c1bd8e3981f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# To load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893f374f-db34-4493-ba0f-a4df5e54aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"./data/voca2idex.pickle\", \"rb\") as fr:\n",
    "    voca2idex = pickle.load(fr)\n",
    "with open(\"./data/idex2voca.pickle\", \"rb\") as fr:\n",
    "    idex2voca = pickle.load(fr)\n",
    "with open(\"./data/tag2idex.pickle\", \"rb\") as fr:\n",
    "    tag2idex = pickle.load(fr)\n",
    "with open(\"./data/idex2tag.pickle\", \"rb\") as fr:\n",
    "    idex2tag = pickle.load(fr)\n",
    "    \n",
    "# configuration parameters\n",
    "VOCA_SIZE = len(voca2idex)\n",
    "print(VOCA_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c512bdb-1691-4ede-b318-0435c9930d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "with open(\"./data/content_input_20220531.pickle\", \"rb\") as fr:\n",
    "    content_input = pickle.load(fr)\n",
    "with open(\"./data/tag_input_20220531.pickle\", \"rb\") as fr:\n",
    "    tag_input = pickle.load(fr)\n",
    "with open(\"./data/word_id_input_20220531.pickle\", \"rb\") as fr:\n",
    "    word_id_input = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3344b-1a1f-48c0-8a25-54800a34acc5",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc21303-e2cc-409f-84f4-c6eecb14ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f60645-bdb5-42f7-88b9-2a7ccddf066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = torch.tensor(content_input)\n",
    "input_y = torch.tensor(tag_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cea956-097c-4bc1-bdbf-0ed119e81766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 13782, 75328, 67477,     0, 13189,     0, 75210, 73263, 13782,\n",
       "        75328,     0, 75210, 73263, 13782,     0, 73916, 60672,     0, 75210,\n",
       "        73263, 13782, 75328,     0, 75486, 74865, 74865,     0, 74865, 74397,\n",
       "        74865, 74397, 74865, 74865, 64762, 68574, 74865, 74865,     0, 74939,\n",
       "            0, 66726, 66726, 66726, 66726, 35799, 74778, 74778, 74874, 35799,\n",
       "        74778, 74874, 74778, 75206, 66063, 70938, 68762, 74703, 74623, 75206,\n",
       "        66063, 70938, 75494, 68762, 74703, 74623, 75206, 66063, 70938, 68762,\n",
       "        74703, 74623, 75197, 75206, 66063, 70938, 68762, 74703, 74623, 72157,\n",
       "        74865, 44274,     0, 72157, 74865, 44274, 72157, 75260, 74865, 44274,\n",
       "        72157, 43623, 74865, 44274,     0, 71938,     0,     0, 44972, 53063,\n",
       "            0,     0,     0, 44972,     0, 75112,     0,     0,     0, 44972,\n",
       "            0, 44972, 75112, 75112, 74865, 74865, 74865,     0, 74865, 74517,\n",
       "        72509,     0, 75256, 72509, 67432, 74517, 72509, 75256, 72509, 67432,\n",
       "            0,     0,     0, 74517, 72509, 75256, 72509, 67432, 74517, 72509,\n",
       "            0, 75256, 72509, 67432,     0,     0,     0, 72510,     0, 75241,\n",
       "        72510,     0, 75241, 75328,  1307, 75102, 72510,     0, 75241, 44555,\n",
       "        72510,     0, 75241,     0, 70962, 47694, 70962, 47694, 70962, 70962,\n",
       "        47694,     0,     0,     0,     0,     0,     0, 74599, 75014, 73793,\n",
       "            0,     0, 75105, 66877, 64078, 75461, 43044, 66877, 64078, 75461,\n",
       "        43044, 55651, 66877, 64078, 75461, 43044,     0, 55651, 66877, 64078,\n",
       "            0, 75461, 43044,     0, 71645, 75408, 71645, 75408, 61681, 71546,\n",
       "        71546, 71083, 61681, 74110,     0,     0,     0, 74759, 60508,     0,\n",
       "            0, 74759, 60508,     0, 75210, 47072, 74865, 75157, 40654, 75210,\n",
       "        47072, 74865, 75157, 40654, 75210, 47072, 74865, 75157, 40654,     0,\n",
       "        75210, 47072, 74865, 75157, 40654,     0,     0, 58039, 66840, 58039,\n",
       "            0, 68068,     0,     0,     0, 13353, 74366,     0, 13353, 74366,\n",
       "            0,     0, 13353, 74366,     0,     0, 13353, 74366, 43494, 75225,\n",
       "        69571,     0,  4840,  4840, 75560,     0,     0,  4840,     0,  4840,\n",
       "        71648,  2405, 59161,     0,     0, 71648,  2405, 59161, 71648,  2405,\n",
       "        59161, 71648,  2405, 59161,     0,     0, 72157, 72157, 72157, 16672,\n",
       "        72157, 29676,   329, 43044,     0, 29676,   329, 29676,   329,     0,\n",
       "        29676,   329, 75279, 43044,     0, 72157, 35799, 75348, 75357, 47694,\n",
       "        72157, 43592, 69775, 75357, 72157, 35799, 75348, 75357, 73793,     0,\n",
       "        72157, 75357, 47694,     0, 19985,     0,     0,     0, 59161, 59161,\n",
       "        67622, 59161, 75112, 75107, 74831, 75107, 59161, 75112, 75107, 74831,\n",
       "        75107, 75484,     0, 73087,     0,     0, 74865,     0, 12611,     0,\n",
       "        74865, 12611, 74865, 74356, 75289, 75348, 43592, 12611, 74865,     0,\n",
       "        74356, 75289, 75348, 43592, 12611, 72157, 41829, 72157, 41829, 72157,\n",
       "        41829, 72157,     0, 41829,     0,     0,  4244,     0, 75380, 62190,\n",
       "        72289, 21264,  4244, 75380, 62190, 72289, 21264, 75328,  4244, 72289,\n",
       "        72168, 75289, 21264, 75328,  4244, 72289, 72168, 75289,     0, 46150,\n",
       "            0,     0, 46150, 69674, 74049,     0, 46150, 57775,     0, 26727,\n",
       "        29676,     0, 74049,     0, 29676,     0, 29676,     0, 74299, 29676,\n",
       "            0, 55247,     0,  4611, 50935,  7692, 50935,  7692, 50935,     0,\n",
       "         7692,  4611, 50935,     0,     0,     0, 43592,     0,     0,     0,\n",
       "        43592, 75044,     0,     0,     0, 43592, 75044,     0,     0,     0,\n",
       "        75044, 75229, 75540,     0, 74865, 46090, 65960,     0,     0, 74865,\n",
       "        46090, 65960,     0,     0,     0,     0, 74865, 74438, 46090, 65960,\n",
       "            0,     0, 19397, 74959, 74865,     0, 46090, 65960,     0,     0,\n",
       "            0,     0, 72157, 37827, 74441, 72157, 37827, 74441, 74918, 72157,\n",
       "        37827, 74441, 72157, 37827, 74441, 56161,  7936, 19397, 56161,  7936,\n",
       "        19397,     3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4643717-f5cf-4866-b894-1bf87bd95868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    5,    6,    6, -100,    6,    6, -100,    6,    6, -100,    6,\n",
       "           5,    6, -100,    5, -100, -100, -100,    5,    6,    5,    6,    5,\n",
       "           5,    6,    5,    5,    5, -100,    6,    5,    5,    5,    6,    6,\n",
       "           5, -100,    6,    6,    5, -100,    6,    6,    5, -100,    6,    6,\n",
       "           5, -100, -100,    5,    6,    5,    6,    5,    5,    6,    6,    5,\n",
       "           6,    6,    6,    5,    6,    5,    5,    6,    6,    5,    5,    6,\n",
       "           5,    5,    6,    6,    6,    5, -100, -100,    6,    6,    5,    5,\n",
       "        -100, -100,    6,    6,    5,    5, -100, -100,    5,    6,    6,    5,\n",
       "           5, -100, -100,    6,    6,    6,    6,    6,    5,    6,    6,    6,\n",
       "           6,    6,    6,    6,    6,    5, -100,    6,    6,    6,    6,    5,\n",
       "        -100,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "           5,    6,    5, -100,    5,    5, -100,    5,    6,    5, -100,    5,\n",
       "           6,    5,    5,    5,    6,    5,    6, -100,    6, -100,    6, -100,\n",
       "           6, -100,    5,    6,    6,    6,    5,    5,    6,    6,    5, -100,\n",
       "           6,    6,    6,    6,    6,    6,    6,    6,    5,    6,    6,    6,\n",
       "           6,    6,    6,    6,    5,    6,    6,    6,    6,    5,    6,    6,\n",
       "           5,    6,    6,    5,    5,    5, -100,    6,    6,    6,    6, -100,\n",
       "           6,    6,    6,    6,    6, -100,    6,    6,    6,    6,    5,    6,\n",
       "        -100,    6,    6,    5,    5, -100,    6,    6,    6, -100,    5,    6,\n",
       "           6, -100,    6,    6,    6, -100,    5,    5,    6,    6,    6, -100,\n",
       "           6,    6,    6, -100,    5,    6,    6,    5, -100, -100,    6,    6,\n",
       "           6,    6,    6,    6,    5, -100,    6,    6,    5, -100,    5, -100,\n",
       "        -100,    5, -100,    5,    5, -100, -100,    5, -100,    6, -100,    6,\n",
       "           5,    6,    5,    6, -100,    6,    6,    6, -100,    6,    6,    6,\n",
       "        -100,    6,    6,    5,    6, -100,    6,    6, -100,    5,    5,    5,\n",
       "           6,    5, -100,    6, -100,    6,    5, -100,    6, -100,    5,    6,\n",
       "           5, -100,    6,    5, -100,    6,    6,    5,    6,    5,    6,    5,\n",
       "        -100,    6,    6,    5, -100,    6,    6,    5, -100,    5, -100, -100,\n",
       "           6,    5,    5, -100, -100,    5, -100,    6,    5,    5, -100,    6,\n",
       "           5,    5,    5, -100,    6,    6, -100,    6,    6, -100,    6,    6,\n",
       "        -100,    5,    6,    6, -100,    6,    6,    6,    5,    6,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5, -100,    5, -100,    5, -100,    5,\n",
       "        -100,    6,    5,    6,    6,    5,    6,    6,    6,    5,    6,    5,\n",
       "           5,    6,    5,    6,    6,    6,    5,    6,    6,    5,    6,    5,\n",
       "           6,    6,    6,    6,    6,    5,    6,    6,    6, -100,    6,    6,\n",
       "        -100,    6,    5,    6,    6, -100,    6,    6, -100,    6,    5, -100,\n",
       "           6,    6, -100,    6,    6, -100,    6,    5,    6,    6, -100,    5,\n",
       "           6,    6, -100,    6,    5,    5,    5,    5,    5,    6,    6,    6,\n",
       "           6,    6,    6,    5,    5,    5,    6,    5,    6,    6,    5,    6,\n",
       "           6,    6,    5,    5,    5, -100,    5, -100,    6,    6,    5,    6,\n",
       "           6,    6,    6,    5,    6,    5, -100, -100,    6,    5, -100,    5,\n",
       "        -100,    5,    5,    5, -100,    5,    5,    5,    5, -100,    5, -100,\n",
       "           6,    6, -100,    6,    5,    6,    6,    3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_y[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f8101c-e84b-4a07-864e-a32064d469cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6064\n",
      "4851\n",
      "606\n",
      "607\n",
      "sanity check:  True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size=8\n",
    "total_dataset = TensorDataset(input_x, input_y)\n",
    "\n",
    "num_train = int(len(total_dataset)*0.8)\n",
    "num_validation = int(len(total_dataset)*0.1)\n",
    "num_test = len(total_dataset)-num_train-num_validation\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(total_dataset, [num_train, num_validation, num_test])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "print(len(total_dataset))\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(test_dataset))\n",
    "print('sanity check: ', len(total_dataset)==len(train_dataset)+len(validation_dataset)+len(test_dataset))\n",
    "\n",
    "\n",
    "len(train_loader)\n",
    "len(validation_loader)\n",
    "len(test_loader)\n",
    "\n",
    "del total_dataset, num_train, num_validation, num_test, train_dataset, validation_dataset, test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
