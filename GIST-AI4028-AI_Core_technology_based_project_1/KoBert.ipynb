{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "- This jupyter notebook is designed for named entity recognition. Although some codes were omitted for server security,  all necessary codes are included.\n",
    "- Point of contact: Jihun Jeung, jihun@gm.gist.ac.kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: mxnet in /usr/local/lib/python3.8/dist-packages (1.7.0.post1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.19.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.25.1)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gluonnlp in ./.local/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.55.1)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from gluonnlp) (21.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.19.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.13)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp) (2.4.6)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.94)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.8/site-packages (4.19.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.55.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.7.1+cu110)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-wc2f0lq0/kobert-tokenizer_c8334ac4ce3741518a4c6a241b93439d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-wc2f0lq0/kobert-tokenizer_c8334ac4ce3741518a4c6a241b93439d\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit e1f2f37055e7460d8427f6912579c0162cb69831\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://github.com/naver/nlp-challenge/blob/master/missions/ner/data/train/train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/train_data.txt', sep='\\t',  header=None)\n",
    "# data.columns = ['index' , 'content', 'tag']\n",
    "# data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/indexingFile.csv', sep=',',  header=0)\n",
    "data.columns = ['number', 'index' , 'content', 'tag']\n",
    "data = data.drop(['number'], axis=1)\n",
    "data['word_id'] = [k for k in range(len(data['content']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>활동</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>잘할</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>것</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>같은</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>스타는?.</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>갤러</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>리</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>정보</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>모음.</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>커피</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>갤</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>임시</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>규정</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>(</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>주딱</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>구인</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>중).</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index content  tag  word_id\n",
       "0       1      강사    1        0\n",
       "1       2      활동    0        1\n",
       "2       3      하면    0        2\n",
       "3       4      잘할    0        3\n",
       "4       5       것    0        4\n",
       "5       6      같은    0        5\n",
       "6       7   스타는?.    0        6\n",
       "7       1      커피    1        7\n",
       "8       2      갤러    0        8\n",
       "9       3       리    0        9\n",
       "10      4      정보    0       10\n",
       "11      5     모음.    0       11\n",
       "12      1      커피    1       12\n",
       "13      2       갤    0       13\n",
       "14      3      임시    0       14\n",
       "15      4      규정    0       15\n",
       "16      5       (    0       16\n",
       "17      6      주딱    0       17\n",
       "18      7      구인    0       18\n",
       "19      8     중).    0       19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split by a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_split = []\n",
    "tag_split = []\n",
    "word_id_split = []\n",
    "\n",
    "content_spliter = []\n",
    "tag_spliter = []\n",
    "word_spliter = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    content_spliter.append(data['content'].iloc[i])\n",
    "    tag_spliter.append(data['tag'].iloc[i])\n",
    "    word_spliter.append(data['word_id'].iloc[i])\n",
    "    if (i == len(data)-1) : # the last word in a data\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "    elif (data['index'].iloc[i+1] == 1) : # the last word in a sentence\n",
    "        content_split.append(content_spliter)\n",
    "        content_spliter = []\n",
    "        tag_split.append(tag_spliter)\n",
    "        tag_spliter = []\n",
    "        word_id_split.append(word_spliter)\n",
    "        word_spliter = []\n",
    "        \n",
    "# sanity check\n",
    "len(content_split) == len(tag_split) == len(word_id_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31249"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag integer encodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = data['tag'].unique()\n",
    "num_tags = len(tag_list)\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idex2tag = {}\n",
    "idex2tag[0]='[UNK]'\n",
    "idex2tag[1]='[PAD]'\n",
    "idex2tag[2]='[CLS]'\n",
    "idex2tag[3]='[SEP]'\n",
    "idex2tag[4]='[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idex = {'[UNK]':0,'[PAD]':1,'[CLS]':2,'[SEP]':3,'[MASK]':4, 0:5, 1:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# seq_to_index function\n",
    "def tag_to_index(sequence, embedding_index):\n",
    "    '''\n",
    "    INPUT\n",
    "        sequence : string list\n",
    "    OUTPUT\n",
    "        integer list\n",
    "    '''\n",
    "    converted_index = [embedding_index[word] if word in embedding_index.keys() else embedding_index['[UNK]'] for word in sequence]\n",
    "    return converted_index\n",
    "\n",
    "# index2gene\n",
    "idex2tag = {v:k for k,v in tag2idex.items()}\n",
    "idex2tag[0]='[UNK]'\n",
    "idex2tag[1]='[PAD]'\n",
    "idex2tag[2]='[CLS]'\n",
    "idex2tag[3]='[SEP]'\n",
    "idex2tag[4]='[MASK]'\n",
    "#print(list(gene2idex.keys())[list(gene2idex.values()).index(16)]) \n",
    "print(f'{idex2tag[1]}')\n",
    "\n",
    "print(tag_to_index(['5', '6'], tag2idex)) # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [PAD] : 1\n",
    "- [CLS] : 2\n",
    "- [SEP] : 3\n",
    "- [MASK] : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43fcbabb2cd442284183039d4b10c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e350fbb2ed1d4a419d826b3c7708061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a4ab5eedaa4cef961cc3d69dd82d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import gluonnlp as nlp\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'강사'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['content'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강사', '활동', '하면', '잘할', '것']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['content'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(list(data['content'][0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 807,\n",
       " 6493,\n",
       " 5141,\n",
       " 4939,\n",
       " 3942,\n",
       " 7836,\n",
       " 905,\n",
       " 833,\n",
       " 2938,\n",
       " 5760,\n",
       " 258,\n",
       " 54,\n",
       " 4656,\n",
       " 517,\n",
       " 5371,\n",
       " 6037,\n",
       " 1900,\n",
       " 4103,\n",
       " 2044,\n",
       " 7089,\n",
       " 54,\n",
       " 4656,\n",
       " 517,\n",
       " 5371,\n",
       " 3831,\n",
       " 1182,\n",
       " 522,\n",
       " 4213,\n",
       " 5960,\n",
       " 1115,\n",
       " 7119,\n",
       " 4257,\n",
       " 40,\n",
       " 54,\n",
       " 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '강',\n",
       " '사',\n",
       " '활동',\n",
       " '하면',\n",
       " '잘',\n",
       " '할',\n",
       " '것',\n",
       " '같은',\n",
       " '스타',\n",
       " '는',\n",
       " '?',\n",
       " '.',\n",
       " '커피',\n",
       " '',\n",
       " '갤',\n",
       " '러',\n",
       " '리',\n",
       " '정보',\n",
       " '모',\n",
       " '음',\n",
       " '.',\n",
       " '커피',\n",
       " '',\n",
       " '갤',\n",
       " '임시',\n",
       " '규정',\n",
       " '(',\n",
       " '주',\n",
       " '딱',\n",
       " '구',\n",
       " '인',\n",
       " '중',\n",
       " ')',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(token) for token in tokenizer.encode(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>강사</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>활동</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>하면</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>잘할</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>것</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177392</th>\n",
       "      <td>3</td>\n",
       "      <td>게샤</td>\n",
       "      <td>1</td>\n",
       "      <td>177392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177393</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>177393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177394</th>\n",
       "      <td>5</td>\n",
       "      <td>-GW</td>\n",
       "      <td>0</td>\n",
       "      <td>177394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177395</th>\n",
       "      <td>6</td>\n",
       "      <td>워시드</td>\n",
       "      <td>1</td>\n",
       "      <td>177395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177396</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>177396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177397 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index content  tag  word_id\n",
       "0           1      강사    1        0\n",
       "1           2      활동    0        1\n",
       "2           3      하면    0        2\n",
       "3           4      잘할    0        3\n",
       "4           5       것    0        4\n",
       "...       ...     ...  ...      ...\n",
       "177392      3      게샤    1   177392\n",
       "177393      4       6    1   177393\n",
       "177394      5     -GW    0   177394\n",
       "177395      6     워시드    1   177395\n",
       "177396      7       .    0   177396\n",
       "\n",
       "[177397 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177397"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'강사'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 807, 6493, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(data['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              0\n",
       "1              1\n",
       "2              2\n",
       "3              3\n",
       "4              4\n",
       "           ...  \n",
       "177392    177392\n",
       "177393    177393\n",
       "177394    177394\n",
       "177395    177395\n",
       "177396    177396\n",
       "Name: word_id, Length: 177397, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "5    5\n",
       "6    6\n",
       "7    7\n",
       "8    8\n",
       "9    9\n",
       "Name: word_id, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word_id'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tag'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idex[data['tag'].iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_per_sentece(content_list, tag_list, word_id_list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        content_split_list : a list that represents a sentence \n",
    "            e.g. ['비토리오', '양일', '만에', '영사관', '감호', '용퇴,', '항룡', '압력설', '의심만', '가율']\n",
    "        tag_split_list\n",
    "            e.g. ['PER_B', 'DAT_B', '-', 'ORG_B', 'CVL_B', '-', '-', '-', '-', '-']\n",
    "        word_id_split_list\n",
    "            e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    OUTPUT: tokenzier results\n",
    "        content_tokenizer : a tokenized content list without special tokens\n",
    "        word_id_tokenizer : a word id list\n",
    "        tag_id_tokenizer : a tag id list that the only first content has a tag.\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    if not (len(content_list)==len(tag_list)==len(word_id_list)):\n",
    "        print(\"check the input data\")\n",
    "    \n",
    "    # tokenizer\n",
    "    content_tokenizer = []\n",
    "    word_id_tokenizer = []\n",
    "    tag_id_tokenizer = []\n",
    "    for i in range(len(word_id_list)):\n",
    "        # tokenizer\n",
    "        content_tokenizer += tokenizer.encode(content_list[i])[1:-1] # to remove special tokens [CLS] and [SEP]\n",
    "        word_id_tokenizer += [word_id_list[i]] * (len(tokenizer.encode(content_list[i]))-2)\n",
    "        tag_id_tokenizer += [tag2idex[tag_list[i]]] + [-100] * (len(tokenizer.encode(content_list[i]))-3)\n",
    "    \n",
    "    if not (len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)):\n",
    "        print(f'sanity check : {len(content_tokenizer) == len(word_id_tokenizer) == len(tag_id_tokenizer)}')\n",
    "    return content_tokenizer, tag_id_tokenizer, word_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenizer_per_sentece(content_split[300], tag_split[300], word_id_split[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>word_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "      <td>6</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7490</td>\n",
       "      <td>-100</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7491</td>\n",
       "      <td>-100</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6392</td>\n",
       "      <td>-100</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3084</td>\n",
       "      <td>5</td>\n",
       "      <td>1693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6198</td>\n",
       "      <td>-100</td>\n",
       "      <td>1693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3537</td>\n",
       "      <td>5</td>\n",
       "      <td>1694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>517</td>\n",
       "      <td>5</td>\n",
       "      <td>1695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54</td>\n",
       "      <td>-100</td>\n",
       "      <td>1695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  tag_id  word_ids\n",
       "0    517       6      1692\n",
       "1   7490    -100      1692\n",
       "2   7491    -100      1692\n",
       "3   6392    -100      1692\n",
       "4   3084       5      1693\n",
       "5   6198    -100      1693\n",
       "6   3537       5      1694\n",
       "7    517       5      1695\n",
       "8     54    -100      1695"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenizer = pd.DataFrame({'token': a, 'tag_id': b, 'word_ids': c})\n",
    "data_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def tokenizer_all_data(content_split, tag_split, word_id_split):\n",
    "    # sanity check\n",
    "    if not (len(content_split)==len(tag_split)==len(word_id_split)):\n",
    "        print(\"check the input data\")\n",
    "        \n",
    "    content_split_token, tag_split_token, word_id_split_token = [], [], []\n",
    "    for i in tqdm(range(len((content_split)))):\n",
    "        content, tag, wordID = tokenizer_per_sentece(content_split[i], tag_split[i], word_id_split[i])\n",
    "        content_split_token.append(content)\n",
    "        tag_split_token.append(tag)\n",
    "        word_id_split_token.append(wordID)\n",
    "    return content_split_token, tag_split_token, word_id_split_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 31249/31249 [00:32<00:00, 967.02it/s]\n"
     ]
    }
   ],
   "source": [
    "content_split_tok, tag_split_tok, word_id_split_tok = tokenizer_all_data(content_split, tag_split, word_id_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강', '사', '활동', '하면', '잘', '할', '것', '같은', '스타', '는', '?', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "[tokenizer.decode(token) for token in content_split_tok[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "12.790169285417134\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x) for x in content_split_tok]))\n",
    "print(sum([len(x) for x in content_split_tok])/len(content_split_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(token_list, max_len=512):\n",
    "    '''\n",
    "    INPUT\n",
    "        token_list : list\n",
    "            e.g. [[1,2,3,4], [5,6,7,8]]\n",
    "        max_len : int\n",
    "    OUTPUT\n",
    "        token_ls : list\n",
    "    '''\n",
    "    import copy\n",
    "    token_ls = copy.deepcopy(token_list)\n",
    "    max_len = max_len-2 # to consider the special tokens\n",
    "    for index, tokens in enumerate(token_ls):\n",
    "        n_token = len(tokens)\n",
    "        pad = 1 # an index of special token\n",
    "        bos = 2 # an index of special token\n",
    "        eos = 3 # an index of special token\n",
    "        # add padding if the length is shorter than max_len\n",
    "        if n_token < max_len:\n",
    "            token_ls[index] += [pad] * (max_len - n_token) # 부족한 만큼 padding을 추가함\n",
    "        \n",
    "        # if a sentence is longer than max_len, cut a sentence.\n",
    "        elif n_token > max_len:\n",
    "            token_ls[index] = tokens[:max_len]\n",
    "        \n",
    "        # to add begining token and end token\n",
    "        token_ls[index] = [bos] + token_ls[index] + [eos]\n",
    "    return token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_input = add_padding(content_split_tok, 512)\n",
    "tag_input = add_padding(tag_split_tok, 512)\n",
    "word_id_input = add_padding(word_id_split_tok, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_input[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# To load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = torch.tensor(content_input)\n",
    "input_y = torch.tensor(tag_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31249\n",
      "24999\n",
      "3124\n",
      "3126\n",
      "sanity check:  True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size=8\n",
    "total_dataset = TensorDataset(input_x, input_y)\n",
    "\n",
    "num_train = int(len(total_dataset)*0.8)\n",
    "num_validation = int(len(total_dataset)*0.1)\n",
    "num_test = len(total_dataset)-num_train-num_validation\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(total_dataset, [num_train, num_validation, num_test])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "print(len(total_dataset))\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(test_dataset))\n",
    "print('sanity check: ', len(total_dataset)==len(train_dataset)+len(validation_dataset)+len(test_dataset))\n",
    "\n",
    "\n",
    "len(train_loader)\n",
    "len(validation_loader)\n",
    "len(test_loader)\n",
    "\n",
    "del total_dataset, num_train, num_validation, num_test, train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = seq_length = 512\n",
    "num_tag = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "pretrained_model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class KoBERTNER(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768, #768\n",
    "                 num_labels=7,\n",
    "                 max_length=512, \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(KoBERTNER, self).__init__()\n",
    "        self.bert = bert # Load model body\n",
    "        self.dr_rate = dr_rate\n",
    "        self.max_length = max_length\n",
    "        # Set up token classification head\n",
    "        self.num_labels = num_labels # num_tags\n",
    "        self.classifier = nn.Linear(hidden_size, self.max_length*self.num_labels)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "            \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask) #[batch_size, hidden_dim]\n",
    "            # ouitput[0] : the attention weights\n",
    "            # output[1] : the output from the model\n",
    "        # Apply classifier to encoder representation\n",
    "        output = self.dropout(output[1])\n",
    "        output = self.classifier(output) #[batch_size, max_length*num_label]\n",
    "        output = output.view([-1, self.num_labels, self.max_length]) # [batch_size, num_label, max_length]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = KoBERTNER(pretrained_model, dr_rate=0.3).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, max_length=max_length, DEVICE='cuda'):\n",
    "    import gc\n",
    "    from tqdm import tqdm\n",
    "    model.train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    losses = 0\n",
    "    num_correct = []\n",
    "    num_test = 0\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for src, tgt in data_loader:\n",
    "        src = src.type(torch.LongTensor).to(DEVICE)\n",
    "        tgt = tgt.type(torch.LongTensor).to(DEVICE)\n",
    "        logits = model(input_ids= src, attention_mask= torch.tensor([[1]*max_length]*batch_size).to(DEVICE))\n",
    "            #torch.Size([batch_size, , num_tag, src_seq_length])\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        # accuracy\n",
    "        _, pred_word = torch.max(logits, dim=1)\n",
    "        num_correct.append(int((torch.flatten(tgt)==torch.flatten(pred_word)).sum()))\n",
    "        num_test += int(torch.flatten(pred_word).size(0))\n",
    "        \n",
    "    accuracy = sum(num_correct) / num_test\n",
    "    \n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    " \n",
    "    return losses / len(data_loader), accuracy\n",
    "\n",
    "\n",
    "def test_epoch(model, optimizer, data_loader, max_length=max_length, DEVICE='cuda'):\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        losses = 0\n",
    "        num_correct = []\n",
    "        num_test = 0\n",
    "        y_probability = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        y_true = np.array([])\n",
    "        for src, tgt in data_loader:\n",
    "            src = src.type(torch.LongTensor).to(DEVICE)\n",
    "            tgt = tgt.type(torch.LongTensor).to(DEVICE)\n",
    "            logits = model(input_ids= src, attention_mask= torch.tensor([[1]*max_length]*batch_size).to(DEVICE))\n",
    "                #torch.Size([batch_size, , num_tag, src_seq_length])\n",
    "            \n",
    "            loss = loss_fn(logits, tgt)\n",
    "            losses += loss.item()\n",
    "            \n",
    "            # accuracy\n",
    "            _, pred_word = torch.max(logits, dim=1)\n",
    "            num_correct.append(int((torch.flatten(tgt)==torch.flatten(pred_word)).sum()))\n",
    "            num_test += int(torch.flatten(pred_word).size(0))\n",
    "            \n",
    "            y_probability = np.append(y_probability, logits.cpu().detach().numpy())\n",
    "            y_pred = np.append(y_pred, pred_word.cpu().detach().numpy())\n",
    "            y_true = np.append(y_true, tgt.cpu().detach().numpy())\n",
    "            \n",
    "    accuracy = sum(num_correct) / num_test\n",
    "    # garbage collector\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return losses / len(data_loader), accuracy,  y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_sample(model, optimizer, max_length, train_loader, validation_loader, test_loader, device, total_num_epoch, running_num_epoch, tf_board_directory, model_save_directory, best_model_save_directory=None, slack_message=False):\n",
    "    '''\n",
    "    Parameters:\n",
    "        train_loader\n",
    "        validation_loader\n",
    "        test_loader\n",
    "        total_num_epoch : the total number of epoch that have run\n",
    "        running_num_epoch : the number of epoch that run in this time\n",
    "        model_save_directory : save directory to save the final model\n",
    "        best_model_save_directory : [optional] save directory to save the best model (best validation accuracy)\n",
    "    '''\n",
    "  \n",
    "\n",
    "    if (total_num_epoch < 0 or running_num_epoch <= 0):\n",
    "        import sys\n",
    "        sys.exit(\"Check the number of epoch. It is incorrect\")\n",
    "\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    total_time_start = time.time() # to measure time\n",
    "    best_validation_accuracy = None\n",
    "    for epoch in tqdm(range(1, running_num_epoch+1)):\n",
    "        epoch_time_start = time.time() # to measure time\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        loss, train_accuracy = train_epoch(model, optimizer, train_loader, max_length=max_length)\n",
    "        _, validation_accuracy, _, _ = test_epoch(model, optimizer, validation_loader, max_length=max_length)\n",
    "\n",
    "        # to save the metrics and model\n",
    "        if best_validation_accuracy is None or validation_accuracy >= best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            if not(best_model_save_directory is None):            \n",
    "                torch.save({\n",
    "                'epoch': total_num_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, best_model_save_directory)\n",
    "        _, test_accuracy, _, _ = test_epoch(model, optimizer, test_loader, max_length=max_length)\n",
    "        total_num_epoch = total_num_epoch + 1\n",
    "        epoch_time_end = time.time() # to measure time    \n",
    "   \n",
    "    total_time_finish = time.time() # to measure time\n",
    "    print(f'Done. Total running Time: {total_time_finish - total_time_start}')\n",
    "    writer.close() #tensorboard : if close() is not declared, the writer does not save any valeus.\n",
    "\n",
    "    # model save\n",
    "    torch.save({\n",
    "            'epoch': total_num_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, model_save_directory)\n",
    "    print('total number of epoches : ', total_num_epoch)\n",
    "    print(\"-------------------------done------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'run_00'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.8, patience=10, min_lr=1e-4)\n",
    "\n",
    "train_all_sample(model=model,\n",
    "                optimizer=optimizer,\n",
    "                max_length=max_length,\n",
    "                train_loader=train_loader, \n",
    "                validation_loader=validation_loader, \n",
    "                test_loader=test_loader, \n",
    "                device='cuda', \n",
    "                total_num_epoch=0, \n",
    "                running_num_epoch=300, \n",
    "                tf_board_directory = 'tfboard/'+save_directory, \n",
    "                model_save_directory='model/'+save_directory, \n",
    "                best_model_save_directory='model/'+save_directory+'_best',\n",
    "                slack_message = True)\n",
    "del save_directory\n",
    "slack_alarm('The program execution on JupyterNotebook is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# To load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "pretrained_model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = KoBERTNER(pretrained_model, dr_rate=0.3).to(DEVICE)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0015334791245047435)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.8, patience=5, min_lr=0.00001)\n",
    "#optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = 'model/run_00_best'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, pred, true = test_epoch(model=model, optimizer=optimizer, data_loader= test_loader, max_length=max_length, DEVICE='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019896473274685633"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 2., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 2.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    3.,    2.,    6., -100.,\n",
       "       -100., -100.,    6., -100., -100.,    6.,    6.,    5., -100.,\n",
       "       -100., -100., -100., -100.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    3.,    2.,    6., -100., -100.,\n",
       "          5., -100.,    5.,    5.,    5.,    5., -100., -100.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n",
       "          1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[1000:2000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
