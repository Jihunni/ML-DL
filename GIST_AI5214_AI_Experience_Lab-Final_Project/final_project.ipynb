{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05554d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006fc66-e114-4580-a948-1dac3148f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6ce480-15bb-401c-8a86-701ceb330969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.10.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfe8712-e01b-4703-b972-24e44c802d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56499c55-6990-4635-a74f-d864feb40c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607ae13a-9599-4e0e-9787-cf7057b8f2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecb998b-0410-4379-a260-b7c2b72d02f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d28a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932cd54-eb52-47bf-a0c7-054c12590c8b",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42737d-1cb2-431c-9034-e48bf90681ea",
   "metadata": {},
   "source": [
    "tensorboard --logdir=./python/run/GAT_Net/run_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4b7fc-69a6-497d-9299-b4ae4d91f989",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9786f-e842-4e27-9a91-d3d15dffec94",
   "metadata": {},
   "source": [
    "QM9\n",
    "- dataset reference :  “MoleculeNet: A Benchmark for Molecular Machine Learning” \n",
    "- dataset link : https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b191a1-ed6a-4f34-836d-d49c67fd419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.datasets.qm9.QM9'>\n",
      "Dataset: QM9(130831):\n",
      "======================\n",
      "Number of graphs: 130831\n",
      "Number of features: 11\n",
      "Number of classes: 19\n"
     ]
    }
   ],
   "source": [
    "# load example dataset\n",
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "dataset = QM9(root='/home/data/QM9')\n",
    "print(type(dataset))\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# #shuffle the dataset\n",
    "# option 1 :\n",
    "# dataset = dataset.shffule()\n",
    "# option 2 :\n",
    "# perm = torch.randperm(len(dataset))\n",
    "# dataset_random = dataset[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aed791-3d5b-445d-9b22-481d663e6d08",
   "metadata": {},
   "source": [
    "The QM9 dataset from the `\"MoleculeNet: A Benchmark for Molecular Machine Learning\" <https://arxiv.org/abs/1703.00564>` paper, consisting of about 130,000 molecules with 19 regression targets. Each molecule includes complete spatial information for the single low energy conformation of the atoms in the molecule. In addition, we provide the atom features from the `\"Neural Message Passing for Quantum Chemistry\" <https://arxiv.org/abs/1704.01212>` paper. In short, the dataset contains the (simplified) molecules composed of H, C, O, N, F. \n",
    "\n",
    "- 'name' contains the name of graph\n",
    "- 'idx' contains the index of graph\n",
    "- each node is a molecule.\n",
    "- each node features is [H, C, N, O , F, atomic_number, aromatic, sp, sp2, sp3, num_hs]\n",
    "    - one_hot_encoding (5 dim) + molecular properties (6 dim) = 11 dim\n",
    "    - H, C, N, O, F : one hot encoding\n",
    "    - aromatic : if armoatic, 1. Otherwise, 1\n",
    "    - sp : if sp, 1. Otherwise, 1\n",
    "    - num_hs : the number of Hydrogen atoms\n",
    "- each edge is a bond between two atoms\n",
    "- each edge features is one hot encoder {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "- 'z' contains atom number.\n",
    "- 'pos' contains 3 dimensional coordinate position. [3, # of nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc84a609-dd78-4fd3-abd0-60d09cd2d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499f6f3-1f51-482c-93cf-c1cb0fd68988",
   "metadata": {},
   "source": [
    "# data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14cc38e9-a58c-48ab-80b3-4e41ff507ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], z=[5], name='gdb_1', idx=[1])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print(data) # A graph contains 5 nodes (11 features), 8 edges (4 features). A label for graph contains 19 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1c624-5d44-41aa-b165-bbd85ed9d358",
   "metadata": {},
   "source": [
    "5 atoms, 8/2 (undirected) bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348b8065-6201-4296-a801-ddadbd515e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x found in data\n",
      "edge_index found in data\n",
      "edge_attr found in data\n",
      "y found in data\n",
      "pos found in data\n",
      "z found in data\n",
      "name found in data\n",
      "idx found in data\n"
     ]
    }
   ],
   "source": [
    "for key, item in data:\n",
    "    print(\"{} found in data\".format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd633e6-7831-43a5-9259-cdad9e6acfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdb_1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c96be6-8a20-43f5-a895-82f80a5b4b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709850fe-2b35-4d30-a7f9-93f6b533a5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c15e1b-7a5c-41e1-8293-2b6538a804a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2df2b-9055-47b3-bf1e-0534b7e70075",
   "metadata": {},
   "source": [
    "CH4  \n",
    "Note that SP3 for first node sholud be 1. Data set has error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87d5212-232b-46ec-8817-bf6dab798672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0000,    13.2100,   -10.5499,     3.1865,    13.7363,    35.3641,\n",
       "             1.2177, -1101.4878, -1101.4098, -1101.3840, -1102.0229,     6.4690,\n",
       "           -17.1722,   -17.2868,   -17.3897,   -16.1519,   157.7118,   157.7100,\n",
       "           157.7070]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822e25e5-f60e-4b7d-a38d-acb0653b091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 2, 3, 4],\n",
       "        [1, 2, 3, 4, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2fb649-db39-4603-9d93-fe029215ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "to_dense_adj(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165e5dca-91b9-40f9-9754-68fe2a66cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select the source of edges\n",
    "indices = torch.tensor([0]) \n",
    "source = torch.index_select(data.edge_index, 0, indices)\n",
    "\n",
    "# to select the target of edges\n",
    "indices = torch.tensor([1]) \n",
    "target = torch.index_select(data.edge_index, 0, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f1df38-ae34-44eb-8535-046a4a5d8d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d529c5c-bfb1-498a-b372-2bf1665c1872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f43ec2-f658-4b21-a98f-9341eb6450a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.index_select(data.x, 0, source.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0dd24f7-4bdc-4d37-bce0-c2bdd02c134a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 11])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d04aa1f-bc31-408a-b522-8037264094a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16910a36-89f4-413f-a2fa-7a43164ffaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40682f2d-6b91-434f-9022-a5960f5e7adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.index_select(data.x, 0, source.squeeze()), data.edge_attr), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eba3bcec-d516-49b5-a6ce-28ee23dce5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((data.edge_attr, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae38756e-2c98-469f-88f9-344d69cfeb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fffc82d8-34ad-4561-9944-50a234a1e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59f7656b-a15f-4c05-8f7f-31d0e05f02ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2700e-02,  1.0858e+00,  8.0000e-03],\n",
       "        [ 2.2000e-03, -6.0000e-03,  2.0000e-03],\n",
       "        [ 1.0117e+00,  1.4638e+00,  3.0000e-04],\n",
       "        [-5.4080e-01,  1.4475e+00, -8.7660e-01],\n",
       "        [-5.2380e-01,  1.4379e+00,  9.0640e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "474ff27f-07e7-4222-baca-8938562809eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88e234-5658-4d99-9eeb-eafec6be9201",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040ef59-9670-4653-9cf4-f18a5272d2ea",
   "metadata": {},
   "source": [
    "To select Free energy among data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38712ac5-95e6-4e1b-bc44-f514b0681494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of total data :  130831\n",
      "sanity check :  True\n",
      "the number of train set :  104665\n",
      "the number of validation set :  13083\n",
      "the number of test set :  13083\n"
     ]
    }
   ],
   "source": [
    "# shuffle the dataset\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# normalize\n",
    "\n",
    "\n",
    "# split the dataset\n",
    "train_set = dataset[:round(dataset.len()*0.8)]\n",
    "validation_set = dataset[round(dataset.len()*0.8):round(dataset.len()*0.9)]\n",
    "test_set = dataset[round(dataset.len()*0.9):]\n",
    "\n",
    "print(\"the number of total data : \", dataset.len())\n",
    "print(\"sanity check : \", dataset.len()  == len(train_set) + len(validation_set) + len(test_set))\n",
    "\n",
    "print(\"the number of train set : \", len(train_set))\n",
    "print(\"the number of validation set : \",len(validation_set))\n",
    "print(\"the number of test set : \",len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2701e382-4a12-414d-bf01-34fa8e3209c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last = True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=32, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba36b972-fd13-4659-91a7-5dd912e59e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c4892-0372-4ff9-a20a-1a503cfa99f9",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf6e910-cf7a-4539-b97f-71db4ecba8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "#from torch_geometric.nn import global_mean_pool\n",
    "#from torch_geometric.nn import global_max_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.num_node_features = train_set.num_node_features\n",
    "        self.num_hidden = 80\n",
    "        \n",
    "        # model structure\n",
    "        self.conv1 = GCNConv(self.num_node_features, self.num_hidden)\n",
    "        self.conv2 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv3 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv4 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv5 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.lin1 = Linear(self.num_hidden, self.num_hidden)\n",
    "        self.lin2 = Linear(self.num_hidden, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "#         x = self.conv4(x, edge_index)\n",
    "#         x = F.elu(x)\n",
    "        \n",
    "#         x = self.conv5(x, edge_index)\n",
    "#         x = F.elu(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "            #[32, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b90a67f-dba2-47d9-b073-b2063043f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path as osp\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "# #from torch_geometric.nn import GCNConv\n",
    "# #from torch_geometric.nn import SAGEConv\n",
    "# from torch_geometric.nn import GINConv, global_add_pool\n",
    "# #from torch_geometric.nn import global_mean_pool\n",
    "# #from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import GINConv, global_add_pool, SAGPooling\n",
    "\n",
    "\n",
    "class GINConv(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # parameters\n",
    "#         self.in_channels = in_channels # train_set.num_node_features\n",
    "#         self.hidden_channels = hidden_channels  \n",
    "#         self.out_channels = out_channels # 16\n",
    "        \n",
    "#         # model structure\n",
    "#         self.conv1 = GINConv(\n",
    "#             Sequential(Linear(self.in_channels, self.hidden_channels), BatchNorm1d(self.hidden_channels), ReLU(),\n",
    "#                        Linear(self.hidden_channels, self.hidden_channels), ReLU()))\n",
    "#         self.conv2 = GINConv(\n",
    "#             Sequential(Linear(self.hidden_channels, self.hidden_channels), BatchNorm1d(self.hidden_channels), ReLU(),\n",
    "#                        Linear(self.hidden_channels, self.hidden_channels), ReLU()))\n",
    "#         self.conv3 = GINConv(\n",
    "#             Sequential(Linear(self.hidden_channels, self.hidden_channels), BatchNorm1d(self.hidden_channels), ReLU(),\n",
    "#                        Linear(self.hidden_channels, self.hidden_channels), ReLU()))\n",
    "#         self.conv4 = GINConv(\n",
    "#             Sequential(Linear(self.hidden_channels, self.hidden_channels), BatchNorm1d(self.hidden_channels), ReLU(),\n",
    "#                        Linear(self.hidden_channels, self.hidden_channels), ReLU()))\n",
    "#         self.conv5 = GINConv(\n",
    "#             Sequential(Linear(self.hidden_channels, self.hidden_channels), BatchNorm1d(self.hidden_channels), ReLU(),\n",
    "#                        Linear(self.hidden_channels, self.hidden_channels), ReLU()))\n",
    "#         self.lin1 = Linear(self.num_hidden, self.num_hidden)\n",
    "#         self.lin2 = Linear(self.num_hidden, self.out_channels_channels)\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GINConv(Seq(Lin(in_channels, 64), ReLU(), Lin(64, 64)))\n",
    "        self.pool1 = SAGPooling(64, min_score=0.001, GNN=GCNConv)\n",
    "        self.conv2 = GINConv(Seq(Lin(64, 64), ReLU(), Lin(64, 64)))\n",
    "        self.pool2 = SAGPooling(64, min_score=0.001, GNN=GCNConv)\n",
    "        self.conv3 = GINConv(Seq(Lin(64, 64), ReLU(), Lin(64, 64)))\n",
    "\n",
    "        self.lin = torch.nn.Linear(64, 1) \n",
    "    \n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        #x = self.conv5(x, edge_index)\n",
    "        #x = F.elu(x)\n",
    "        \n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "            #[32, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6c4d7eaf-3a87-4f65-a637-50007d5b2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b5b280b-fd1f-4011-ac24-408e68252c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cff074d4-c6ee-46b4-814d-cbf1498e25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=5, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54e35d13-813c-4b48-86f1-b5fed1c8c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CPU --> GPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GINConv().to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f619bb8-c960-4b9c-834e-1d390e09b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(11, 80)\n",
      "  (conv2): GCNConv(80, 80)\n",
      "  (conv3): GCNConv(80, 80)\n",
      "  (conv4): GCNConv(80, 80)\n",
      "  (conv5): GCNConv(80, 80)\n",
      "  (lin1): Linear(in_features=80, out_features=80, bias=True)\n",
      "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef461d5c-7e36-4f22-9e35-9346890cffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|     Modules      | Parameters |\n",
      "+------------------+------------+\n",
      "|    conv1.bias    |     80     |\n",
      "| conv1.lin.weight |    880     |\n",
      "|    conv2.bias    |     80     |\n",
      "| conv2.lin.weight |    6400    |\n",
      "|    conv3.bias    |     80     |\n",
      "| conv3.lin.weight |    6400    |\n",
      "|    conv4.bias    |     80     |\n",
      "| conv4.lin.weight |    6400    |\n",
      "|    conv5.bias    |     80     |\n",
      "| conv5.lin.weight |    6400    |\n",
      "|   lin1.weight    |    6400    |\n",
      "|    lin1.bias     |     80     |\n",
      "|   lin2.weight    |     80     |\n",
      "|    lin2.bias     |     1      |\n",
      "+------------------+------------+\n",
      "Total Trainable Params: 33441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33441"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9125047d-4c90-4ee0-8b8e-b515288210b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device)).to(device)\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1eefee4f-febc-4141-ae8e-15f3f35a5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        #batch = data.to(device) #it trigger error!\n",
    "        out = model(batch.to(device)).to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device)).to(device)\n",
    "        tmp = ((out - y)**2).to(device)\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22699d07-ae57-4388-bf2a-ccd497ff5f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125923835.4799358"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9608574-7d05-4e54-8729-2ed62179f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=5, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "811e1d79-2de3-45dc-9082-bcb8e734e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTal Epoch: 301, LR: 0.000010, Loss: 171704.9833863, Val MAE: 784.2457055, Test MAE: 779.8401596, Time: 26.78816294670105\n",
      "ToTal Epoch: 302, LR: 0.000010, Loss: 171333.2174903, Val MAE: 511.6222288, Test MAE: 513.5180486, Time: 25.7104172706604\n",
      "ToTal Epoch: 303, LR: 0.000010, Loss: 170514.6141404, Val MAE: 490.5260780, Test MAE: 492.8555336, Time: 26.78656578063965\n",
      "ToTal Epoch: 304, LR: 0.000010, Loss: 170383.0775820, Val MAE: 603.9109742, Test MAE: 492.8555336, Time: 23.423296689987183\n",
      "ToTal Epoch: 305, LR: 0.000010, Loss: 171329.8163558, Val MAE: 904.7040907, Test MAE: 492.8555336, Time: 24.966158866882324\n",
      "ToTal Epoch: 306, LR: 0.000010, Loss: 171793.0874421, Val MAE: 883.3468297, Test MAE: 492.8555336, Time: 22.79279351234436\n",
      "ToTal Epoch: 307, LR: 0.000010, Loss: 171676.7368700, Val MAE: 821.6614908, Test MAE: 492.8555336, Time: 22.894547700881958\n",
      "ToTal Epoch: 308, LR: 0.000010, Loss: 171377.9870898, Val MAE: 879.3428337, Test MAE: 492.8555336, Time: 22.52784776687622\n",
      "ToTal Epoch: 309, LR: 0.000010, Loss: 170069.0138740, Val MAE: 699.0397842, Test MAE: 492.8555336, Time: 22.551006317138672\n",
      "ToTal Epoch: 310, LR: 0.000007, Loss: 170436.9484594, Val MAE: 483.5737135, Test MAE: 484.6261162, Time: 24.820883989334106\n",
      "ToTal Epoch: 311, LR: 0.000007, Loss: 170919.4953698, Val MAE: 644.9145229, Test MAE: 484.6261162, Time: 22.95384693145752\n",
      "ToTal Epoch: 312, LR: 0.000007, Loss: 170019.7575789, Val MAE: 504.7803290, Test MAE: 484.6261162, Time: 23.83374333381653\n",
      "ToTal Epoch: 313, LR: 0.000007, Loss: 172104.9012134, Val MAE: 537.4573780, Test MAE: 484.6261162, Time: 25.03723382949829\n",
      "ToTal Epoch: 314, LR: 0.000007, Loss: 170598.5695803, Val MAE: 907.1002788, Test MAE: 484.6261162, Time: 23.205174446105957\n",
      "ToTal Epoch: 315, LR: 0.000007, Loss: 169470.1516147, Val MAE: 630.4484648, Test MAE: 484.6261162, Time: 23.326727867126465\n",
      "ToTal Epoch: 316, LR: 0.000007, Loss: 171529.7464159, Val MAE: 986.2815634, Test MAE: 484.6261162, Time: 22.602972745895386\n",
      "ToTal Epoch: 317, LR: 0.000005, Loss: 171504.3063990, Val MAE: 591.0777009, Test MAE: 484.6261162, Time: 22.465736389160156\n",
      "ToTal Epoch: 318, LR: 0.000005, Loss: 171765.5781876, Val MAE: 592.7245844, Test MAE: 484.6261162, Time: 23.005094528198242\n",
      "ToTal Epoch: 319, LR: 0.000005, Loss: 171495.8026812, Val MAE: 531.3930089, Test MAE: 484.6261162, Time: 22.96909475326538\n",
      "ToTal Epoch: 320, LR: 0.000005, Loss: 171775.3564038, Val MAE: 986.8956463, Test MAE: 484.6261162, Time: 22.662070512771606\n",
      "ToTal Epoch: 321, LR: 0.000005, Loss: 170490.3026083, Val MAE: 818.0330262, Test MAE: 484.6261162, Time: 22.879552602767944\n",
      "ToTal Epoch: 322, LR: 0.000005, Loss: 170038.9786760, Val MAE: 604.8677836, Test MAE: 484.6261162, Time: 23.21494722366333\n",
      "ToTal Epoch: 323, LR: 0.000003, Loss: 171976.9307887, Val MAE: 629.0085250, Test MAE: 484.6261162, Time: 22.726417541503906\n",
      "ToTal Epoch: 324, LR: 0.000003, Loss: 170781.5767281, Val MAE: 740.9660018, Test MAE: 484.6261162, Time: 23.255362272262573\n",
      "ToTal Epoch: 325, LR: 0.000003, Loss: 172595.3923972, Val MAE: 583.4355355, Test MAE: 484.6261162, Time: 22.75439763069153\n",
      "ToTal Epoch: 326, LR: 0.000003, Loss: 170270.1419636, Val MAE: 595.0818124, Test MAE: 484.6261162, Time: 23.53607177734375\n",
      "ToTal Epoch: 327, LR: 0.000003, Loss: 171692.6272608, Val MAE: 749.4333492, Test MAE: 484.6261162, Time: 23.463416576385498\n",
      "ToTal Epoch: 328, LR: 0.000003, Loss: 170323.9638752, Val MAE: 679.5318331, Test MAE: 484.6261162, Time: 23.03310537338257\n",
      "ToTal Epoch: 329, LR: 0.000002, Loss: 171392.8978288, Val MAE: 779.4382096, Test MAE: 484.6261162, Time: 22.730262756347656\n",
      "ToTal Epoch: 330, LR: 0.000002, Loss: 170809.6640651, Val MAE: 760.6806362, Test MAE: 484.6261162, Time: 22.62819194793701\n",
      "ToTal Epoch: 331, LR: 0.000002, Loss: 171222.4003189, Val MAE: 809.8708910, Test MAE: 484.6261162, Time: 22.821199655532837\n",
      "ToTal Epoch: 332, LR: 0.000002, Loss: 170295.7992798, Val MAE: 571.7406660, Test MAE: 484.6261162, Time: 22.56602692604065\n",
      "ToTal Epoch: 333, LR: 0.000002, Loss: 171259.7827688, Val MAE: 711.8736581, Test MAE: 484.6261162, Time: 22.738901615142822\n",
      "ToTal Epoch: 334, LR: 0.000002, Loss: 169911.9414370, Val MAE: 872.4959057, Test MAE: 484.6261162, Time: 23.386605739593506\n",
      "ToTal Epoch: 335, LR: 0.000002, Loss: 171161.5116765, Val MAE: 818.7761721, Test MAE: 484.6261162, Time: 23.279889345169067\n",
      "ToTal Epoch: 336, LR: 0.000002, Loss: 171681.5310407, Val MAE: 637.7610773, Test MAE: 484.6261162, Time: 22.739080667495728\n",
      "ToTal Epoch: 337, LR: 0.000002, Loss: 171179.2970585, Val MAE: 665.4343261, Test MAE: 484.6261162, Time: 22.80903720855713\n",
      "ToTal Epoch: 338, LR: 0.000002, Loss: 170095.5934935, Val MAE: 755.7686241, Test MAE: 484.6261162, Time: 22.701580047607422\n",
      "ToTal Epoch: 339, LR: 0.000002, Loss: 170594.3929716, Val MAE: 678.9154992, Test MAE: 484.6261162, Time: 22.70242953300476\n",
      "ToTal Epoch: 340, LR: 0.000002, Loss: 171123.6789614, Val MAE: 673.4337847, Test MAE: 484.6261162, Time: 22.424641847610474\n",
      "ToTal Epoch: 341, LR: 0.000001, Loss: 170573.4726138, Val MAE: 788.7778117, Test MAE: 484.6261162, Time: 22.876599311828613\n",
      "ToTal Epoch: 342, LR: 0.000001, Loss: 171966.4240314, Val MAE: 725.2308812, Test MAE: 484.6261162, Time: 22.725603580474854\n",
      "ToTal Epoch: 343, LR: 0.000001, Loss: 171361.5584484, Val MAE: 759.4609838, Test MAE: 484.6261162, Time: 23.114701747894287\n",
      "ToTal Epoch: 344, LR: 0.000001, Loss: 171138.6176527, Val MAE: 683.7717887, Test MAE: 484.6261162, Time: 23.16520667076111\n",
      "ToTal Epoch: 345, LR: 0.000001, Loss: 170376.4481106, Val MAE: 802.4676127, Test MAE: 484.6261162, Time: 22.629416704177856\n",
      "ToTal Epoch: 346, LR: 0.000001, Loss: 170821.5569567, Val MAE: 801.4433674, Test MAE: 484.6261162, Time: 22.69302797317505\n",
      "ToTal Epoch: 347, LR: 0.000001, Loss: 171520.3390986, Val MAE: 736.7060796, Test MAE: 484.6261162, Time: 22.983203172683716\n",
      "ToTal Epoch: 348, LR: 0.000001, Loss: 171768.8099986, Val MAE: 636.1755755, Test MAE: 484.6261162, Time: 22.66330599784851\n",
      "ToTal Epoch: 349, LR: 0.000001, Loss: 171942.5962284, Val MAE: 731.6759142, Test MAE: 484.6261162, Time: 22.7975332736969\n",
      "ToTal Epoch: 350, LR: 0.000001, Loss: 170523.3141189, Val MAE: 802.7103661, Test MAE: 484.6261162, Time: 22.657694101333618\n",
      "ToTal Epoch: 351, LR: 0.000001, Loss: 171835.6317441, Val MAE: 678.6809716, Test MAE: 484.6261162, Time: 22.63620162010193\n",
      "ToTal Epoch: 352, LR: 0.000001, Loss: 170896.7303265, Val MAE: 621.6728732, Test MAE: 484.6261162, Time: 22.934186697006226\n",
      "ToTal Epoch: 353, LR: 0.000001, Loss: 169876.5301378, Val MAE: 788.3070591, Test MAE: 484.6261162, Time: 23.03100609779358\n",
      "ToTal Epoch: 354, LR: 0.000001, Loss: 171738.8623525, Val MAE: 725.5048989, Test MAE: 484.6261162, Time: 22.46748113632202\n",
      "ToTal Epoch: 355, LR: 0.000001, Loss: 169837.2741437, Val MAE: 735.9708671, Test MAE: 484.6261162, Time: 22.609861135482788\n",
      "ToTal Epoch: 356, LR: 0.000001, Loss: 170576.7510522, Val MAE: 631.8697446, Test MAE: 484.6261162, Time: 22.720289945602417\n",
      "ToTal Epoch: 357, LR: 0.000001, Loss: 171120.8175321, Val MAE: 652.4849086, Test MAE: 484.6261162, Time: 23.269221782684326\n",
      "ToTal Epoch: 358, LR: 0.000001, Loss: 171689.5103951, Val MAE: 739.4287496, Test MAE: 484.6261162, Time: 23.615641593933105\n",
      "ToTal Epoch: 359, LR: 0.000001, Loss: 172130.7481644, Val MAE: 631.1884300, Test MAE: 484.6261162, Time: 24.786463737487793\n",
      "ToTal Epoch: 360, LR: 0.000001, Loss: 170721.0811637, Val MAE: 617.2554365, Test MAE: 484.6261162, Time: 23.79583430290222\n",
      "ToTal Epoch: 361, LR: 0.000001, Loss: 171241.6287954, Val MAE: 780.8053040, Test MAE: 484.6261162, Time: 22.85371971130371\n",
      "ToTal Epoch: 362, LR: 0.000001, Loss: 170877.8522942, Val MAE: 708.3127084, Test MAE: 484.6261162, Time: 23.49600863456726\n",
      "ToTal Epoch: 363, LR: 0.000001, Loss: 171512.6996513, Val MAE: 691.2133795, Test MAE: 484.6261162, Time: 24.2547869682312\n",
      "ToTal Epoch: 364, LR: 0.000001, Loss: 169550.4814217, Val MAE: 799.6874705, Test MAE: 484.6261162, Time: 22.77948546409607\n",
      "ToTal Epoch: 365, LR: 0.000001, Loss: 169442.3523551, Val MAE: 676.0590484, Test MAE: 484.6261162, Time: 23.12044405937195\n",
      "ToTal Epoch: 366, LR: 0.000001, Loss: 172142.6345734, Val MAE: 737.8551798, Test MAE: 484.6261162, Time: 22.994646787643433\n",
      "ToTal Epoch: 367, LR: 0.000001, Loss: 171041.3217193, Val MAE: 649.0174604, Test MAE: 484.6261162, Time: 22.92089056968689\n",
      "ToTal Epoch: 368, LR: 0.000001, Loss: 171832.9048249, Val MAE: 594.5940222, Test MAE: 484.6261162, Time: 23.18706440925598\n",
      "ToTal Epoch: 369, LR: 0.000001, Loss: 171088.9783751, Val MAE: 721.0517475, Test MAE: 484.6261162, Time: 22.64613962173462\n",
      "ToTal Epoch: 370, LR: 0.000001, Loss: 170857.3194000, Val MAE: 803.6679611, Test MAE: 484.6261162, Time: 24.19216251373291\n",
      "ToTal Epoch: 371, LR: 0.000001, Loss: 171211.9745187, Val MAE: 696.6672793, Test MAE: 484.6261162, Time: 23.634298086166382\n",
      "ToTal Epoch: 372, LR: 0.000001, Loss: 170995.7583230, Val MAE: 676.5496938, Test MAE: 484.6261162, Time: 23.805801153182983\n",
      "ToTal Epoch: 373, LR: 0.000001, Loss: 171565.3269252, Val MAE: 811.6745234, Test MAE: 484.6261162, Time: 23.552896976470947\n",
      "ToTal Epoch: 374, LR: 0.000001, Loss: 171377.8024077, Val MAE: 763.1305016, Test MAE: 484.6261162, Time: 23.619582176208496\n",
      "ToTal Epoch: 375, LR: 0.000001, Loss: 169553.3887630, Val MAE: 667.5244104, Test MAE: 484.6261162, Time: 24.5158953666687\n",
      "ToTal Epoch: 376, LR: 0.000001, Loss: 171215.3957412, Val MAE: 647.2353128, Test MAE: 484.6261162, Time: 24.000465393066406\n",
      "ToTal Epoch: 377, LR: 0.000001, Loss: 172209.7227177, Val MAE: 594.8289823, Test MAE: 484.6261162, Time: 23.21462345123291\n",
      "ToTal Epoch: 378, LR: 0.000001, Loss: 170564.8902092, Val MAE: 632.1947106, Test MAE: 484.6261162, Time: 22.799920558929443\n",
      "ToTal Epoch: 379, LR: 0.000001, Loss: 170853.4308102, Val MAE: 723.5925889, Test MAE: 484.6261162, Time: 22.866764068603516\n",
      "ToTal Epoch: 380, LR: 0.000001, Loss: 172121.0507524, Val MAE: 672.3328171, Test MAE: 484.6261162, Time: 22.580087661743164\n",
      "ToTal Epoch: 381, LR: 0.000001, Loss: 170478.8143195, Val MAE: 742.4250786, Test MAE: 484.6261162, Time: 23.7435142993927\n",
      "ToTal Epoch: 382, LR: 0.000001, Loss: 170332.9271378, Val MAE: 690.5417892, Test MAE: 484.6261162, Time: 22.923168182373047\n",
      "ToTal Epoch: 383, LR: 0.000001, Loss: 172026.9174951, Val MAE: 627.5201606, Test MAE: 484.6261162, Time: 23.55763816833496\n",
      "ToTal Epoch: 384, LR: 0.000001, Loss: 170257.0572075, Val MAE: 753.4775230, Test MAE: 484.6261162, Time: 24.487058639526367\n",
      "ToTal Epoch: 385, LR: 0.000001, Loss: 171198.4156332, Val MAE: 683.8940259, Test MAE: 484.6261162, Time: 23.955047607421875\n",
      "ToTal Epoch: 386, LR: 0.000001, Loss: 171355.9473965, Val MAE: 703.4007703, Test MAE: 484.6261162, Time: 23.366942644119263\n",
      "ToTal Epoch: 387, LR: 0.000001, Loss: 171181.3446723, Val MAE: 654.2447227, Test MAE: 484.6261162, Time: 25.140204668045044\n",
      "ToTal Epoch: 388, LR: 0.000001, Loss: 171199.5036880, Val MAE: 707.1329907, Test MAE: 484.6261162, Time: 23.26298499107361\n",
      "ToTal Epoch: 389, LR: 0.000001, Loss: 170509.7103521, Val MAE: 693.5882044, Test MAE: 484.6261162, Time: 24.023756504058838\n",
      "ToTal Epoch: 390, LR: 0.000001, Loss: 170239.7792720, Val MAE: 664.0930760, Test MAE: 484.6261162, Time: 23.458584785461426\n",
      "ToTal Epoch: 391, LR: 0.000001, Loss: 170772.2247014, Val MAE: 761.0626150, Test MAE: 484.6261162, Time: 23.148462772369385\n",
      "ToTal Epoch: 392, LR: 0.000001, Loss: 171313.8778675, Val MAE: 692.6481903, Test MAE: 484.6261162, Time: 23.425331592559814\n",
      "ToTal Epoch: 393, LR: 0.000001, Loss: 171403.9813847, Val MAE: 616.1172520, Test MAE: 484.6261162, Time: 24.216376066207886\n",
      "ToTal Epoch: 394, LR: 0.000001, Loss: 170618.6529786, Val MAE: 790.7304840, Test MAE: 484.6261162, Time: 23.402226209640503\n",
      "ToTal Epoch: 395, LR: 0.000001, Loss: 171062.1871148, Val MAE: 680.4133975, Test MAE: 484.6261162, Time: 23.50900411605835\n",
      "ToTal Epoch: 396, LR: 0.000001, Loss: 170181.6739060, Val MAE: 760.6053310, Test MAE: 484.6261162, Time: 23.09771203994751\n",
      "ToTal Epoch: 397, LR: 0.000001, Loss: 169863.6878780, Val MAE: 759.1282425, Test MAE: 484.6261162, Time: 22.976160049438477\n",
      "ToTal Epoch: 398, LR: 0.000001, Loss: 169986.1004705, Val MAE: 789.2145812, Test MAE: 484.6261162, Time: 23.00996232032776\n",
      "ToTal Epoch: 399, LR: 0.000001, Loss: 171588.8489789, Val MAE: 724.4401161, Test MAE: 484.6261162, Time: 22.78203296661377\n",
      "ToTal Epoch: 400, LR: 0.000001, Loss: 171570.1725004, Val MAE: 656.0732646, Test MAE: 484.6261162, Time: 23.032917261123657\n",
      "ToTal Epoch: 401, LR: 0.000001, Loss: 171444.4288599, Val MAE: 706.7926715, Test MAE: 484.6261162, Time: 25.557305335998535\n",
      "ToTal Epoch: 402, LR: 0.000001, Loss: 172189.9303432, Val MAE: 749.5236070, Test MAE: 484.6261162, Time: 24.374207973480225\n",
      "ToTal Epoch: 403, LR: 0.000001, Loss: 171253.8371471, Val MAE: 790.8135322, Test MAE: 484.6261162, Time: 24.05281114578247\n",
      "ToTal Epoch: 404, LR: 0.000001, Loss: 170749.5937085, Val MAE: 780.4377380, Test MAE: 484.6261162, Time: 22.98191237449646\n",
      "ToTal Epoch: 405, LR: 0.000001, Loss: 173094.7725004, Val MAE: 754.1441848, Test MAE: 484.6261162, Time: 22.80807876586914\n",
      "ToTal Epoch: 406, LR: 0.000001, Loss: 170979.3094408, Val MAE: 710.3016273, Test MAE: 484.6261162, Time: 23.02777075767517\n",
      "ToTal Epoch: 407, LR: 0.000001, Loss: 171081.9468638, Val MAE: 686.7494354, Test MAE: 484.6261162, Time: 23.01538586616516\n",
      "ToTal Epoch: 408, LR: 0.000001, Loss: 171360.9340157, Val MAE: 616.4960060, Test MAE: 484.6261162, Time: 22.88903307914734\n",
      "ToTal Epoch: 409, LR: 0.000001, Loss: 171262.9407646, Val MAE: 753.9492971, Test MAE: 484.6261162, Time: 23.098763465881348\n",
      "ToTal Epoch: 410, LR: 0.000001, Loss: 170601.4839488, Val MAE: 716.5653634, Test MAE: 484.6261162, Time: 22.541186094284058\n",
      "ToTal Epoch: 411, LR: 0.000001, Loss: 170577.4544380, Val MAE: 743.4489356, Test MAE: 484.6261162, Time: 22.947383642196655\n",
      "ToTal Epoch: 412, LR: 0.000001, Loss: 170969.2237161, Val MAE: 615.1302245, Test MAE: 484.6261162, Time: 22.911132335662842\n",
      "ToTal Epoch: 413, LR: 0.000001, Loss: 171537.4647912, Val MAE: 665.6372789, Test MAE: 484.6261162, Time: 23.113266944885254\n",
      "ToTal Epoch: 414, LR: 0.000001, Loss: 171335.8810729, Val MAE: 706.8686675, Test MAE: 484.6261162, Time: 22.952975273132324\n",
      "ToTal Epoch: 415, LR: 0.000001, Loss: 169484.3700174, Val MAE: 832.2788226, Test MAE: 484.6261162, Time: 23.09096932411194\n",
      "ToTal Epoch: 416, LR: 0.000001, Loss: 171066.4390102, Val MAE: 680.7804423, Test MAE: 484.6261162, Time: 22.97399663925171\n",
      "ToTal Epoch: 417, LR: 0.000001, Loss: 170228.1218638, Val MAE: 756.9125276, Test MAE: 484.6261162, Time: 23.075684785842896\n",
      "ToTal Epoch: 418, LR: 0.000001, Loss: 171569.4028436, Val MAE: 765.2404656, Test MAE: 484.6261162, Time: 23.24910545349121\n",
      "ToTal Epoch: 419, LR: 0.000001, Loss: 170933.3696054, Val MAE: 699.1078955, Test MAE: 484.6261162, Time: 24.389008283615112\n",
      "ToTal Epoch: 420, LR: 0.000001, Loss: 170326.4331964, Val MAE: 749.2973567, Test MAE: 484.6261162, Time: 23.821049451828003\n",
      "ToTal Epoch: 421, LR: 0.000001, Loss: 172439.6931723, Val MAE: 644.6076347, Test MAE: 484.6261162, Time: 23.34925413131714\n",
      "ToTal Epoch: 422, LR: 0.000001, Loss: 170896.4514236, Val MAE: 712.2687205, Test MAE: 484.6261162, Time: 22.83484983444214\n",
      "ToTal Epoch: 423, LR: 0.000001, Loss: 170970.3125579, Val MAE: 805.7679713, Test MAE: 484.6261162, Time: 24.06265926361084\n",
      "ToTal Epoch: 424, LR: 0.000001, Loss: 171622.8328883, Val MAE: 636.8108121, Test MAE: 484.6261162, Time: 24.066532135009766\n",
      "ToTal Epoch: 425, LR: 0.000001, Loss: 169721.1393780, Val MAE: 775.9211396, Test MAE: 484.6261162, Time: 23.324981689453125\n",
      "ToTal Epoch: 426, LR: 0.000001, Loss: 171054.6864425, Val MAE: 726.8403852, Test MAE: 484.6261162, Time: 23.04359459877014\n",
      "ToTal Epoch: 427, LR: 0.000001, Loss: 170496.4869393, Val MAE: 695.1424057, Test MAE: 484.6261162, Time: 22.69320034980774\n",
      "ToTal Epoch: 428, LR: 0.000001, Loss: 170788.6925883, Val MAE: 845.6159230, Test MAE: 484.6261162, Time: 22.814703226089478\n",
      "ToTal Epoch: 429, LR: 0.000001, Loss: 171849.8576028, Val MAE: 622.3079681, Test MAE: 484.6261162, Time: 22.943471908569336\n",
      "ToTal Epoch: 430, LR: 0.000001, Loss: 169604.0030836, Val MAE: 748.9217483, Test MAE: 484.6261162, Time: 23.277507305145264\n",
      "ToTal Epoch: 431, LR: 0.000001, Loss: 171357.6551127, Val MAE: 693.2188290, Test MAE: 484.6261162, Time: 23.61815357208252\n",
      "ToTal Epoch: 432, LR: 0.000001, Loss: 170967.6623919, Val MAE: 707.9604911, Test MAE: 484.6261162, Time: 23.049823999404907\n",
      "ToTal Epoch: 433, LR: 0.000001, Loss: 170068.7156571, Val MAE: 684.5508590, Test MAE: 484.6261162, Time: 22.753241062164307\n",
      "ToTal Epoch: 434, LR: 0.000001, Loss: 170288.2866527, Val MAE: 718.0658911, Test MAE: 484.6261162, Time: 23.11722445487976\n",
      "ToTal Epoch: 435, LR: 0.000001, Loss: 171435.5049885, Val MAE: 623.9141692, Test MAE: 484.6261162, Time: 22.702327728271484\n",
      "ToTal Epoch: 436, LR: 0.000001, Loss: 171140.0121961, Val MAE: 680.4860796, Test MAE: 484.6261162, Time: 22.696823835372925\n",
      "ToTal Epoch: 437, LR: 0.000001, Loss: 172285.5319352, Val MAE: 628.0272390, Test MAE: 484.6261162, Time: 23.461130619049072\n",
      "ToTal Epoch: 438, LR: 0.000001, Loss: 170917.5399154, Val MAE: 665.7513739, Test MAE: 484.6261162, Time: 23.423723936080933\n",
      "ToTal Epoch: 439, LR: 0.000001, Loss: 170881.7416030, Val MAE: 667.0522345, Test MAE: 484.6261162, Time: 23.29435133934021\n",
      "ToTal Epoch: 440, LR: 0.000001, Loss: 171421.7410046, Val MAE: 689.5383685, Test MAE: 484.6261162, Time: 23.03818106651306\n",
      "ToTal Epoch: 441, LR: 0.000001, Loss: 170502.5154517, Val MAE: 743.5480842, Test MAE: 484.6261162, Time: 23.2908296585083\n",
      "ToTal Epoch: 442, LR: 0.000001, Loss: 170194.2114042, Val MAE: 664.3449648, Test MAE: 484.6261162, Time: 23.030858516693115\n",
      "ToTal Epoch: 443, LR: 0.000001, Loss: 171593.0388872, Val MAE: 565.6274017, Test MAE: 484.6261162, Time: 23.084482192993164\n",
      "ToTal Epoch: 444, LR: 0.000001, Loss: 172603.8434816, Val MAE: 712.5329021, Test MAE: 484.6261162, Time: 22.744089365005493\n",
      "ToTal Epoch: 445, LR: 0.000001, Loss: 171479.4021820, Val MAE: 633.4663457, Test MAE: 484.6261162, Time: 22.86580181121826\n",
      "ToTal Epoch: 446, LR: 0.000001, Loss: 169786.8523205, Val MAE: 796.0321557, Test MAE: 484.6261162, Time: 23.57243323326111\n",
      "ToTal Epoch: 447, LR: 0.000001, Loss: 171251.5822027, Val MAE: 684.5419358, Test MAE: 484.6261162, Time: 23.637653589248657\n",
      "ToTal Epoch: 448, LR: 0.000001, Loss: 169881.5133760, Val MAE: 723.2763805, Test MAE: 484.6261162, Time: 24.08231234550476\n",
      "ToTal Epoch: 449, LR: 0.000001, Loss: 171557.3763460, Val MAE: 730.3034753, Test MAE: 484.6261162, Time: 24.330897092819214\n",
      "ToTal Epoch: 450, LR: 0.000001, Loss: 171619.2693928, Val MAE: 792.0645109, Test MAE: 484.6261162, Time: 24.18933939933777\n",
      "ToTal Epoch: 451, LR: 0.000001, Loss: 171694.6524459, Val MAE: 687.1983183, Test MAE: 484.6261162, Time: 23.405039310455322\n",
      "ToTal Epoch: 452, LR: 0.000001, Loss: 171913.1450282, Val MAE: 671.9442790, Test MAE: 484.6261162, Time: 22.946609258651733\n",
      "ToTal Epoch: 453, LR: 0.000001, Loss: 171980.6178008, Val MAE: 717.8775085, Test MAE: 484.6261162, Time: 23.523676872253418\n",
      "ToTal Epoch: 454, LR: 0.000001, Loss: 171246.9176432, Val MAE: 609.8407145, Test MAE: 484.6261162, Time: 23.30267095565796\n",
      "ToTal Epoch: 455, LR: 0.000001, Loss: 169902.5350045, Val MAE: 733.8488024, Test MAE: 484.6261162, Time: 23.166414737701416\n",
      "ToTal Epoch: 456, LR: 0.000001, Loss: 171451.3038838, Val MAE: 729.2693861, Test MAE: 484.6261162, Time: 22.996097564697266\n",
      "ToTal Epoch: 457, LR: 0.000001, Loss: 170171.8839882, Val MAE: 698.0926274, Test MAE: 484.6261162, Time: 24.082981824874878\n",
      "ToTal Epoch: 458, LR: 0.000001, Loss: 171679.1412960, Val MAE: 704.1932095, Test MAE: 484.6261162, Time: 24.098517894744873\n",
      "ToTal Epoch: 459, LR: 0.000001, Loss: 169890.0172945, Val MAE: 702.7872059, Test MAE: 484.6261162, Time: 24.128955602645874\n",
      "ToTal Epoch: 460, LR: 0.000001, Loss: 171967.5095376, Val MAE: 722.6328730, Test MAE: 484.6261162, Time: 23.649314641952515\n",
      "ToTal Epoch: 461, LR: 0.000001, Loss: 170725.4449840, Val MAE: 802.7267561, Test MAE: 484.6261162, Time: 23.313035249710083\n",
      "ToTal Epoch: 462, LR: 0.000001, Loss: 171863.3329229, Val MAE: 854.0581541, Test MAE: 484.6261162, Time: 23.687456369400024\n",
      "ToTal Epoch: 463, LR: 0.000001, Loss: 170806.4520363, Val MAE: 717.4841199, Test MAE: 484.6261162, Time: 22.839300394058228\n",
      "ToTal Epoch: 464, LR: 0.000001, Loss: 171040.1041263, Val MAE: 667.8318452, Test MAE: 484.6261162, Time: 22.804203748703003\n",
      "ToTal Epoch: 465, LR: 0.000001, Loss: 171665.3780729, Val MAE: 717.3299953, Test MAE: 484.6261162, Time: 23.150251626968384\n",
      "ToTal Epoch: 466, LR: 0.000001, Loss: 170763.4324607, Val MAE: 707.5875511, Test MAE: 484.6261162, Time: 22.797141075134277\n",
      "ToTal Epoch: 467, LR: 0.000001, Loss: 170323.3616216, Val MAE: 797.0414674, Test MAE: 484.6261162, Time: 22.656558513641357\n",
      "ToTal Epoch: 468, LR: 0.000001, Loss: 170343.9344826, Val MAE: 727.6511904, Test MAE: 484.6261162, Time: 23.155295372009277\n",
      "ToTal Epoch: 469, LR: 0.000001, Loss: 170871.3017604, Val MAE: 647.1897587, Test MAE: 484.6261162, Time: 23.08286762237549\n",
      "ToTal Epoch: 470, LR: 0.000001, Loss: 171209.1282962, Val MAE: 724.9828447, Test MAE: 484.6261162, Time: 22.80483651161194\n",
      "ToTal Epoch: 471, LR: 0.000001, Loss: 169971.2278711, Val MAE: 650.0283879, Test MAE: 484.6261162, Time: 23.288626194000244\n",
      "ToTal Epoch: 472, LR: 0.000001, Loss: 171651.4061995, Val MAE: 690.2058635, Test MAE: 484.6261162, Time: 23.074917316436768\n",
      "ToTal Epoch: 473, LR: 0.000001, Loss: 171455.0983985, Val MAE: 700.4718332, Test MAE: 484.6261162, Time: 24.30226159095764\n",
      "ToTal Epoch: 474, LR: 0.000001, Loss: 171105.8125221, Val MAE: 744.4985556, Test MAE: 484.6261162, Time: 22.93710470199585\n",
      "ToTal Epoch: 475, LR: 0.000001, Loss: 170703.7706827, Val MAE: 789.5992035, Test MAE: 484.6261162, Time: 23.89694857597351\n",
      "ToTal Epoch: 476, LR: 0.000001, Loss: 171585.8819352, Val MAE: 657.9424372, Test MAE: 484.6261162, Time: 23.702471256256104\n",
      "ToTal Epoch: 477, LR: 0.000001, Loss: 170984.7351108, Val MAE: 656.4055163, Test MAE: 484.6261162, Time: 23.007797718048096\n",
      "ToTal Epoch: 478, LR: 0.000001, Loss: 171111.8474789, Val MAE: 775.3250663, Test MAE: 484.6261162, Time: 23.02564024925232\n",
      "ToTal Epoch: 479, LR: 0.000001, Loss: 171909.5429967, Val MAE: 828.4171784, Test MAE: 484.6261162, Time: 24.117174863815308\n",
      "ToTal Epoch: 480, LR: 0.000001, Loss: 170374.0699147, Val MAE: 681.3113901, Test MAE: 484.6261162, Time: 22.768176317214966\n",
      "ToTal Epoch: 481, LR: 0.000001, Loss: 172317.5986528, Val MAE: 656.4433920, Test MAE: 484.6261162, Time: 23.301748991012573\n",
      "ToTal Epoch: 482, LR: 0.000001, Loss: 170749.4080017, Val MAE: 655.9903212, Test MAE: 484.6261162, Time: 23.868268251419067\n",
      "ToTal Epoch: 483, LR: 0.000001, Loss: 170774.0988069, Val MAE: 747.9101121, Test MAE: 484.6261162, Time: 23.395528316497803\n",
      "ToTal Epoch: 484, LR: 0.000001, Loss: 169752.0867673, Val MAE: 770.4894052, Test MAE: 484.6261162, Time: 24.52845573425293\n",
      "ToTal Epoch: 485, LR: 0.000001, Loss: 171208.4065901, Val MAE: 660.5810588, Test MAE: 484.6261162, Time: 24.636481761932373\n",
      "ToTal Epoch: 486, LR: 0.000001, Loss: 170218.3625125, Val MAE: 797.2001722, Test MAE: 484.6261162, Time: 23.470929384231567\n",
      "ToTal Epoch: 487, LR: 0.000001, Loss: 169927.5080519, Val MAE: 710.6705213, Test MAE: 484.6261162, Time: 23.49355673789978\n",
      "ToTal Epoch: 488, LR: 0.000001, Loss: 173379.5080053, Val MAE: 668.9644064, Test MAE: 484.6261162, Time: 24.48170566558838\n",
      "ToTal Epoch: 489, LR: 0.000001, Loss: 171492.9430098, Val MAE: 659.8906267, Test MAE: 484.6261162, Time: 23.760565757751465\n",
      "ToTal Epoch: 490, LR: 0.000001, Loss: 170228.6173864, Val MAE: 674.6468287, Test MAE: 484.6261162, Time: 22.7077374458313\n",
      "ToTal Epoch: 491, LR: 0.000001, Loss: 170303.7418394, Val MAE: 668.4811254, Test MAE: 484.6261162, Time: 22.70785880088806\n",
      "ToTal Epoch: 492, LR: 0.000001, Loss: 170524.8712356, Val MAE: 722.4917344, Test MAE: 484.6261162, Time: 22.62636137008667\n",
      "ToTal Epoch: 493, LR: 0.000001, Loss: 171738.7989036, Val MAE: 723.0167544, Test MAE: 484.6261162, Time: 22.48420524597168\n",
      "ToTal Epoch: 494, LR: 0.000001, Loss: 169865.5193271, Val MAE: 674.7296982, Test MAE: 484.6261162, Time: 22.580275535583496\n",
      "ToTal Epoch: 495, LR: 0.000001, Loss: 168615.3159198, Val MAE: 737.4217391, Test MAE: 484.6261162, Time: 22.734296560287476\n",
      "ToTal Epoch: 496, LR: 0.000001, Loss: 169595.5345627, Val MAE: 717.7682043, Test MAE: 484.6261162, Time: 22.596501111984253\n",
      "ToTal Epoch: 497, LR: 0.000001, Loss: 170754.1424151, Val MAE: 724.6802838, Test MAE: 484.6261162, Time: 23.840315341949463\n",
      "ToTal Epoch: 498, LR: 0.000001, Loss: 170678.5976998, Val MAE: 762.7443549, Test MAE: 484.6261162, Time: 23.484387397766113\n",
      "ToTal Epoch: 499, LR: 0.000001, Loss: 170745.3585893, Val MAE: 584.9559110, Test MAE: 484.6261162, Time: 23.9760525226593\n",
      "ToTal Epoch: 500, LR: 0.000001, Loss: 171731.4893947, Val MAE: 720.1054468, Test MAE: 484.6261162, Time: 23.270618438720703\n",
      "ToTal Epoch: 501, LR: 0.000001, Loss: 170440.9692961, Val MAE: 779.2280897, Test MAE: 484.6261162, Time: 23.924937963485718\n",
      "ToTal Epoch: 502, LR: 0.000001, Loss: 169677.7119309, Val MAE: 724.8391271, Test MAE: 484.6261162, Time: 23.41558861732483\n",
      "ToTal Epoch: 503, LR: 0.000001, Loss: 171246.2791908, Val MAE: 700.3776007, Test MAE: 484.6261162, Time: 23.69087815284729\n",
      "ToTal Epoch: 504, LR: 0.000001, Loss: 172041.7662937, Val MAE: 729.9314951, Test MAE: 484.6261162, Time: 24.703012466430664\n",
      "ToTal Epoch: 505, LR: 0.000001, Loss: 170443.0393625, Val MAE: 843.1355139, Test MAE: 484.6261162, Time: 23.733377695083618\n",
      "ToTal Epoch: 506, LR: 0.000001, Loss: 170160.2706468, Val MAE: 750.3702186, Test MAE: 484.6261162, Time: 23.40803027153015\n",
      "ToTal Epoch: 507, LR: 0.000001, Loss: 170264.1310383, Val MAE: 639.9359998, Test MAE: 484.6261162, Time: 23.429927110671997\n",
      "ToTal Epoch: 508, LR: 0.000001, Loss: 171746.9044141, Val MAE: 736.9827783, Test MAE: 484.6261162, Time: 24.152071714401245\n",
      "ToTal Epoch: 509, LR: 0.000001, Loss: 170710.2357032, Val MAE: 718.0921150, Test MAE: 484.6261162, Time: 24.241516590118408\n",
      "ToTal Epoch: 510, LR: 0.000001, Loss: 171919.2921022, Val MAE: 637.4980180, Test MAE: 484.6261162, Time: 24.063785791397095\n",
      "ToTal Epoch: 511, LR: 0.000001, Loss: 172070.2897148, Val MAE: 678.3810829, Test MAE: 484.6261162, Time: 23.609825372695923\n",
      "ToTal Epoch: 512, LR: 0.000001, Loss: 170717.1886029, Val MAE: 845.8107442, Test MAE: 484.6261162, Time: 23.212863445281982\n",
      "ToTal Epoch: 513, LR: 0.000001, Loss: 171836.6152725, Val MAE: 634.4423802, Test MAE: 484.6261162, Time: 23.327073335647583\n",
      "ToTal Epoch: 514, LR: 0.000001, Loss: 170534.7134083, Val MAE: 728.5632704, Test MAE: 484.6261162, Time: 23.020989894866943\n",
      "ToTal Epoch: 515, LR: 0.000001, Loss: 169118.6211293, Val MAE: 685.1906730, Test MAE: 484.6261162, Time: 23.60578155517578\n",
      "ToTal Epoch: 516, LR: 0.000001, Loss: 171066.1617709, Val MAE: 731.2171714, Test MAE: 484.6261162, Time: 22.581650733947754\n",
      "ToTal Epoch: 517, LR: 0.000001, Loss: 171316.1640090, Val MAE: 639.7107434, Test MAE: 484.6261162, Time: 23.54865264892578\n",
      "ToTal Epoch: 518, LR: 0.000001, Loss: 169262.2052477, Val MAE: 845.5978967, Test MAE: 484.6261162, Time: 23.46936321258545\n",
      "ToTal Epoch: 519, LR: 0.000001, Loss: 170876.6881718, Val MAE: 730.2052618, Test MAE: 484.6261162, Time: 23.41695499420166\n",
      "ToTal Epoch: 520, LR: 0.000001, Loss: 172668.1859839, Val MAE: 626.1664877, Test MAE: 484.6261162, Time: 23.73762059211731\n",
      "ToTal Epoch: 521, LR: 0.000001, Loss: 170675.1019658, Val MAE: 769.6191603, Test MAE: 484.6261162, Time: 23.190194606781006\n",
      "ToTal Epoch: 522, LR: 0.000001, Loss: 169911.3718650, Val MAE: 749.3554128, Test MAE: 484.6261162, Time: 24.53850817680359\n",
      "ToTal Epoch: 523, LR: 0.000001, Loss: 170184.8681603, Val MAE: 653.8009173, Test MAE: 484.6261162, Time: 22.92388916015625\n",
      "ToTal Epoch: 524, LR: 0.000001, Loss: 170854.5706134, Val MAE: 674.4052466, Test MAE: 484.6261162, Time: 22.887733221054077\n",
      "ToTal Epoch: 525, LR: 0.000001, Loss: 171835.4144150, Val MAE: 694.0142422, Test MAE: 484.6261162, Time: 25.903860092163086\n",
      "ToTal Epoch: 526, LR: 0.000001, Loss: 171677.1031350, Val MAE: 728.3765174, Test MAE: 484.6261162, Time: 25.69380211830139\n",
      "ToTal Epoch: 527, LR: 0.000001, Loss: 169834.0886149, Val MAE: 703.1708639, Test MAE: 484.6261162, Time: 25.002853631973267\n",
      "ToTal Epoch: 528, LR: 0.000001, Loss: 171570.9263460, Val MAE: 724.4758520, Test MAE: 484.6261162, Time: 24.502050638198853\n",
      "ToTal Epoch: 529, LR: 0.000001, Loss: 171183.1978837, Val MAE: 694.6597240, Test MAE: 484.6261162, Time: 24.127707719802856\n",
      "ToTal Epoch: 530, LR: 0.000001, Loss: 170683.7293376, Val MAE: 637.2599814, Test MAE: 484.6261162, Time: 24.95214557647705\n",
      "ToTal Epoch: 531, LR: 0.000001, Loss: 170907.4031911, Val MAE: 650.5438374, Test MAE: 484.6261162, Time: 23.402867317199707\n",
      "ToTal Epoch: 532, LR: 0.000001, Loss: 170414.0729602, Val MAE: 729.8861865, Test MAE: 484.6261162, Time: 23.68714714050293\n",
      "ToTal Epoch: 533, LR: 0.000001, Loss: 169284.2891499, Val MAE: 799.8283236, Test MAE: 484.6261162, Time: 24.576894760131836\n",
      "ToTal Epoch: 534, LR: 0.000001, Loss: 171718.6077294, Val MAE: 634.0443187, Test MAE: 484.6261162, Time: 28.361192226409912\n",
      "ToTal Epoch: 535, LR: 0.000001, Loss: 172243.2165839, Val MAE: 694.8411393, Test MAE: 484.6261162, Time: 24.80828356742859\n",
      "ToTal Epoch: 536, LR: 0.000001, Loss: 172463.5280777, Val MAE: 662.4777020, Test MAE: 484.6261162, Time: 23.692009449005127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63020/3188207803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_time_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to measure time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_63020/1019365969.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     67\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# Collate attributes into a unified representation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             value, slices, incs = _collate(attr, values, data_list, stores,\n\u001b[0m\u001b[1;32m     86\u001b[0m                                            increment)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mincs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_dim\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# tensorboard writer\n",
    "tb_writer = SummaryWriter('run/GCN/run_10')\n",
    "    ## logdir=./python/run/GAT_Net/run_02\n",
    "\n",
    "#input parameters\n",
    "total_num_epoch = 300 #the total number of epoch that have run\n",
    "running_num_epoch = 300 #the number of epoch that run in this time\n",
    "\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    \n",
    "    tb_writer.add_scalar('loss in train', loss, total_num_epoch) #tensorboard\n",
    "    tb_writer.add_scalar('validation MAE', validation_error, total_num_epoch) #tensorboard\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "    \n",
    "    epoch_time_end = time.time() # to measure time\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "  \n",
    "    tb_writer.add_scalar('test MAE', test_error, total_num_epoch) #tensorboard\n",
    "    tb_writer.add_scalar('learning rate', lr, total_num_epoch) #tensorboard\n",
    "total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total Time: {total_time_finish - total_time_start}')\n",
    "tb_writer.add_hparams({'num_conv': 5, 'hidden_channels' : 11}, {'hparam/total_epoch' : total_num_epoch, 'hparam/total_time' : total_time_finish - total_time_start}) #tensorboard\n",
    "tb_writer.close() #tensorboard : if close() is not declared, the writer does not save any valeus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d41729-0e54-4c51-baef-6c7a7be4ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301, LR: 0.010000, Loss: 171055.7439927, Val MAE: 749.5268311, Test MAE: 729.6581324\n",
      "Epoch: 302, LR: 0.010000, Loss: 171381.7415659, Val MAE: 477.4276311, Test MAE: 455.7549674\n",
      "Epoch: 303, LR: 0.010000, Loss: 171330.0292433, Val MAE: 810.1738107, Test MAE: 455.7549674\n",
      "Epoch: 304, LR: 0.010000, Loss: 170884.6685138, Val MAE: 644.2184661, Test MAE: 455.7549674\n",
      "Epoch: 305, LR: 0.010000, Loss: 170795.6376415, Val MAE: 725.1176548, Test MAE: 455.7549674\n",
      "Epoch: 306, LR: 0.010000, Loss: 172353.8815686, Val MAE: 808.8852974, Test MAE: 455.7549674\n",
      "Epoch: 307, LR: 0.010000, Loss: 172006.6827867, Val MAE: 608.6071959, Test MAE: 455.7549674\n",
      "Epoch: 308, LR: 0.010000, Loss: 171673.4699255, Val MAE: 991.0886944, Test MAE: 455.7549674\n",
      "Epoch: 309, LR: 0.007000, Loss: 171889.5727810, Val MAE: 592.2459863, Test MAE: 455.7549674\n",
      "Epoch: 310, LR: 0.007000, Loss: 171105.7444060, Val MAE: 767.3122511, Test MAE: 455.7549674\n",
      "Epoch: 311, LR: 0.007000, Loss: 171220.2660882, Val MAE: 586.7691918, Test MAE: 455.7549674\n",
      "Epoch: 312, LR: 0.007000, Loss: 170640.5204677, Val MAE: 462.1567043, Test MAE: 441.7442079\n",
      "Epoch: 313, LR: 0.007000, Loss: 172619.0445672, Val MAE: 459.9430470, Test MAE: 436.3925752\n",
      "Epoch: 314, LR: 0.007000, Loss: 171674.6192830, Val MAE: 498.7058404, Test MAE: 436.3925752\n",
      "Epoch: 315, LR: 0.007000, Loss: 172693.7357175, Val MAE: 884.8183041, Test MAE: 436.3925752\n",
      "Epoch: 316, LR: 0.007000, Loss: 171967.7244363, Val MAE: 611.7091473, Test MAE: 436.3925752\n",
      "Epoch: 317, LR: 0.007000, Loss: 171456.9432057, Val MAE: 665.5367051, Test MAE: 436.3925752\n",
      "Epoch: 318, LR: 0.007000, Loss: 171781.7247910, Val MAE: 679.3335402, Test MAE: 436.3925752\n",
      "Epoch: 319, LR: 0.007000, Loss: 170704.9500072, Val MAE: 961.7950270, Test MAE: 436.3925752\n",
      "Epoch: 320, LR: 0.004900, Loss: 171999.5462320, Val MAE: 549.4944879, Test MAE: 436.3925752\n",
      "Epoch: 321, LR: 0.004900, Loss: 172342.5595758, Val MAE: 659.4014471, Test MAE: 436.3925752\n",
      "Epoch: 322, LR: 0.004900, Loss: 171584.2002221, Val MAE: 1072.7421700, Test MAE: 436.3925752\n",
      "Epoch: 323, LR: 0.004900, Loss: 169724.4187288, Val MAE: 854.0843285, Test MAE: 436.3925752\n",
      "Epoch: 324, LR: 0.004900, Loss: 171995.5008432, Val MAE: 470.4044688, Test MAE: 436.3925752\n",
      "Epoch: 325, LR: 0.004900, Loss: 171058.5602649, Val MAE: 601.9677935, Test MAE: 436.3925752\n",
      "Epoch: 326, LR: 0.003430, Loss: 172012.7211759, Val MAE: 950.4797572, Test MAE: 436.3925752\n",
      "Epoch: 327, LR: 0.003430, Loss: 171371.8740279, Val MAE: 698.6472975, Test MAE: 436.3925752\n",
      "Epoch: 328, LR: 0.003430, Loss: 172493.2743634, Val MAE: 585.8745936, Test MAE: 436.3925752\n",
      "Epoch: 329, LR: 0.003430, Loss: 171665.8643147, Val MAE: 1078.3780438, Test MAE: 436.3925752\n",
      "Epoch: 330, LR: 0.003430, Loss: 170753.5009710, Val MAE: 830.8902421, Test MAE: 436.3925752\n",
      "Epoch: 331, LR: 0.003430, Loss: 171429.1172825, Val MAE: 532.5769968, Test MAE: 436.3925752\n",
      "Epoch: 332, LR: 0.002401, Loss: 170397.6743754, Val MAE: 1050.6902647, Test MAE: 436.3925752\n",
      "Epoch: 333, LR: 0.002401, Loss: 171745.7489789, Val MAE: 549.2793129, Test MAE: 436.3925752\n",
      "Epoch: 334, LR: 0.002401, Loss: 171693.2012098, Val MAE: 794.1339803, Test MAE: 436.3925752\n",
      "Epoch: 335, LR: 0.002401, Loss: 171561.6212547, Val MAE: 520.7041250, Test MAE: 436.3925752\n",
      "Epoch: 336, LR: 0.002401, Loss: 172214.2582609, Val MAE: 626.1558483, Test MAE: 436.3925752\n",
      "Epoch: 337, LR: 0.002401, Loss: 171568.0565626, Val MAE: 616.2271123, Test MAE: 436.3925752\n",
      "Epoch: 338, LR: 0.001681, Loss: 170690.8748877, Val MAE: 830.3946085, Test MAE: 436.3925752\n",
      "Epoch: 339, LR: 0.001681, Loss: 170490.8173219, Val MAE: 548.8878621, Test MAE: 436.3925752\n",
      "Epoch: 340, LR: 0.001681, Loss: 171068.8227750, Val MAE: 1074.3269712, Test MAE: 436.3925752\n",
      "Epoch: 341, LR: 0.001681, Loss: 171422.1382184, Val MAE: 570.0277955, Test MAE: 436.3925752\n",
      "Epoch: 342, LR: 0.001681, Loss: 172578.4069018, Val MAE: 688.4638361, Test MAE: 436.3925752\n",
      "Epoch: 343, LR: 0.001681, Loss: 169799.6212810, Val MAE: 1035.3796245, Test MAE: 436.3925752\n",
      "Epoch: 344, LR: 0.001176, Loss: 171443.1500609, Val MAE: 867.1785846, Test MAE: 436.3925752\n",
      "Epoch: 345, LR: 0.001176, Loss: 171997.4000358, Val MAE: 548.5753475, Test MAE: 436.3925752\n",
      "Epoch: 346, LR: 0.001176, Loss: 171207.0621268, Val MAE: 490.9322462, Test MAE: 436.3925752\n",
      "Epoch: 347, LR: 0.001176, Loss: 171222.4275988, Val MAE: 680.6217255, Test MAE: 436.3925752\n",
      "Epoch: 348, LR: 0.001176, Loss: 171101.1057493, Val MAE: 553.5880494, Test MAE: 436.3925752\n",
      "Epoch: 349, LR: 0.001176, Loss: 171001.4503869, Val MAE: 473.7499845, Test MAE: 436.3925752\n",
      "Epoch: 350, LR: 0.000824, Loss: 171671.5049109, Val MAE: 668.5189118, Test MAE: 436.3925752\n",
      "Epoch: 351, LR: 0.000824, Loss: 170785.4412375, Val MAE: 725.2018343, Test MAE: 436.3925752\n",
      "Epoch: 352, LR: 0.000824, Loss: 171510.3521306, Val MAE: 634.1421491, Test MAE: 436.3925752\n",
      "Epoch: 353, LR: 0.000824, Loss: 170464.8429633, Val MAE: 640.1829942, Test MAE: 436.3925752\n",
      "Epoch: 354, LR: 0.000824, Loss: 171035.0167343, Val MAE: 682.1168833, Test MAE: 436.3925752\n",
      "Epoch: 355, LR: 0.000824, Loss: 172422.5272429, Val MAE: 1080.4246462, Test MAE: 436.3925752\n",
      "Epoch: 356, LR: 0.000576, Loss: 172053.0097000, Val MAE: 808.9312194, Test MAE: 436.3925752\n",
      "Epoch: 357, LR: 0.000576, Loss: 171047.2664417, Val MAE: 632.0335900, Test MAE: 436.3925752\n",
      "Epoch: 358, LR: 0.000576, Loss: 171069.4076363, Val MAE: 552.6526470, Test MAE: 436.3925752\n",
      "Epoch: 359, LR: 0.000576, Loss: 172037.6994769, Val MAE: 514.6691823, Test MAE: 436.3925752\n",
      "Epoch: 360, LR: 0.000576, Loss: 170356.4845841, Val MAE: 801.3324899, Test MAE: 436.3925752\n",
      "Epoch: 361, LR: 0.000576, Loss: 172259.3276573, Val MAE: 775.3084444, Test MAE: 436.3925752\n",
      "Epoch: 362, LR: 0.000404, Loss: 171916.7758479, Val MAE: 777.6644779, Test MAE: 436.3925752\n",
      "Epoch: 363, LR: 0.000404, Loss: 171760.2379341, Val MAE: 814.2669173, Test MAE: 436.3925752\n",
      "Epoch: 364, LR: 0.000404, Loss: 171884.2078692, Val MAE: 602.6283281, Test MAE: 436.3925752\n",
      "Epoch: 365, LR: 0.000404, Loss: 169841.6537584, Val MAE: 723.3239330, Test MAE: 436.3925752\n",
      "Epoch: 366, LR: 0.000404, Loss: 169592.8042564, Val MAE: 722.1896548, Test MAE: 436.3925752\n",
      "Epoch: 367, LR: 0.000404, Loss: 172673.6079217, Val MAE: 752.1734182, Test MAE: 436.3925752\n",
      "Epoch: 368, LR: 0.000282, Loss: 171725.9583911, Val MAE: 872.8425841, Test MAE: 436.3925752\n",
      "Epoch: 369, LR: 0.000282, Loss: 171790.5397781, Val MAE: 635.9374281, Test MAE: 436.3925752\n",
      "Epoch: 370, LR: 0.000282, Loss: 170965.7699936, Val MAE: 889.0352543, Test MAE: 436.3925752\n",
      "Epoch: 371, LR: 0.000282, Loss: 171198.2520542, Val MAE: 679.4773366, Test MAE: 436.3925752\n",
      "Epoch: 372, LR: 0.000282, Loss: 172059.6020386, Val MAE: 617.8765181, Test MAE: 436.3925752\n",
      "Epoch: 373, LR: 0.000282, Loss: 171283.2978527, Val MAE: 795.3179280, Test MAE: 436.3925752\n",
      "Epoch: 374, LR: 0.000198, Loss: 171426.9929561, Val MAE: 552.0700663, Test MAE: 436.3925752\n",
      "Epoch: 375, LR: 0.000198, Loss: 171951.3226831, Val MAE: 494.1793666, Test MAE: 436.3925752\n",
      "Epoch: 376, LR: 0.000198, Loss: 171567.3253260, Val MAE: 760.1958749, Test MAE: 436.3925752\n",
      "Epoch: 377, LR: 0.000198, Loss: 171754.9886137, Val MAE: 659.2468270, Test MAE: 436.3925752\n",
      "Epoch: 378, LR: 0.000198, Loss: 171901.0370933, Val MAE: 733.0576141, Test MAE: 436.3925752\n",
      "Epoch: 379, LR: 0.000198, Loss: 171180.2652654, Val MAE: 442.4238098, Test MAE: 421.9882445\n",
      "Epoch: 380, LR: 0.000198, Loss: 171268.1562330, Val MAE: 525.5980002, Test MAE: 421.9882445\n",
      "Epoch: 381, LR: 0.000198, Loss: 171024.4476246, Val MAE: 666.0030011, Test MAE: 421.9882445\n",
      "Epoch: 382, LR: 0.000198, Loss: 171356.5521019, Val MAE: 545.5790205, Test MAE: 421.9882445\n",
      "Epoch: 383, LR: 0.000198, Loss: 172231.6629365, Val MAE: 1103.9832407, Test MAE: 421.9882445\n",
      "Epoch: 384, LR: 0.000198, Loss: 171300.4566319, Val MAE: 636.9163724, Test MAE: 421.9882445\n",
      "Epoch: 385, LR: 0.000198, Loss: 172551.3495187, Val MAE: 683.8529614, Test MAE: 421.9882445\n",
      "Epoch: 386, LR: 0.000138, Loss: 171109.2532473, Val MAE: 697.6355189, Test MAE: 421.9882445\n",
      "Epoch: 387, LR: 0.000138, Loss: 171159.7305260, Val MAE: 519.7655429, Test MAE: 421.9882445\n",
      "Epoch: 388, LR: 0.000138, Loss: 171249.1473093, Val MAE: 604.1620450, Test MAE: 421.9882445\n",
      "Epoch: 389, LR: 0.000138, Loss: 172178.2022512, Val MAE: 817.6350019, Test MAE: 421.9882445\n",
      "Epoch: 390, LR: 0.000138, Loss: 171470.9576446, Val MAE: 670.7478214, Test MAE: 421.9882445\n",
      "Epoch: 391, LR: 0.000138, Loss: 170971.2835989, Val MAE: 567.3982094, Test MAE: 421.9882445\n",
      "Epoch: 392, LR: 0.000097, Loss: 170756.6483435, Val MAE: 824.5895082, Test MAE: 421.9882445\n",
      "Epoch: 393, LR: 0.000097, Loss: 171712.7447320, Val MAE: 914.3679000, Test MAE: 421.9882445\n",
      "Epoch: 394, LR: 0.000097, Loss: 170906.6211126, Val MAE: 644.4928055, Test MAE: 421.9882445\n",
      "Epoch: 395, LR: 0.000097, Loss: 171788.8094862, Val MAE: 725.6605942, Test MAE: 421.9882445\n",
      "Epoch: 396, LR: 0.000097, Loss: 171876.5919433, Val MAE: 527.6738185, Test MAE: 421.9882445\n",
      "Epoch: 397, LR: 0.000097, Loss: 170559.7432427, Val MAE: 903.1810262, Test MAE: 421.9882445\n",
      "Epoch: 398, LR: 0.000068, Loss: 171706.7540092, Val MAE: 513.1109561, Test MAE: 421.9882445\n",
      "Epoch: 399, LR: 0.000068, Loss: 171361.8400611, Val MAE: 1107.8136044, Test MAE: 421.9882445\n",
      "Epoch: 400, LR: 0.000068, Loss: 172970.8735012, Val MAE: 650.3755138, Test MAE: 421.9882445\n"
     ]
    }
   ],
   "source": [
    "total_num_epoch = 300\n",
    "\n",
    "best_validation_error = None\n",
    "num_epoch = 100\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    \n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "\n",
    "    print(f'Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d79432-e9f6-4704-8439-626145aa5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# pred = model(test_loader)\n",
    "# y = torch.index_select(batch.y, 1, torch.tensor(10))\n",
    "# correct = (pred == data.y[data.test_mask]).sum()\n",
    "# acc = int(correct) / int(data.test_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12daac1a-5d8f-453b-8a0a-1ef7ffcf5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental = [x for x in true_all]\n",
    "# prediction = [x for x in out_all]\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.plot(range(-1, 16), range(-1,16), 'r--')\n",
    "# plt.scatter(experimental, prediction, marker = '.')\n",
    "# plt.xlabel(\"Experimental\", fontsize='xx-large')\n",
    "# plt.ylabel(\"Prediction\", fontsize='xx-large')\n",
    "# plt.xlim(-0.5, 12)\n",
    "# plt.ylim(-0.5, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47fd74-e679-4740-92f2-0f1454ff6f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3cf48-f736-43ee-ace3-30e161313022",
   "metadata": {},
   "source": [
    "https://tutorials.pytorch.kr/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a25f881b-738e-4bcf-b686-b6ab674111a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'model/GCNConv01_epoch300_20211207')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2761ce05-665e-4212-9c35-104929cc5d6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/GCNConv02_epoch300_20211216'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63020/1731111237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/GCNConv02_epoch300_20211216'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = torch.load('model/GCNConv01-2_epoch300_20211207')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/GCNConv02_epoch300_20211216'"
     ]
    }
   ],
   "source": [
    "model = torch.load('model/GCNConv02_epoch300_20211216')\n",
    "\n",
    "#model = torch.load('model/GCNConv01-2_epoch300_20211207')\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9bc2e-5764-46cf-87cc-b009cbc6912a",
   "metadata": {},
   "source": [
    "## save checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a585d8-2062-422d-878f-86d42b224e90",
   "metadata": {},
   "source": [
    "to save checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa94482-01de-42bb-9374-6b9eb9e6ae20",
   "metadata": {},
   "source": [
    "general code  \n",
    "Ref : https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222caa07-9832-4a57-9f17-0cfdf2485b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional information\n",
    "# EPOCH = 5\n",
    "# PATH = \"model.pt\"\n",
    "# LOSS = 0.4\n",
    "\n",
    "# torch.save({\n",
    "#             'epoch': EPOCH,\n",
    "#             'model_state_dict': net.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': LOSS,\n",
    "#             }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e09d54c3-5994-4034-b4c1-bd2fcf2adbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOPCH = total_num_epoch\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EOPCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model/GCNConv01_epoch400_20211207')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661432c-786b-4075-a464-b7110043083d",
   "metadata": {},
   "source": [
    "To load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a03e2901-c4f5-4b66-b09f-13cd20d9aa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(11, 80)\n",
       "  (conv2): GCNConv(80, 80)\n",
       "  (conv3): GCNConv(80, 80)\n",
       "  (conv4): GCNConv(80, 80)\n",
       "  (conv5): GCNConv(80, 80)\n",
       "  (lin1): Linear(in_features=80, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = 'model/GCNConv01_epoch400_20211207'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23ece6-b132-4428-9783-837fc03c3a84",
   "metadata": {},
   "source": [
    "# Model\n",
    "- message passing neural network\n",
    "    - attention\n",
    "    - gate\n",
    "    - Directed-MPNN\n",
    "- data augmentation : \n",
    "    - global feature\n",
    "    - the concatenation of the elarned molecule feature vector and the commputed global feature on the readout phase\n",
    "- cross validation\n",
    "- pretrain\n",
    "- skip connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe204b79-d5f3-43bb-a790-44c193c37f4a",
   "metadata": {},
   "source": [
    "## GCN_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc07b8c3-3912-4612-8c9c-4de47f327e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "#from torch_geometric.nn import global_mean_pool\n",
    "#from torch_geometric.nn import global_max_pool\n",
    "\n",
    "class GCN_skip(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.num_node_features = train_set.num_node_features\n",
    "        self.num_hidden = 80\n",
    "        \n",
    "        # model structure\n",
    "        self.conv1 = GCNConv(self.num_node_features, self.num_hidden)\n",
    "        self.conv2 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv3 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv4 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv5 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv6 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        self.conv7 = GCNConv(self.num_hidden, self.num_hidden)\n",
    "        #self.lin1 = Linear(self.num_hidden, self.num_hidden)\n",
    "        self.lin1 = Linear(self.num_hidden+self.num_node_features, self.num_hidden)\n",
    "        self.lin2 = Linear(self.num_hidden, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x_original = data.x\n",
    "        #print(x.device)\n",
    "        #print(x_original.device)\n",
    "        #x_original = x.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv6(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv7(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.cat((x, x_original), dim=1)\n",
    "        #print(x.size())\n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        #x = self.lin1(x)\n",
    "        x = self.lin1(x) #skip connect\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "            #[32, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6ff6e5-4804-4c5b-b8b7-41fd32aea652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN_skip(\n",
       "  (conv1): GCNConv(11, 80)\n",
       "  (conv2): GCNConv(80, 80)\n",
       "  (conv3): GCNConv(80, 80)\n",
       "  (conv4): GCNConv(80, 80)\n",
       "  (conv5): GCNConv(80, 80)\n",
       "  (conv6): GCNConv(80, 80)\n",
       "  (conv7): GCNConv(80, 80)\n",
       "  (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCN_skip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bab92b82-3551-4d28-ba4f-fd79bb467564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN_skip().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.8, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b6d4bd2f-4305-4b23-975d-792dfce9e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a1177b6d-77d7-4b36-88b4-6c05fe4f3c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_skip(\n",
      "  (conv1): GCNConv(11, 80)\n",
      "  (conv2): GCNConv(80, 80)\n",
      "  (conv3): GCNConv(80, 80)\n",
      "  (conv4): GCNConv(80, 80)\n",
      "  (conv5): GCNConv(80, 80)\n",
      "  (conv6): GCNConv(80, 80)\n",
      "  (conv7): GCNConv(80, 80)\n",
      "  (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
      "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c12197aa-013c-4d87-b7b3-6d73d2e15f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|     Modules      | Parameters |\n",
      "+------------------+------------+\n",
      "|    conv1.bias    |     80     |\n",
      "| conv1.lin.weight |    880     |\n",
      "|    conv2.bias    |     80     |\n",
      "| conv2.lin.weight |    6400    |\n",
      "|    conv3.bias    |     80     |\n",
      "| conv3.lin.weight |    6400    |\n",
      "|    conv4.bias    |     80     |\n",
      "| conv4.lin.weight |    6400    |\n",
      "|    conv5.bias    |     80     |\n",
      "| conv5.lin.weight |    6400    |\n",
      "|    conv6.bias    |     80     |\n",
      "| conv6.lin.weight |    6400    |\n",
      "|    conv7.bias    |     80     |\n",
      "| conv7.lin.weight |    6400    |\n",
      "|   lin1.weight    |    7280    |\n",
      "|    lin1.bias     |     80     |\n",
      "|   lin2.weight    |     80     |\n",
      "|    lin2.bias     |     1      |\n",
      "+------------------+------------+\n",
      "Total Trainable Params: 47281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47281"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b149d790-015b-4e1c-8380-e82e6bae65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device))\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y.to(device)) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "01956f6d-f13b-498c-a574-a6906829fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device) #it trigger error!\n",
    "        out = model(batch)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device))\n",
    "        tmp = (out - y)**2\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a52c4256-5126-4e49-9471-015f8d61ee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125653546.22640067"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check any error\n",
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f7a36333-cc9c-4585-b328-b40a4ac924b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTal Epoch: 001, LR: 0.010000, Loss: 553035.5833827, Val MAE: 28769.0459996, Test MAE: 28667.6621689, Time: 46.35320591926575\n",
      "ToTal Epoch: 002, LR: 0.010000, Loss: 259212.2142335, Val MAE: 28819.2577008, Test MAE: 28667.6621689, Time: 40.69047927856445\n",
      "ToTal Epoch: 003, LR: 0.010000, Loss: 238199.6275450, Val MAE: 61601.1636857, Test MAE: 28667.6621689, Time: 44.202409982681274\n",
      "ToTal Epoch: 004, LR: 0.010000, Loss: 107959409268.3384857, Val MAE: 59562.8773743, Test MAE: 28667.6621689, Time: 42.927422285079956\n",
      "ToTal Epoch: 005, LR: 0.010000, Loss: 260502.8809631, Val MAE: 107356.7166743, Test MAE: 28667.6621689, Time: 42.91780114173889\n",
      "ToTal Epoch: 006, LR: 0.010000, Loss: 256944.0292791, Val MAE: 35695.8297695, Test MAE: 28667.6621689, Time: 43.42213034629822\n",
      "ToTal Epoch: 007, LR: 0.010000, Loss: 249950.2496775, Val MAE: 34070.1182236, Test MAE: 28667.6621689, Time: 46.07858180999756\n",
      "ToTal Epoch: 008, LR: 0.008000, Loss: 249127.1324798, Val MAE: 42020.9864471, Test MAE: 28667.6621689, Time: 43.267706632614136\n",
      "ToTal Epoch: 009, LR: 0.008000, Loss: 245801.1693785, Val MAE: 23743.8837661, Test MAE: 23356.6081652, Time: 46.574028730392456\n",
      "ToTal Epoch: 010, LR: 0.008000, Loss: 238666.1730545, Val MAE: 62884.9395876, Test MAE: 23356.6081652, Time: 40.78993463516235\n",
      "ToTal Epoch: 011, LR: 0.008000, Loss: 237146.8778484, Val MAE: 99230.6524880, Test MAE: 23356.6081652, Time: 40.831050395965576\n",
      "ToTal Epoch: 012, LR: 0.008000, Loss: 320530.8855420, Val MAE: 39124.4356608, Test MAE: 23356.6081652, Time: 42.20411562919617\n",
      "ToTal Epoch: 013, LR: 0.008000, Loss: 238817.0794439, Val MAE: 97904.6145953, Test MAE: 23356.6081652, Time: 46.4033305644989\n",
      "ToTal Epoch: 014, LR: 0.008000, Loss: 209843578.6777074, Val MAE: 3575512.2445922, Test MAE: 23356.6081652, Time: 41.60753798484802\n",
      "ToTal Epoch: 015, LR: 0.008000, Loss: 2588365.4834376, Val MAE: 957571.6477108, Test MAE: 23356.6081652, Time: 44.48343896865845\n",
      "ToTal Epoch: 016, LR: 0.006400, Loss: 563260.8727631, Val MAE: 49482.7316675, Test MAE: 23356.6081652, Time: 45.24949312210083\n",
      "ToTal Epoch: 017, LR: 0.006400, Loss: 201281.5208618, Val MAE: 6828.5272119, Test MAE: 7615.7223452, Time: 46.86336541175842\n",
      "ToTal Epoch: 018, LR: 0.006400, Loss: 182346.6063094, Val MAE: 3041.4822038, Test MAE: 3296.0939573, Time: 47.91636657714844\n",
      "ToTal Epoch: 019, LR: 0.006400, Loss: 181966.3323938, Val MAE: 947.8050426, Test MAE: 1029.8576968, Time: 50.327613830566406\n",
      "ToTal Epoch: 020, LR: 0.006400, Loss: 185843.6687264, Val MAE: 12224.7589357, Test MAE: 1029.8576968, Time: 43.827948331832886\n",
      "ToTal Epoch: 021, LR: 0.006400, Loss: 185579.6883497, Val MAE: 725.4353499, Test MAE: 748.6534057, Time: 48.24854564666748\n",
      "ToTal Epoch: 022, LR: 0.006400, Loss: 186946.0736743, Val MAE: 524.8768902, Test MAE: 553.5089216, Time: 49.35134673118591\n",
      "ToTal Epoch: 023, LR: 0.006400, Loss: 187567.1600965, Val MAE: 88449.4377245, Test MAE: 553.5089216, Time: 47.6989963054657\n",
      "ToTal Epoch: 024, LR: 0.006400, Loss: 187638.4061147, Val MAE: 430.9925801, Test MAE: 447.0925520, Time: 51.013535022735596\n",
      "ToTal Epoch: 025, LR: 0.006400, Loss: 1000675180815.3427734, Val MAE: 270404.3082817, Test MAE: 447.0925520, Time: 44.42919564247131\n",
      "ToTal Epoch: 026, LR: 0.006400, Loss: 2460653.4693546, Val MAE: 118810.6166829, Test MAE: 447.0925520, Time: 46.08629584312439\n",
      "ToTal Epoch: 027, LR: 0.006400, Loss: 846195.0105193, Val MAE: 57928.4747836, Test MAE: 447.0925520, Time: 47.25566339492798\n",
      "ToTal Epoch: 028, LR: 0.006400, Loss: 378676.2883915, Val MAE: 143909.9118035, Test MAE: 447.0925520, Time: 46.56023955345154\n",
      "ToTal Epoch: 029, LR: 0.006400, Loss: 290275.8476066, Val MAE: 49497.8463416, Test MAE: 447.0925520, Time: 46.88531136512756\n",
      "ToTal Epoch: 030, LR: 0.006400, Loss: 277937.3397602, Val MAE: 47158.9415224, Test MAE: 447.0925520, Time: 44.628151655197144\n",
      "ToTal Epoch: 031, LR: 0.005120, Loss: 240790.6326100, Val MAE: 157392.6318123, Test MAE: 447.0925520, Time: 43.95828342437744\n",
      "ToTal Epoch: 032, LR: 0.005120, Loss: 242739.4956265, Val MAE: 70604.8635061, Test MAE: 447.0925520, Time: 40.084898233413696\n",
      "ToTal Epoch: 033, LR: 0.005120, Loss: 234087.5849568, Val MAE: 14153.2694551, Test MAE: 447.0925520, Time: 44.70027470588684\n",
      "ToTal Epoch: 034, LR: 0.005120, Loss: 234655.1199661, Val MAE: 30114.7810852, Test MAE: 447.0925520, Time: 46.952293157577515\n",
      "ToTal Epoch: 035, LR: 0.005120, Loss: 234420.7868951, Val MAE: 18053.5813747, Test MAE: 447.0925520, Time: 43.779381275177\n",
      "ToTal Epoch: 036, LR: 0.005120, Loss: 225644.7502102, Val MAE: 69063.3502159, Test MAE: 447.0925520, Time: 46.16984701156616\n",
      "ToTal Epoch: 037, LR: 0.004096, Loss: 212084.2842748, Val MAE: 14205.4974633, Test MAE: 447.0925520, Time: 42.94539546966553\n",
      "ToTal Epoch: 038, LR: 0.004096, Loss: 1548095938.1557446, Val MAE: 22125.7095993, Test MAE: 447.0925520, Time: 47.59788751602173\n",
      "ToTal Epoch: 039, LR: 0.004096, Loss: 257571.7683514, Val MAE: 35217.2255886, Test MAE: 447.0925520, Time: 48.06795001029968\n",
      "ToTal Epoch: 040, LR: 0.004096, Loss: 213889.0309440, Val MAE: 11304.5337688, Test MAE: 447.0925520, Time: 41.56908559799194\n",
      "ToTal Epoch: 041, LR: 0.004096, Loss: 212303.7890102, Val MAE: 8232.7952717, Test MAE: 447.0925520, Time: 45.77178931236267\n",
      "ToTal Epoch: 042, LR: 0.004096, Loss: 198458.0005983, Val MAE: 2495.7238440, Test MAE: 447.0925520, Time: 44.3441104888916\n",
      "ToTal Epoch: 043, LR: 0.003277, Loss: 194687.5286760, Val MAE: 8705.6919679, Test MAE: 447.0925520, Time: 43.96966028213501\n",
      "ToTal Epoch: 044, LR: 0.003277, Loss: 194710.2776215, Val MAE: 1946.4782608, Test MAE: 447.0925520, Time: 42.236377000808716\n",
      "ToTal Epoch: 045, LR: 0.003277, Loss: 192813.5891798, Val MAE: 38362.3577543, Test MAE: 447.0925520, Time: 45.365466594696045\n",
      "ToTal Epoch: 046, LR: 0.003277, Loss: 189349.5701787, Val MAE: 20514.8084728, Test MAE: 447.0925520, Time: 43.24799346923828\n",
      "ToTal Epoch: 047, LR: 0.003277, Loss: 189902.2354309, Val MAE: 25189.9417899, Test MAE: 447.0925520, Time: 43.228718757629395\n",
      "ToTal Epoch: 048, LR: 0.003277, Loss: 23577201.8208331, Val MAE: 31034.7772730, Test MAE: 447.0925520, Time: 41.796141624450684\n",
      "ToTal Epoch: 049, LR: 0.002621, Loss: 265479.2701070, Val MAE: 60801.4625659, Test MAE: 447.0925520, Time: 40.88695526123047\n",
      "ToTal Epoch: 050, LR: 0.002621, Loss: 198284.3186141, Val MAE: 8038.2795147, Test MAE: 447.0925520, Time: 41.68929433822632\n",
      "ToTal Epoch: 051, LR: 0.002621, Loss: 186825.7557087, Val MAE: 29508.2593299, Test MAE: 447.0925520, Time: 43.06305432319641\n",
      "ToTal Epoch: 052, LR: 0.002621, Loss: 186213.6785387, Val MAE: 9729.4022802, Test MAE: 447.0925520, Time: 43.120203256607056\n",
      "ToTal Epoch: 053, LR: 0.002621, Loss: 188782.0246202, Val MAE: 34662.9075852, Test MAE: 447.0925520, Time: 45.534186363220215\n",
      "ToTal Epoch: 054, LR: 0.002621, Loss: 186374.8605229, Val MAE: 2272.0738034, Test MAE: 447.0925520, Time: 40.51668047904968\n",
      "ToTal Epoch: 055, LR: 0.002097, Loss: 183704.4045335, Val MAE: 1841.2223308, Test MAE: 447.0925520, Time: 45.252991914749146\n",
      "ToTal Epoch: 056, LR: 0.002097, Loss: 183694.7228730, Val MAE: 459.6702197, Test MAE: 447.0925520, Time: 43.12139678001404\n",
      "ToTal Epoch: 057, LR: 0.002097, Loss: 184264.3827354, Val MAE: 596.3208853, Test MAE: 447.0925520, Time: 41.741474628448486\n",
      "ToTal Epoch: 058, LR: 0.002097, Loss: 182265.9318731, Val MAE: 28939.7343499, Test MAE: 447.0925520, Time: 41.75444507598877\n",
      "ToTal Epoch: 059, LR: 0.002097, Loss: 181491.1328453, Val MAE: 881.4674309, Test MAE: 447.0925520, Time: 43.96956539154053\n",
      "ToTal Epoch: 060, LR: 0.002097, Loss: 182047.0763065, Val MAE: 8832.2001333, Test MAE: 447.0925520, Time: 42.555665016174316\n",
      "ToTal Epoch: 061, LR: 0.001678, Loss: 178925.4333588, Val MAE: 4772.0588174, Test MAE: 447.0925520, Time: 44.910958766937256\n",
      "ToTal Epoch: 062, LR: 0.001678, Loss: 180827.3296864, Val MAE: 5322.7137005, Test MAE: 447.0925520, Time: 42.72331500053406\n",
      "ToTal Epoch: 063, LR: 0.001678, Loss: 179399.0046780, Val MAE: 2204.1367641, Test MAE: 447.0925520, Time: 43.247854471206665\n",
      "ToTal Epoch: 064, LR: 0.001678, Loss: 178558.4627203, Val MAE: 1522.6147496, Test MAE: 447.0925520, Time: 43.677932024002075\n",
      "ToTal Epoch: 065, LR: 0.001678, Loss: 179985.3018906, Val MAE: 1113.9899600, Test MAE: 447.0925520, Time: 42.32131743431091\n",
      "ToTal Epoch: 066, LR: 0.001678, Loss: 178271.0640341, Val MAE: 919.0896270, Test MAE: 447.0925520, Time: 40.762934923172\n",
      "ToTal Epoch: 067, LR: 0.001342, Loss: 177716.5838294, Val MAE: 1274.9219162, Test MAE: 447.0925520, Time: 45.64718508720398\n",
      "ToTal Epoch: 068, LR: 0.001342, Loss: 177724.2777791, Val MAE: 3929.7741230, Test MAE: 447.0925520, Time: 44.64923930168152\n",
      "ToTal Epoch: 069, LR: 0.001342, Loss: 178328.1492022, Val MAE: 419.8739790, Test MAE: 449.7359215, Time: 53.37469553947449\n",
      "ToTal Epoch: 070, LR: 0.001342, Loss: 176299.6278460, Val MAE: 486.6904537, Test MAE: 449.7359215, Time: 48.03404998779297\n",
      "ToTal Epoch: 071, LR: 0.001342, Loss: 177827.2870945, Val MAE: 7060.9058619, Test MAE: 449.7359215, Time: 41.73629593849182\n",
      "ToTal Epoch: 072, LR: 0.001342, Loss: 177282.7797473, Val MAE: 25473.8275768, Test MAE: 449.7359215, Time: 45.578641176223755\n",
      "ToTal Epoch: 073, LR: 0.001342, Loss: 178056.7916567, Val MAE: 6482.1364856, Test MAE: 449.7359215, Time: 43.12838840484619\n",
      "ToTal Epoch: 074, LR: 0.001342, Loss: 177453.8724621, Val MAE: 231.6647733, Test MAE: 253.3072042, Time: 45.5701220035553\n",
      "ToTal Epoch: 075, LR: 0.001342, Loss: 177636.8550900, Val MAE: 12523.4445607, Test MAE: 253.3072042, Time: 43.12212514877319\n",
      "ToTal Epoch: 076, LR: 0.001342, Loss: 177533.2961436, Val MAE: 1347.6645895, Test MAE: 253.3072042, Time: 42.587401390075684\n",
      "ToTal Epoch: 077, LR: 0.001342, Loss: 177263.2808293, Val MAE: 933.7250826, Test MAE: 253.3072042, Time: 42.596118211746216\n",
      "ToTal Epoch: 078, LR: 0.001342, Loss: 178084.0832310, Val MAE: 817.5606486, Test MAE: 253.3072042, Time: 43.0933575630188\n",
      "ToTal Epoch: 079, LR: 0.001342, Loss: 176134.0284742, Val MAE: 5718.1488047, Test MAE: 253.3072042, Time: 43.332417249679565\n",
      "ToTal Epoch: 080, LR: 0.001342, Loss: 177774.3787202, Val MAE: 7336.6534422, Test MAE: 253.3072042, Time: 42.12733721733093\n",
      "ToTal Epoch: 081, LR: 0.001074, Loss: 175847.6436380, Val MAE: 437.5752726, Test MAE: 253.3072042, Time: 41.48648762702942\n",
      "ToTal Epoch: 082, LR: 0.001074, Loss: 176211.9351741, Val MAE: 3645.6309625, Test MAE: 253.3072042, Time: 41.41871428489685\n",
      "ToTal Epoch: 083, LR: 0.001074, Loss: 176393.6334544, Val MAE: 1141.4668530, Test MAE: 253.3072042, Time: 41.28584671020508\n",
      "ToTal Epoch: 084, LR: 0.001074, Loss: 175608.7477894, Val MAE: 3474.7636574, Test MAE: 253.3072042, Time: 42.05073356628418\n",
      "ToTal Epoch: 085, LR: 0.001074, Loss: 174651.1222090, Val MAE: 1642.4499001, Test MAE: 253.3072042, Time: 43.97773003578186\n",
      "ToTal Epoch: 086, LR: 0.001074, Loss: 175504.3748507, Val MAE: 1115.9672837, Test MAE: 253.3072042, Time: 40.54179763793945\n",
      "ToTal Epoch: 087, LR: 0.000859, Loss: 175076.8699314, Val MAE: 6289.7464386, Test MAE: 253.3072042, Time: 42.985883951187134\n",
      "ToTal Epoch: 088, LR: 0.000859, Loss: 176578.4933956, Val MAE: 179.3914171, Test MAE: 199.4256922, Time: 49.04043769836426\n",
      "ToTal Epoch: 089, LR: 0.000859, Loss: 174706.3108477, Val MAE: 1734.9117241, Test MAE: 199.4256922, Time: 44.13520002365112\n",
      "ToTal Epoch: 090, LR: 0.000859, Loss: 174667.8306287, Val MAE: 1244.1829632, Test MAE: 199.4256922, Time: 43.002196073532104\n",
      "ToTal Epoch: 091, LR: 0.000859, Loss: 173524.5660918, Val MAE: 4877.3902930, Test MAE: 199.4256922, Time: 42.43696331977844\n",
      "ToTal Epoch: 092, LR: 0.000859, Loss: 174304.1297353, Val MAE: 2864.0707556, Test MAE: 199.4256922, Time: 44.27487111091614\n",
      "ToTal Epoch: 093, LR: 0.000859, Loss: 175620.1493336, Val MAE: 900.7379869, Test MAE: 199.4256922, Time: 44.59315609931946\n",
      "ToTal Epoch: 094, LR: 0.000859, Loss: 174240.7601801, Val MAE: 19416.6668195, Test MAE: 199.4256922, Time: 41.8269157409668\n",
      "ToTal Epoch: 095, LR: 0.000687, Loss: 174309.0784145, Val MAE: 1036.0699184, Test MAE: 199.4256922, Time: 41.09200620651245\n",
      "ToTal Epoch: 096, LR: 0.000687, Loss: 173189.9982707, Val MAE: 4169.1208898, Test MAE: 199.4256922, Time: 44.280373334884644\n",
      "ToTal Epoch: 097, LR: 0.000687, Loss: 173170.1268308, Val MAE: 584.4346830, Test MAE: 199.4256922, Time: 40.2862651348114\n",
      "ToTal Epoch: 098, LR: 0.000687, Loss: 174323.0741867, Val MAE: 757.5644133, Test MAE: 199.4256922, Time: 43.06945848464966\n",
      "ToTal Epoch: 099, LR: 0.000687, Loss: 173083.6324727, Val MAE: 534.3249437, Test MAE: 199.4256922, Time: 44.27316451072693\n",
      "ToTal Epoch: 100, LR: 0.000687, Loss: 174961.2821550, Val MAE: 4496.7551647, Test MAE: 199.4256922, Time: 43.28783059120178\n",
      "ToTal Epoch: 101, LR: 0.000550, Loss: 173377.9393410, Val MAE: 4485.6283219, Test MAE: 199.4256922, Time: 43.61660146713257\n",
      "ToTal Epoch: 102, LR: 0.000550, Loss: 173512.5069722, Val MAE: 254.6441090, Test MAE: 199.4256922, Time: 43.782448291778564\n",
      "ToTal Epoch: 103, LR: 0.000550, Loss: 173398.9519455, Val MAE: 282.6330273, Test MAE: 199.4256922, Time: 44.433842420578\n",
      "ToTal Epoch: 104, LR: 0.000550, Loss: 173905.9228180, Val MAE: 674.3338514, Test MAE: 199.4256922, Time: 43.23713517189026\n",
      "ToTal Epoch: 105, LR: 0.000550, Loss: 174578.1365428, Val MAE: 2883.4812215, Test MAE: 199.4256922, Time: 46.751134395599365\n",
      "ToTal Epoch: 106, LR: 0.000550, Loss: 173143.6422897, Val MAE: 717.3958468, Test MAE: 199.4256922, Time: 45.51163411140442\n",
      "ToTal Epoch: 107, LR: 0.000440, Loss: 173310.4105718, Val MAE: 3438.8340236, Test MAE: 199.4256922, Time: 50.12570929527283\n",
      "ToTal Epoch: 108, LR: 0.000440, Loss: 171325.3430134, Val MAE: 692.2655925, Test MAE: 199.4256922, Time: 42.45194339752197\n",
      "ToTal Epoch: 109, LR: 0.000440, Loss: 171497.7900874, Val MAE: 602.0870250, Test MAE: 199.4256922, Time: 49.44169020652771\n",
      "ToTal Epoch: 110, LR: 0.000440, Loss: 174562.7477953, Val MAE: 421.4019753, Test MAE: 199.4256922, Time: 48.54708766937256\n",
      "ToTal Epoch: 111, LR: 0.000440, Loss: 172123.8519013, Val MAE: 1736.7409248, Test MAE: 199.4256922, Time: 47.87796092033386\n",
      "ToTal Epoch: 112, LR: 0.000440, Loss: 172786.2414370, Val MAE: 1533.5058123, Test MAE: 199.4256922, Time: 41.903892517089844\n",
      "ToTal Epoch: 113, LR: 0.000352, Loss: 170395.2930457, Val MAE: 288.8873582, Test MAE: 199.4256922, Time: 47.04447555541992\n",
      "ToTal Epoch: 114, LR: 0.000352, Loss: 173487.1507846, Val MAE: 628.7742884, Test MAE: 199.4256922, Time: 43.639368772506714\n",
      "ToTal Epoch: 115, LR: 0.000352, Loss: 173038.4014785, Val MAE: 1109.4430704, Test MAE: 199.4256922, Time: 43.096394300460815\n",
      "ToTal Epoch: 116, LR: 0.000352, Loss: 172974.3760450, Val MAE: 197.5123082, Test MAE: 199.4256922, Time: 44.22995185852051\n",
      "ToTal Epoch: 117, LR: 0.000352, Loss: 172553.6581952, Val MAE: 357.9310771, Test MAE: 199.4256922, Time: 42.44368839263916\n",
      "ToTal Epoch: 118, LR: 0.000352, Loss: 172546.9770649, Val MAE: 596.8689568, Test MAE: 199.4256922, Time: 42.45566129684448\n",
      "ToTal Epoch: 119, LR: 0.000281, Loss: 171230.8489299, Val MAE: 2342.0734693, Test MAE: 199.4256922, Time: 41.13665509223938\n",
      "ToTal Epoch: 120, LR: 0.000281, Loss: 172604.8057385, Val MAE: 2147.2177148, Test MAE: 199.4256922, Time: 43.764267921447754\n",
      "ToTal Epoch: 121, LR: 0.000281, Loss: 173581.4395297, Val MAE: 427.4508843, Test MAE: 199.4256922, Time: 45.69661831855774\n",
      "ToTal Epoch: 122, LR: 0.000281, Loss: 173503.5310801, Val MAE: 389.6501315, Test MAE: 199.4256922, Time: 42.72387957572937\n",
      "ToTal Epoch: 123, LR: 0.000281, Loss: 172068.6783762, Val MAE: 841.6106314, Test MAE: 199.4256922, Time: 43.81377196311951\n",
      "ToTal Epoch: 124, LR: 0.000281, Loss: 172347.2468614, Val MAE: 1864.3979213, Test MAE: 199.4256922, Time: 41.433385610580444\n",
      "ToTal Epoch: 125, LR: 0.000225, Loss: 173659.4575443, Val MAE: 3188.5510306, Test MAE: 199.4256922, Time: 40.69850826263428\n",
      "ToTal Epoch: 126, LR: 0.000225, Loss: 173299.8889743, Val MAE: 194.8593646, Test MAE: 199.4256922, Time: 44.58729648590088\n",
      "ToTal Epoch: 127, LR: 0.000225, Loss: 173203.5289208, Val MAE: 596.3352249, Test MAE: 199.4256922, Time: 42.21395230293274\n",
      "ToTal Epoch: 128, LR: 0.000225, Loss: 172006.5243372, Val MAE: 1566.6024024, Test MAE: 199.4256922, Time: 41.76594066619873\n",
      "ToTal Epoch: 129, LR: 0.000225, Loss: 171508.7590885, Val MAE: 223.1753205, Test MAE: 199.4256922, Time: 41.87207531929016\n",
      "ToTal Epoch: 130, LR: 0.000225, Loss: 172336.0753774, Val MAE: 1006.1942879, Test MAE: 199.4256922, Time: 44.40715551376343\n",
      "ToTal Epoch: 131, LR: 0.000180, Loss: 171963.8489825, Val MAE: 275.4917815, Test MAE: 199.4256922, Time: 41.26940965652466\n",
      "ToTal Epoch: 132, LR: 0.000180, Loss: 172568.2946018, Val MAE: 3113.8357201, Test MAE: 199.4256922, Time: 44.28684091567993\n",
      "ToTal Epoch: 133, LR: 0.000180, Loss: 173345.5697320, Val MAE: 487.1832823, Test MAE: 199.4256922, Time: 43.531723737716675\n",
      "ToTal Epoch: 134, LR: 0.000180, Loss: 172568.4095089, Val MAE: 1734.8516506, Test MAE: 199.4256922, Time: 41.37023711204529\n",
      "ToTal Epoch: 135, LR: 0.000180, Loss: 173487.4948478, Val MAE: 284.0962267, Test MAE: 199.4256922, Time: 47.304335594177246\n",
      "ToTal Epoch: 136, LR: 0.000180, Loss: 172020.1440763, Val MAE: 174.0345488, Test MAE: 191.9121637, Time: 43.68720626831055\n",
      "ToTal Epoch: 137, LR: 0.000180, Loss: 171447.2751959, Val MAE: 174.3368161, Test MAE: 191.9121637, Time: 40.63088870048523\n",
      "ToTal Epoch: 138, LR: 0.000180, Loss: 172745.5477476, Val MAE: 272.2490254, Test MAE: 191.9121637, Time: 41.986021518707275\n",
      "ToTal Epoch: 139, LR: 0.000180, Loss: 172636.4864054, Val MAE: 483.1971690, Test MAE: 191.9121637, Time: 43.24631977081299\n",
      "ToTal Epoch: 140, LR: 0.000180, Loss: 172045.4808269, Val MAE: 1675.5527229, Test MAE: 191.9121637, Time: 41.364755153656006\n",
      "ToTal Epoch: 141, LR: 0.000180, Loss: 172186.6047760, Val MAE: 161.4126442, Test MAE: 178.1307321, Time: 47.02961826324463\n",
      "ToTal Epoch: 142, LR: 0.000180, Loss: 173324.6148904, Val MAE: 165.3023795, Test MAE: 178.1307321, Time: 44.09342336654663\n",
      "ToTal Epoch: 143, LR: 0.000180, Loss: 172582.9452754, Val MAE: 1430.2242808, Test MAE: 178.1307321, Time: 41.867281675338745\n",
      "ToTal Epoch: 144, LR: 0.000180, Loss: 172663.0365153, Val MAE: 1118.8408897, Test MAE: 178.1307321, Time: 42.70696139335632\n",
      "ToTal Epoch: 145, LR: 0.000180, Loss: 173262.3762170, Val MAE: 401.6773912, Test MAE: 178.1307321, Time: 41.27985143661499\n",
      "ToTal Epoch: 146, LR: 0.000180, Loss: 171330.8303014, Val MAE: 444.4549568, Test MAE: 178.1307321, Time: 41.42747521400452\n",
      "ToTal Epoch: 147, LR: 0.000180, Loss: 172972.1435007, Val MAE: 2172.8007114, Test MAE: 178.1307321, Time: 41.18874454498291\n",
      "ToTal Epoch: 148, LR: 0.000144, Loss: 171731.7838747, Val MAE: 173.6166528, Test MAE: 178.1307321, Time: 46.58031487464905\n",
      "ToTal Epoch: 149, LR: 0.000144, Loss: 171610.9405114, Val MAE: 444.8329240, Test MAE: 178.1307321, Time: 43.023048639297485\n",
      "ToTal Epoch: 150, LR: 0.000144, Loss: 172080.1276394, Val MAE: 680.8292745, Test MAE: 178.1307321, Time: 39.93904185295105\n",
      "ToTal Epoch: 151, LR: 0.000144, Loss: 172366.4430588, Val MAE: 911.8841528, Test MAE: 178.1307321, Time: 40.84382700920105\n",
      "ToTal Epoch: 152, LR: 0.000144, Loss: 171568.6480700, Val MAE: 3360.4386931, Test MAE: 178.1307321, Time: 41.57902431488037\n",
      "ToTal Epoch: 153, LR: 0.000144, Loss: 171568.3074177, Val MAE: 1618.9221900, Test MAE: 178.1307321, Time: 42.544105768203735\n",
      "ToTal Epoch: 154, LR: 0.000115, Loss: 171557.9427614, Val MAE: 488.0405689, Test MAE: 178.1307321, Time: 41.59089660644531\n",
      "ToTal Epoch: 155, LR: 0.000115, Loss: 171764.4246357, Val MAE: 820.8678793, Test MAE: 178.1307321, Time: 45.30052328109741\n",
      "ToTal Epoch: 156, LR: 0.000115, Loss: 172930.4680158, Val MAE: 294.2215422, Test MAE: 178.1307321, Time: 42.28913474082947\n",
      "ToTal Epoch: 157, LR: 0.000115, Loss: 171631.6232946, Val MAE: 190.6176407, Test MAE: 178.1307321, Time: 43.8273766040802\n",
      "ToTal Epoch: 158, LR: 0.000115, Loss: 171286.1459872, Val MAE: 4187.5891800, Test MAE: 178.1307321, Time: 40.43297266960144\n",
      "ToTal Epoch: 159, LR: 0.000115, Loss: 172443.7321168, Val MAE: 222.7072892, Test MAE: 178.1307321, Time: 43.4935302734375\n",
      "ToTal Epoch: 160, LR: 0.000092, Loss: 173288.6815387, Val MAE: 209.2995795, Test MAE: 178.1307321, Time: 40.65296006202698\n",
      "ToTal Epoch: 161, LR: 0.000092, Loss: 171769.7304185, Val MAE: 686.7821018, Test MAE: 178.1307321, Time: 42.06075477600098\n",
      "ToTal Epoch: 162, LR: 0.000092, Loss: 171988.1392610, Val MAE: 390.6909133, Test MAE: 178.1307321, Time: 42.491923332214355\n",
      "ToTal Epoch: 163, LR: 0.000092, Loss: 172280.8370432, Val MAE: 643.8925641, Test MAE: 178.1307321, Time: 41.746973514556885\n",
      "ToTal Epoch: 164, LR: 0.000092, Loss: 171987.9875639, Val MAE: 205.7791881, Test MAE: 178.1307321, Time: 43.18318438529968\n",
      "ToTal Epoch: 165, LR: 0.000092, Loss: 171844.5877048, Val MAE: 692.4540427, Test MAE: 178.1307321, Time: 41.280221939086914\n",
      "ToTal Epoch: 166, LR: 0.000074, Loss: 171990.1441134, Val MAE: 276.5546603, Test MAE: 178.1307321, Time: 45.53710913658142\n",
      "ToTal Epoch: 167, LR: 0.000074, Loss: 171114.9937300, Val MAE: 317.6012324, Test MAE: 178.1307321, Time: 41.23411512374878\n",
      "ToTal Epoch: 168, LR: 0.000074, Loss: 171675.4581020, Val MAE: 598.1455553, Test MAE: 178.1307321, Time: 40.938159227371216\n",
      "ToTal Epoch: 169, LR: 0.000074, Loss: 171800.0868533, Val MAE: 469.9984485, Test MAE: 178.1307321, Time: 41.470579862594604\n",
      "ToTal Epoch: 170, LR: 0.000074, Loss: 170933.3245020, Val MAE: 250.7019853, Test MAE: 178.1307321, Time: 41.49809288978577\n",
      "ToTal Epoch: 171, LR: 0.000074, Loss: 171399.0290343, Val MAE: 1523.8697496, Test MAE: 178.1307321, Time: 43.47464609146118\n",
      "ToTal Epoch: 172, LR: 0.000059, Loss: 171288.4579110, Val MAE: 177.7549620, Test MAE: 178.1307321, Time: 42.23597311973572\n",
      "ToTal Epoch: 173, LR: 0.000059, Loss: 172184.7238141, Val MAE: 1011.8021837, Test MAE: 178.1307321, Time: 40.62304711341858\n",
      "ToTal Epoch: 174, LR: 0.000059, Loss: 170741.9864138, Val MAE: 153.8053448, Test MAE: 170.6582967, Time: 45.82593894004822\n",
      "ToTal Epoch: 175, LR: 0.000059, Loss: 170826.3766230, Val MAE: 399.3884056, Test MAE: 170.6582967, Time: 41.715351819992065\n",
      "ToTal Epoch: 176, LR: 0.000059, Loss: 172273.6544308, Val MAE: 149.3085824, Test MAE: 167.3754175, Time: 48.085185527801514\n",
      "ToTal Epoch: 177, LR: 0.000059, Loss: 172914.2419457, Val MAE: 568.2516546, Test MAE: 167.3754175, Time: 46.614187479019165\n",
      "ToTal Epoch: 178, LR: 0.000059, Loss: 171524.6766398, Val MAE: 462.7594595, Test MAE: 167.3754175, Time: 44.015512228012085\n",
      "ToTal Epoch: 179, LR: 0.000059, Loss: 173008.2516195, Val MAE: 496.4496737, Test MAE: 167.3754175, Time: 41.36248993873596\n",
      "ToTal Epoch: 180, LR: 0.000059, Loss: 172033.2553683, Val MAE: 205.5403585, Test MAE: 167.3754175, Time: 40.99346041679382\n",
      "ToTal Epoch: 181, LR: 0.000059, Loss: 172443.7612932, Val MAE: 1605.0182298, Test MAE: 167.3754175, Time: 44.42328858375549\n",
      "ToTal Epoch: 182, LR: 0.000059, Loss: 171951.9727380, Val MAE: 453.5372776, Test MAE: 167.3754175, Time: 42.651381969451904\n",
      "ToTal Epoch: 183, LR: 0.000047, Loss: 170659.4526537, Val MAE: 297.3569327, Test MAE: 167.3754175, Time: 40.426878452301025\n",
      "ToTal Epoch: 184, LR: 0.000047, Loss: 170335.6417749, Val MAE: 150.0666894, Test MAE: 167.3754175, Time: 42.69184684753418\n",
      "ToTal Epoch: 185, LR: 0.000047, Loss: 169854.5813142, Val MAE: 325.7887533, Test MAE: 167.3754175, Time: 44.676480531692505\n",
      "ToTal Epoch: 186, LR: 0.000047, Loss: 172385.2453447, Val MAE: 776.5945330, Test MAE: 167.3754175, Time: 44.541690826416016\n",
      "ToTal Epoch: 187, LR: 0.000047, Loss: 171857.6076685, Val MAE: 689.2747936, Test MAE: 167.3754175, Time: 44.73688197135925\n",
      "ToTal Epoch: 188, LR: 0.000047, Loss: 172782.0932033, Val MAE: 586.6178379, Test MAE: 167.3754175, Time: 39.83390831947327\n",
      "ToTal Epoch: 189, LR: 0.000038, Loss: 172158.5455835, Val MAE: 287.6180954, Test MAE: 167.3754175, Time: 40.99636149406433\n",
      "ToTal Epoch: 190, LR: 0.000038, Loss: 170525.5482958, Val MAE: 333.7239253, Test MAE: 167.3754175, Time: 47.867109537124634\n",
      "ToTal Epoch: 191, LR: 0.000038, Loss: 171857.6350643, Val MAE: 1385.5778736, Test MAE: 167.3754175, Time: 44.351725816726685\n",
      "ToTal Epoch: 192, LR: 0.000038, Loss: 172089.6394353, Val MAE: 771.3142187, Test MAE: 167.3754175, Time: 42.704657793045044\n",
      "ToTal Epoch: 193, LR: 0.000038, Loss: 171907.3423852, Val MAE: 412.4144370, Test MAE: 167.3754175, Time: 43.1084406375885\n",
      "ToTal Epoch: 194, LR: 0.000038, Loss: 170627.9490971, Val MAE: 358.7492742, Test MAE: 167.3754175, Time: 42.83381175994873\n",
      "ToTal Epoch: 195, LR: 0.000030, Loss: 171150.7529642, Val MAE: 318.0274296, Test MAE: 167.3754175, Time: 42.96571326255798\n",
      "ToTal Epoch: 196, LR: 0.000030, Loss: 172758.3716046, Val MAE: 321.6150232, Test MAE: 167.3754175, Time: 41.355125188827515\n",
      "ToTal Epoch: 197, LR: 0.000030, Loss: 172813.2051139, Val MAE: 749.9036805, Test MAE: 167.3754175, Time: 42.8573637008667\n",
      "ToTal Epoch: 198, LR: 0.000030, Loss: 171705.9794105, Val MAE: 583.5739158, Test MAE: 167.3754175, Time: 42.62511610984802\n",
      "ToTal Epoch: 199, LR: 0.000030, Loss: 172180.7774447, Val MAE: 348.2562495, Test MAE: 167.3754175, Time: 43.58892774581909\n",
      "ToTal Epoch: 200, LR: 0.000030, Loss: 170972.7897399, Val MAE: 282.3157429, Test MAE: 167.3754175, Time: 42.629348278045654\n",
      "ToTal Epoch: 201, LR: 0.000024, Loss: 171572.6183753, Val MAE: 273.1839077, Test MAE: 167.3754175, Time: 41.339253187179565\n",
      "ToTal Epoch: 202, LR: 0.000024, Loss: 172047.1869417, Val MAE: 839.7331289, Test MAE: 167.3754175, Time: 40.66882109642029\n",
      "ToTal Epoch: 203, LR: 0.000024, Loss: 171685.7344456, Val MAE: 421.6984142, Test MAE: 167.3754175, Time: 42.811604022979736\n",
      "ToTal Epoch: 204, LR: 0.000024, Loss: 171475.8391535, Val MAE: 172.4560446, Test MAE: 167.3754175, Time: 42.824827671051025\n",
      "ToTal Epoch: 205, LR: 0.000024, Loss: 170650.4874851, Val MAE: 301.4044649, Test MAE: 167.3754175, Time: 43.767287492752075\n",
      "ToTal Epoch: 206, LR: 0.000024, Loss: 172631.2711007, Val MAE: 276.2007757, Test MAE: 167.3754175, Time: 43.75069236755371\n",
      "ToTal Epoch: 207, LR: 0.000019, Loss: 171743.7175130, Val MAE: 313.9284057, Test MAE: 167.3754175, Time: 42.74512839317322\n",
      "ToTal Epoch: 208, LR: 0.000019, Loss: 171109.7331510, Val MAE: 706.1306141, Test MAE: 167.3754175, Time: 46.632593631744385\n",
      "ToTal Epoch: 209, LR: 0.000019, Loss: 171647.9525319, Val MAE: 704.7757986, Test MAE: 167.3754175, Time: 43.687499046325684\n",
      "ToTal Epoch: 210, LR: 0.000019, Loss: 171384.6211520, Val MAE: 593.2047311, Test MAE: 167.3754175, Time: 41.43455505371094\n",
      "ToTal Epoch: 211, LR: 0.000019, Loss: 170733.7699589, Val MAE: 155.6699349, Test MAE: 167.3754175, Time: 44.26671743392944\n",
      "ToTal Epoch: 212, LR: 0.000019, Loss: 169868.6061410, Val MAE: 818.4864519, Test MAE: 167.3754175, Time: 42.31800937652588\n",
      "ToTal Epoch: 213, LR: 0.000015, Loss: 173106.4099675, Val MAE: 392.3122231, Test MAE: 167.3754175, Time: 42.35284924507141\n",
      "ToTal Epoch: 214, LR: 0.000015, Loss: 171188.4515991, Val MAE: 557.6675717, Test MAE: 167.3754175, Time: 44.67707943916321\n",
      "ToTal Epoch: 215, LR: 0.000015, Loss: 171194.9556490, Val MAE: 518.9492708, Test MAE: 167.3754175, Time: 43.66219520568848\n",
      "ToTal Epoch: 216, LR: 0.000015, Loss: 169695.1689975, Val MAE: 270.8213785, Test MAE: 167.3754175, Time: 41.574836015701294\n",
      "ToTal Epoch: 217, LR: 0.000015, Loss: 171553.5327724, Val MAE: 183.0355054, Test MAE: 167.3754175, Time: 43.52260088920593\n",
      "ToTal Epoch: 218, LR: 0.000015, Loss: 172861.9365177, Val MAE: 408.2800614, Test MAE: 167.3754175, Time: 48.60367202758789\n",
      "ToTal Epoch: 219, LR: 0.000012, Loss: 170661.6012182, Val MAE: 543.8625806, Test MAE: 167.3754175, Time: 45.683573722839355\n",
      "ToTal Epoch: 220, LR: 0.000012, Loss: 171441.9964840, Val MAE: 697.2939939, Test MAE: 167.3754175, Time: 42.453020095825195\n",
      "ToTal Epoch: 221, LR: 0.000012, Loss: 170619.0854416, Val MAE: 436.8083864, Test MAE: 167.3754175, Time: 46.008883476257324\n",
      "ToTal Epoch: 222, LR: 0.000012, Loss: 171745.9258659, Val MAE: 235.3971646, Test MAE: 167.3754175, Time: 44.149959087371826\n",
      "ToTal Epoch: 223, LR: 0.000012, Loss: 171775.5355969, Val MAE: 201.2448741, Test MAE: 167.3754175, Time: 46.74136757850647\n",
      "ToTal Epoch: 224, LR: 0.000012, Loss: 171751.0769837, Val MAE: 337.3173896, Test MAE: 167.3754175, Time: 46.240867376327515\n",
      "ToTal Epoch: 225, LR: 0.000010, Loss: 172242.6054674, Val MAE: 644.2662386, Test MAE: 167.3754175, Time: 41.458484172821045\n",
      "ToTal Epoch: 226, LR: 0.000010, Loss: 171361.2114628, Val MAE: 355.4494354, Test MAE: 167.3754175, Time: 42.876322746276855\n",
      "ToTal Epoch: 227, LR: 0.000010, Loss: 171008.4980963, Val MAE: 389.2138104, Test MAE: 167.3754175, Time: 42.735514402389526\n",
      "ToTal Epoch: 228, LR: 0.000010, Loss: 171899.3117757, Val MAE: 529.7633096, Test MAE: 167.3754175, Time: 40.23006582260132\n",
      "ToTal Epoch: 229, LR: 0.000010, Loss: 171829.7678653, Val MAE: 619.4685354, Test MAE: 167.3754175, Time: 43.097843408584595\n",
      "ToTal Epoch: 230, LR: 0.000010, Loss: 172128.8860483, Val MAE: 233.5649581, Test MAE: 167.3754175, Time: 43.032596588134766\n",
      "ToTal Epoch: 231, LR: 0.000010, Loss: 171677.7391571, Val MAE: 409.7977665, Test MAE: 167.3754175, Time: 40.61364388465881\n",
      "ToTal Epoch: 232, LR: 0.000010, Loss: 171906.0395094, Val MAE: 182.7440460, Test MAE: 167.3754175, Time: 42.17617416381836\n",
      "ToTal Epoch: 233, LR: 0.000010, Loss: 171529.9881180, Val MAE: 199.2558992, Test MAE: 167.3754175, Time: 43.12052536010742\n",
      "ToTal Epoch: 234, LR: 0.000010, Loss: 171449.6734940, Val MAE: 470.6361039, Test MAE: 167.3754175, Time: 43.93085789680481\n",
      "ToTal Epoch: 235, LR: 0.000010, Loss: 172208.9009972, Val MAE: 412.8999425, Test MAE: 167.3754175, Time: 41.584545373916626\n",
      "ToTal Epoch: 236, LR: 0.000010, Loss: 170631.3700282, Val MAE: 423.7673441, Test MAE: 167.3754175, Time: 42.66244959831238\n",
      "ToTal Epoch: 237, LR: 0.000010, Loss: 171105.9094229, Val MAE: 530.0766716, Test MAE: 167.3754175, Time: 40.83139228820801\n",
      "ToTal Epoch: 238, LR: 0.000010, Loss: 170899.2091435, Val MAE: 379.2277128, Test MAE: 167.3754175, Time: 40.2104275226593\n",
      "ToTal Epoch: 239, LR: 0.000010, Loss: 172257.8477141, Val MAE: 269.4808741, Test MAE: 167.3754175, Time: 41.33141326904297\n",
      "ToTal Epoch: 240, LR: 0.000010, Loss: 171329.7791764, Val MAE: 200.5568502, Test MAE: 167.3754175, Time: 42.13590359687805\n",
      "ToTal Epoch: 241, LR: 0.000010, Loss: 171511.8564802, Val MAE: 229.0073910, Test MAE: 167.3754175, Time: 42.51624298095703\n",
      "ToTal Epoch: 242, LR: 0.000010, Loss: 172439.0244972, Val MAE: 463.2028430, Test MAE: 167.3754175, Time: 41.67553687095642\n",
      "ToTal Epoch: 243, LR: 0.000010, Loss: 170153.4698526, Val MAE: 502.5167728, Test MAE: 167.3754175, Time: 41.902860164642334\n",
      "ToTal Epoch: 244, LR: 0.000010, Loss: 172104.4376045, Val MAE: 334.6469280, Test MAE: 167.3754175, Time: 44.90492582321167\n",
      "ToTal Epoch: 245, LR: 0.000010, Loss: 170988.5702169, Val MAE: 510.2617494, Test MAE: 167.3754175, Time: 43.64522981643677\n",
      "ToTal Epoch: 246, LR: 0.000010, Loss: 171309.1915062, Val MAE: 331.2685501, Test MAE: 167.3754175, Time: 41.62441635131836\n",
      "ToTal Epoch: 247, LR: 0.000010, Loss: 170579.9833815, Val MAE: 386.6444426, Test MAE: 167.3754175, Time: 44.05734395980835\n",
      "ToTal Epoch: 248, LR: 0.000010, Loss: 171130.0286103, Val MAE: 1259.5682574, Test MAE: 167.3754175, Time: 42.124125242233276\n",
      "ToTal Epoch: 249, LR: 0.000010, Loss: 171464.3110985, Val MAE: 509.0230696, Test MAE: 167.3754175, Time: 41.72075939178467\n",
      "ToTal Epoch: 250, LR: 0.000010, Loss: 172109.7315626, Val MAE: 304.9917445, Test MAE: 167.3754175, Time: 40.54969310760498\n",
      "ToTal Epoch: 251, LR: 0.000010, Loss: 172615.8732360, Val MAE: 320.1987837, Test MAE: 167.3754175, Time: 43.66241645812988\n",
      "ToTal Epoch: 252, LR: 0.000010, Loss: 170995.9188196, Val MAE: 707.7397553, Test MAE: 167.3754175, Time: 43.51877737045288\n",
      "ToTal Epoch: 253, LR: 0.000010, Loss: 171171.9517532, Val MAE: 448.1660506, Test MAE: 167.3754175, Time: 42.5432984828949\n",
      "ToTal Epoch: 254, LR: 0.000010, Loss: 171414.8339894, Val MAE: 326.8111055, Test MAE: 167.3754175, Time: 43.00335669517517\n",
      "ToTal Epoch: 255, LR: 0.000010, Loss: 171203.8807732, Val MAE: 536.6672584, Test MAE: 167.3754175, Time: 41.935383558273315\n",
      "ToTal Epoch: 256, LR: 0.000010, Loss: 171694.0916854, Val MAE: 345.6477019, Test MAE: 167.3754175, Time: 40.85468888282776\n",
      "ToTal Epoch: 257, LR: 0.000010, Loss: 171430.4197619, Val MAE: 320.9455703, Test MAE: 167.3754175, Time: 43.53952646255493\n",
      "ToTal Epoch: 258, LR: 0.000010, Loss: 171453.9175226, Val MAE: 151.0681096, Test MAE: 167.3754175, Time: 42.436118841171265\n",
      "ToTal Epoch: 259, LR: 0.000010, Loss: 172509.4777982, Val MAE: 279.8391000, Test MAE: 167.3754175, Time: 42.54852223396301\n",
      "ToTal Epoch: 260, LR: 0.000010, Loss: 171372.7554579, Val MAE: 324.8304833, Test MAE: 167.3754175, Time: 42.410041093826294\n",
      "ToTal Epoch: 261, LR: 0.000010, Loss: 172517.6921511, Val MAE: 305.4240376, Test MAE: 167.3754175, Time: 44.46160674095154\n",
      "ToTal Epoch: 262, LR: 0.000010, Loss: 171875.1945588, Val MAE: 315.3383507, Test MAE: 167.3754175, Time: 44.72879600524902\n",
      "ToTal Epoch: 263, LR: 0.000010, Loss: 173259.6066785, Val MAE: 571.6013284, Test MAE: 167.3754175, Time: 46.61881709098816\n",
      "ToTal Epoch: 264, LR: 0.000010, Loss: 170068.3073604, Val MAE: 284.9930707, Test MAE: 167.3754175, Time: 44.73028087615967\n",
      "ToTal Epoch: 265, LR: 0.000010, Loss: 172668.8634524, Val MAE: 381.5098910, Test MAE: 167.3754175, Time: 42.09789538383484\n",
      "ToTal Epoch: 266, LR: 0.000010, Loss: 172469.8967300, Val MAE: 501.2997246, Test MAE: 167.3754175, Time: 42.091641902923584\n",
      "ToTal Epoch: 267, LR: 0.000010, Loss: 172398.7667988, Val MAE: 785.7686734, Test MAE: 167.3754175, Time: 45.56872773170471\n",
      "ToTal Epoch: 268, LR: 0.000010, Loss: 170052.8555212, Val MAE: 336.2220258, Test MAE: 167.3754175, Time: 43.721272468566895\n",
      "ToTal Epoch: 269, LR: 0.000010, Loss: 170949.9395822, Val MAE: 674.3555079, Test MAE: 167.3754175, Time: 42.4499454498291\n",
      "ToTal Epoch: 270, LR: 0.000010, Loss: 171373.2088353, Val MAE: 574.5468787, Test MAE: 167.3754175, Time: 40.670591592788696\n",
      "ToTal Epoch: 271, LR: 0.000010, Loss: 171296.0538325, Val MAE: 498.9063435, Test MAE: 167.3754175, Time: 41.23429822921753\n",
      "ToTal Epoch: 272, LR: 0.000010, Loss: 170955.9824953, Val MAE: 320.7953276, Test MAE: 167.3754175, Time: 45.213393449783325\n",
      "ToTal Epoch: 273, LR: 0.000010, Loss: 173227.7953112, Val MAE: 239.0634266, Test MAE: 167.3754175, Time: 40.43201661109924\n",
      "ToTal Epoch: 274, LR: 0.000010, Loss: 173437.1638000, Val MAE: 150.9100455, Test MAE: 167.3754175, Time: 41.50871515274048\n",
      "ToTal Epoch: 275, LR: 0.000010, Loss: 172388.2315542, Val MAE: 153.1363622, Test MAE: 167.3754175, Time: 41.327850580215454\n",
      "ToTal Epoch: 276, LR: 0.000010, Loss: 172507.1495151, Val MAE: 340.9199192, Test MAE: 167.3754175, Time: 40.398112773895264\n",
      "ToTal Epoch: 277, LR: 0.000010, Loss: 171548.7401053, Val MAE: 312.5209169, Test MAE: 167.3754175, Time: 42.62543487548828\n",
      "ToTal Epoch: 278, LR: 0.000010, Loss: 171514.9671022, Val MAE: 435.9403171, Test MAE: 167.3754175, Time: 41.36241340637207\n",
      "ToTal Epoch: 279, LR: 0.000010, Loss: 171206.6398868, Val MAE: 227.5714902, Test MAE: 167.3754175, Time: 41.48086953163147\n",
      "ToTal Epoch: 280, LR: 0.000010, Loss: 171620.1753905, Val MAE: 332.9221797, Test MAE: 167.3754175, Time: 42.0881552696228\n",
      "ToTal Epoch: 281, LR: 0.000010, Loss: 172005.7306382, Val MAE: 505.9158934, Test MAE: 167.3754175, Time: 44.39573812484741\n",
      "ToTal Epoch: 282, LR: 0.000010, Loss: 171743.0110758, Val MAE: 405.5389400, Test MAE: 167.3754175, Time: 43.11048913002014\n",
      "ToTal Epoch: 283, LR: 0.000010, Loss: 171233.9090969, Val MAE: 635.0866818, Test MAE: 167.3754175, Time: 43.2396559715271\n",
      "ToTal Epoch: 284, LR: 0.000010, Loss: 171701.8567740, Val MAE: 414.4297780, Test MAE: 167.3754175, Time: 42.52894163131714\n",
      "ToTal Epoch: 285, LR: 0.000010, Loss: 171971.4148402, Val MAE: 494.3898079, Test MAE: 167.3754175, Time: 45.93928861618042\n",
      "ToTal Epoch: 286, LR: 0.000010, Loss: 171344.1871471, Val MAE: 203.0769909, Test MAE: 167.3754175, Time: 44.152286767959595\n",
      "ToTal Epoch: 287, LR: 0.000010, Loss: 170735.5613147, Val MAE: 499.3498216, Test MAE: 167.3754175, Time: 42.69977903366089\n",
      "ToTal Epoch: 288, LR: 0.000010, Loss: 169703.7139230, Val MAE: 320.1592528, Test MAE: 167.3754175, Time: 42.868897438049316\n",
      "ToTal Epoch: 289, LR: 0.000010, Loss: 171608.9255470, Val MAE: 348.5400234, Test MAE: 167.3754175, Time: 40.972270011901855\n",
      "ToTal Epoch: 290, LR: 0.000010, Loss: 172487.7861104, Val MAE: 425.1074560, Test MAE: 167.3754175, Time: 42.78721070289612\n",
      "ToTal Epoch: 291, LR: 0.000010, Loss: 171855.1375209, Val MAE: 461.5017891, Test MAE: 167.3754175, Time: 40.96487879753113\n",
      "ToTal Epoch: 292, LR: 0.000010, Loss: 171627.3922980, Val MAE: 365.9870558, Test MAE: 167.3754175, Time: 45.66132640838623\n",
      "ToTal Epoch: 293, LR: 0.000010, Loss: 173275.2978503, Val MAE: 362.4699915, Test MAE: 167.3754175, Time: 44.021392583847046\n",
      "ToTal Epoch: 294, LR: 0.000010, Loss: 171316.6858370, Val MAE: 493.3360329, Test MAE: 167.3754175, Time: 41.955137729644775\n",
      "ToTal Epoch: 295, LR: 0.000010, Loss: 171600.5302990, Val MAE: 337.6673051, Test MAE: 167.3754175, Time: 41.750131130218506\n",
      "ToTal Epoch: 296, LR: 0.000010, Loss: 171754.7565232, Val MAE: 268.2495113, Test MAE: 167.3754175, Time: 42.78522253036499\n",
      "ToTal Epoch: 297, LR: 0.000010, Loss: 171441.0215115, Val MAE: 297.3076949, Test MAE: 167.3754175, Time: 44.98853015899658\n",
      "ToTal Epoch: 298, LR: 0.000010, Loss: 171618.5504156, Val MAE: 266.7308450, Test MAE: 167.3754175, Time: 44.888434171676636\n",
      "ToTal Epoch: 299, LR: 0.000010, Loss: 172511.3902797, Val MAE: 216.8346769, Test MAE: 167.3754175, Time: 46.252288579940796\n",
      "ToTal Epoch: 300, LR: 0.000010, Loss: 172503.9681173, Val MAE: 219.6515542, Test MAE: 167.3754175, Time: 41.762603521347046\n",
      "Done. Total Time: 13030.729499578476\n"
     ]
    }
   ],
   "source": [
    "#input parameters\n",
    "total_num_epoch = 0 #the total number of epoch that have run\n",
    "running_num_epoch = 300 #the number of epoch that run in this time\n",
    "\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "    \n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    epoch_time_end = time.time() # to measure time\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "\n",
    "total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total Time: {total_time_finish - total_time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720345fe-2bea-4e1f-9058-64ef2854603e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3cc22-001c-44b7-8794-7fecfd107e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOPCH = total_num_epoch\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EOPCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model/GCN_skip_epoch300_20211211')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bc24c-140f-46ec-a677-889b5a87c922",
   "metadata": {},
   "source": [
    "To load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "183948a0-96a2-495d-bcbc-8c6d5381e4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(11, 80)\n",
       "  (conv2): GCNConv(80, 80)\n",
       "  (conv3): GCNConv(80, 80)\n",
       "  (conv4): GCNConv(80, 80)\n",
       "  (conv5): GCNConv(80, 80)\n",
       "  (lin1): Linear(in_features=80, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = 'model/GCNConv01-2_epoch300_20211207'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0825a82-a146-4898-8ecd-face6a67580c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GCN_skip (ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e90751f-853a-4423-ac04-365569c8e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Ensemble(nn.Module):\n",
    "\n",
    "    def __init__(self, modelA, modelB, modelC, input):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.modelA = modelA()\n",
    "        self.modelB = modelB()\n",
    "        self.modelC = modelC()\n",
    "\n",
    "        self.fc1 = nn.Linear(input, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.modelA.forward(x)\n",
    "        out2 = self.modelB.forward(x)\n",
    "        out3 = self.modelC.forward(x)\n",
    "        \n",
    "        # average\n",
    "        out = (out1 + out2 + out3)/3\n",
    "        #print(out.size())\n",
    "        #x = self.fc1(out)\n",
    "        return out\n",
    "        #return torch.softmax(x, dim=1) #for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555a1e42-b86a-4069-8d2f-3a031223e670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ensemble(\n",
       "  (modelA): GCN_skip(\n",
       "    (conv1): GCNConv(11, 80)\n",
       "    (conv2): GCNConv(80, 80)\n",
       "    (conv3): GCNConv(80, 80)\n",
       "    (conv4): GCNConv(80, 80)\n",
       "    (conv5): GCNConv(80, 80)\n",
       "    (conv6): GCNConv(80, 80)\n",
       "    (conv7): GCNConv(80, 80)\n",
       "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
       "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       "  )\n",
       "  (modelB): GCN_skip(\n",
       "    (conv1): GCNConv(11, 80)\n",
       "    (conv2): GCNConv(80, 80)\n",
       "    (conv3): GCNConv(80, 80)\n",
       "    (conv4): GCNConv(80, 80)\n",
       "    (conv5): GCNConv(80, 80)\n",
       "    (conv6): GCNConv(80, 80)\n",
       "    (conv7): GCNConv(80, 80)\n",
       "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
       "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       "  )\n",
       "  (modelC): GCN_skip(\n",
       "    (conv1): GCNConv(11, 80)\n",
       "    (conv2): GCNConv(80, 80)\n",
       "    (conv3): GCNConv(80, 80)\n",
       "    (conv4): GCNConv(80, 80)\n",
       "    (conv5): GCNConv(80, 80)\n",
       "    (conv6): GCNConv(80, 80)\n",
       "    (conv7): GCNConv(80, 80)\n",
       "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
       "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ensemble(GCN_skip, GCN_skip, GCN_skip, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35eab76e-c1e8-4e47-bf0d-64f7d89f1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Ensemble(GCN_skip, GCN_skip, GCN_skip, 3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b82a739-f7b2-435f-9ace-8cba77ed487b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc8df42-2e16-404d-bb50-0d5e290744e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble(\n",
      "  (modelA): GCN_skip(\n",
      "    (conv1): GCNConv(11, 80)\n",
      "    (conv2): GCNConv(80, 80)\n",
      "    (conv3): GCNConv(80, 80)\n",
      "    (conv4): GCNConv(80, 80)\n",
      "    (conv5): GCNConv(80, 80)\n",
      "    (conv6): GCNConv(80, 80)\n",
      "    (conv7): GCNConv(80, 80)\n",
      "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
      "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
      "  )\n",
      "  (modelB): GCN_skip(\n",
      "    (conv1): GCNConv(11, 80)\n",
      "    (conv2): GCNConv(80, 80)\n",
      "    (conv3): GCNConv(80, 80)\n",
      "    (conv4): GCNConv(80, 80)\n",
      "    (conv5): GCNConv(80, 80)\n",
      "    (conv6): GCNConv(80, 80)\n",
      "    (conv7): GCNConv(80, 80)\n",
      "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
      "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
      "  )\n",
      "  (modelC): GCN_skip(\n",
      "    (conv1): GCNConv(11, 80)\n",
      "    (conv2): GCNConv(80, 80)\n",
      "    (conv3): GCNConv(80, 80)\n",
      "    (conv4): GCNConv(80, 80)\n",
      "    (conv5): GCNConv(80, 80)\n",
      "    (conv6): GCNConv(80, 80)\n",
      "    (conv7): GCNConv(80, 80)\n",
      "    (lin1): Linear(in_features=91, out_features=80, bias=True)\n",
      "    (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a49f576f-6377-44e0-b72c-275de7132f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+\n",
      "|         Modules         | Parameters |\n",
      "+-------------------------+------------+\n",
      "|    modelA.conv1.bias    |     80     |\n",
      "| modelA.conv1.lin.weight |    880     |\n",
      "|    modelA.conv2.bias    |     80     |\n",
      "| modelA.conv2.lin.weight |    6400    |\n",
      "|    modelA.conv3.bias    |     80     |\n",
      "| modelA.conv3.lin.weight |    6400    |\n",
      "|    modelA.conv4.bias    |     80     |\n",
      "| modelA.conv4.lin.weight |    6400    |\n",
      "|    modelA.conv5.bias    |     80     |\n",
      "| modelA.conv5.lin.weight |    6400    |\n",
      "|    modelA.conv6.bias    |     80     |\n",
      "| modelA.conv6.lin.weight |    6400    |\n",
      "|    modelA.conv7.bias    |     80     |\n",
      "| modelA.conv7.lin.weight |    6400    |\n",
      "|    modelA.lin1.weight   |    7280    |\n",
      "|     modelA.lin1.bias    |     80     |\n",
      "|    modelA.lin2.weight   |     80     |\n",
      "|     modelA.lin2.bias    |     1      |\n",
      "|    modelB.conv1.bias    |     80     |\n",
      "| modelB.conv1.lin.weight |    880     |\n",
      "|    modelB.conv2.bias    |     80     |\n",
      "| modelB.conv2.lin.weight |    6400    |\n",
      "|    modelB.conv3.bias    |     80     |\n",
      "| modelB.conv3.lin.weight |    6400    |\n",
      "|    modelB.conv4.bias    |     80     |\n",
      "| modelB.conv4.lin.weight |    6400    |\n",
      "|    modelB.conv5.bias    |     80     |\n",
      "| modelB.conv5.lin.weight |    6400    |\n",
      "|    modelB.conv6.bias    |     80     |\n",
      "| modelB.conv6.lin.weight |    6400    |\n",
      "|    modelB.conv7.bias    |     80     |\n",
      "| modelB.conv7.lin.weight |    6400    |\n",
      "|    modelB.lin1.weight   |    7280    |\n",
      "|     modelB.lin1.bias    |     80     |\n",
      "|    modelB.lin2.weight   |     80     |\n",
      "|     modelB.lin2.bias    |     1      |\n",
      "|    modelC.conv1.bias    |     80     |\n",
      "| modelC.conv1.lin.weight |    880     |\n",
      "|    modelC.conv2.bias    |     80     |\n",
      "| modelC.conv2.lin.weight |    6400    |\n",
      "|    modelC.conv3.bias    |     80     |\n",
      "| modelC.conv3.lin.weight |    6400    |\n",
      "|    modelC.conv4.bias    |     80     |\n",
      "| modelC.conv4.lin.weight |    6400    |\n",
      "|    modelC.conv5.bias    |     80     |\n",
      "| modelC.conv5.lin.weight |    6400    |\n",
      "|    modelC.conv6.bias    |     80     |\n",
      "| modelC.conv6.lin.weight |    6400    |\n",
      "|    modelC.conv7.bias    |     80     |\n",
      "| modelC.conv7.lin.weight |    6400    |\n",
      "|    modelC.lin1.weight   |    7280    |\n",
      "|     modelC.lin1.bias    |     80     |\n",
      "|    modelC.lin2.weight   |     80     |\n",
      "|     modelC.lin2.bias    |     1      |\n",
      "|        fc1.weight       |     3      |\n",
      "|         fc1.bias        |     1      |\n",
      "+-------------------------+------------+\n",
      "Total Trainable Params: 141847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141847"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cff3457-ac55-4b4d-a3df-a424e1ab51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device))\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y.to(device)) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dffce2b1-2ad0-4f8f-8687-afb92dcf8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device) #it trigger error!\n",
    "        out = model(batch)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device))\n",
    "        tmp = (out - y)**2\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9aea49-d0e3-4c08-acb9-e4e9cf1553b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125924755.83153711"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check any error\n",
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e556e6e0-c1ca-491b-a849-882bcebe5573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTal Epoch: 001, LR: 0.010000, Loss: 976162.2059165, Val MAE: 32405.9037276, Test MAE: 33848.4269950, Time: 180.88196778297424\n",
      "ToTal Epoch: 002, LR: 0.010000, Loss: 112609.3658912, Val MAE: 52624.5011561, Test MAE: 33848.4269950, Time: 177.63085198402405\n",
      "ToTal Epoch: 003, LR: 0.010000, Loss: 100776.7408524, Val MAE: 14470.1997499, Test MAE: 14347.1983418, Time: 141.5558626651764\n",
      "ToTal Epoch: 004, LR: 0.010000, Loss: 13614942.6128033, Val MAE: 21709.9153840, Test MAE: 14347.1983418, Time: 131.98660326004028\n",
      "ToTal Epoch: 005, LR: 0.010000, Loss: 256287.6157610, Val MAE: 3309207.1885653, Test MAE: 14347.1983418, Time: 163.6609981060028\n",
      "ToTal Epoch: 006, LR: 0.010000, Loss: 277581.5962679, Val MAE: 9635.1265000, Test MAE: 10049.6434579, Time: 189.06312084197998\n",
      "ToTal Epoch: 007, LR: 0.010000, Loss: 95557.4747623, Val MAE: 19379.0227741, Test MAE: 10049.6434579, Time: 135.97876453399658\n",
      "ToTal Epoch: 008, LR: 0.010000, Loss: 97388.9832364, Val MAE: 20366.9286706, Test MAE: 10049.6434579, Time: 144.24654150009155\n",
      "ToTal Epoch: 009, LR: 0.010000, Loss: 92313.4328465, Val MAE: 69076.6372201, Test MAE: 10049.6434579, Time: 152.0485098361969\n",
      "ToTal Epoch: 010, LR: 0.010000, Loss: 81523.2755745, Val MAE: 9486.0032485, Test MAE: 9543.3883499, Time: 158.0798304080963\n",
      "ToTal Epoch: 011, LR: 0.010000, Loss: 75136.2061518, Val MAE: 1852.1973071, Test MAE: 1856.7338059, Time: 132.7765519618988\n",
      "ToTal Epoch: 012, LR: 0.010000, Loss: 1035358909.6744937, Val MAE: 35813.3046439, Test MAE: 1856.7338059, Time: 149.8587543964386\n",
      "ToTal Epoch: 013, LR: 0.010000, Loss: 2505794449.8520160, Val MAE: 2361421.8741879, Test MAE: 1856.7338059, Time: 135.4136447906494\n",
      "ToTal Epoch: 014, LR: 0.010000, Loss: 13935250373.6711311, Val MAE: 129037.1592334, Test MAE: 1856.7338059, Time: 139.88406991958618\n",
      "ToTal Epoch: 015, LR: 0.010000, Loss: 533070.4204366, Val MAE: 93314.2237637, Test MAE: 1856.7338059, Time: 132.3770821094513\n",
      "ToTal Epoch: 016, LR: 0.010000, Loss: 263119.2759913, Val MAE: 37619.0643846, Test MAE: 1856.7338059, Time: 151.29046630859375\n",
      "ToTal Epoch: 017, LR: 0.010000, Loss: 275740.9548536, Val MAE: 40018.7818973, Test MAE: 1856.7338059, Time: 139.24488854408264\n",
      "ToTal Epoch: 018, LR: 0.005000, Loss: 164664.5669493, Val MAE: 35578.8689234, Test MAE: 1856.7338059, Time: 149.5031774044037\n",
      "ToTal Epoch: 019, LR: 0.005000, Loss: 130423.3152737, Val MAE: 19020.7486242, Test MAE: 1856.7338059, Time: 162.82630491256714\n",
      "ToTal Epoch: 020, LR: 0.005000, Loss: 105812.2406344, Val MAE: 12967.9217448, Test MAE: 1856.7338059, Time: 158.69496393203735\n",
      "ToTal Epoch: 021, LR: 0.005000, Loss: 1114110.6764445, Val MAE: 330576.1500611, Test MAE: 1856.7338059, Time: 158.3893039226532\n",
      "ToTal Epoch: 022, LR: 0.005000, Loss: 187963.8751887, Val MAE: 23571.1488645, Test MAE: 1856.7338059, Time: 162.00051736831665\n",
      "ToTal Epoch: 023, LR: 0.005000, Loss: 3443616.8405687, Val MAE: 12509.8530996, Test MAE: 1856.7338059, Time: 152.64023637771606\n",
      "ToTal Epoch: 024, LR: 0.002500, Loss: 152627.7086872, Val MAE: 7520.3156882, Test MAE: 1856.7338059, Time: 152.22770023345947\n",
      "ToTal Epoch: 025, LR: 0.002500, Loss: 121586.2278161, Val MAE: 12392.3972605, Test MAE: 1856.7338059, Time: 159.18624663352966\n",
      "ToTal Epoch: 026, LR: 0.002500, Loss: 102656.3123704, Val MAE: 36955.7178161, Test MAE: 1856.7338059, Time: 178.09395790100098\n",
      "ToTal Epoch: 027, LR: 0.002500, Loss: 99954.2971170, Val MAE: 17950.8332903, Test MAE: 1856.7338059, Time: 160.99989819526672\n",
      "ToTal Epoch: 028, LR: 0.002500, Loss: 249236.7307941, Val MAE: 6641.4919695, Test MAE: 1856.7338059, Time: 148.17621564865112\n",
      "ToTal Epoch: 029, LR: 0.002500, Loss: 2918112.1709705, Val MAE: 5874.3830729, Test MAE: 1856.7338059, Time: 163.78760433197021\n",
      "ToTal Epoch: 030, LR: 0.001250, Loss: 86835.4877526, Val MAE: 7578.1158774, Test MAE: 1856.7338059, Time: 177.61700558662415\n",
      "ToTal Epoch: 031, LR: 0.001250, Loss: 76701.6737400, Val MAE: 5092.0088330, Test MAE: 1856.7338059, Time: 137.09637141227722\n",
      "ToTal Epoch: 032, LR: 0.001250, Loss: 72268.8254073, Val MAE: 5868.3579556, Test MAE: 1856.7338059, Time: 168.5052785873413\n",
      "ToTal Epoch: 033, LR: 0.001250, Loss: 72786.9210553, Val MAE: 3631.5297691, Test MAE: 1856.7338059, Time: 169.22752118110657\n",
      "ToTal Epoch: 034, LR: 0.001250, Loss: 69676.7988409, Val MAE: 9379.2726726, Test MAE: 1856.7338059, Time: 165.56925916671753\n",
      "ToTal Epoch: 035, LR: 0.001250, Loss: 68582.5242733, Val MAE: 13499.0810666, Test MAE: 1856.7338059, Time: 178.71212124824524\n",
      "ToTal Epoch: 036, LR: 0.000625, Loss: 65021.8534616, Val MAE: 3646.1339184, Test MAE: 1856.7338059, Time: 165.62744879722595\n",
      "ToTal Epoch: 037, LR: 0.000625, Loss: 65744.3055893, Val MAE: 2000.7862126, Test MAE: 1856.7338059, Time: 152.6909589767456\n",
      "ToTal Epoch: 038, LR: 0.000625, Loss: 65134.7741855, Val MAE: 11387.3144395, Test MAE: 1856.7338059, Time: 154.6941385269165\n",
      "ToTal Epoch: 039, LR: 0.000625, Loss: 64496.9954307, Val MAE: 1649.2661512, Test MAE: 1731.2949038, Time: 172.18969440460205\n",
      "ToTal Epoch: 040, LR: 0.000625, Loss: 64638.8870306, Val MAE: 1914.1261688, Test MAE: 1731.2949038, Time: 140.27098178863525\n",
      "ToTal Epoch: 041, LR: 0.000625, Loss: 64132.9130279, Val MAE: 1381.7990489, Test MAE: 1446.7395593, Time: 151.29451632499695\n",
      "ToTal Epoch: 042, LR: 0.000625, Loss: 64022.6286587, Val MAE: 1797.6213189, Test MAE: 1446.7395593, Time: 163.70396780967712\n",
      "ToTal Epoch: 043, LR: 0.000625, Loss: 63554.2230193, Val MAE: 6108.1766301, Test MAE: 1446.7395593, Time: 166.87734484672546\n",
      "ToTal Epoch: 044, LR: 0.000625, Loss: 63399.3746554, Val MAE: 1260.5414238, Test MAE: 1327.0690634, Time: 179.49824118614197\n",
      "ToTal Epoch: 045, LR: 0.000625, Loss: 63747.2850326, Val MAE: 2899.4025963, Test MAE: 1327.0690634, Time: 164.6192545890808\n",
      "ToTal Epoch: 046, LR: 0.000625, Loss: 63776.0069794, Val MAE: 2502.1673423, Test MAE: 1327.0690634, Time: 170.49255347251892\n",
      "ToTal Epoch: 047, LR: 0.000625, Loss: 63539.4130888, Val MAE: 5853.8543739, Test MAE: 1327.0690634, Time: 182.02560019493103\n",
      "ToTal Epoch: 048, LR: 0.000625, Loss: 63639.1442937, Val MAE: 1528.1362388, Test MAE: 1327.0690634, Time: 170.56486177444458\n",
      "ToTal Epoch: 049, LR: 0.000625, Loss: 63246.0310180, Val MAE: 1807.3841234, Test MAE: 1327.0690634, Time: 159.69196939468384\n",
      "ToTal Epoch: 050, LR: 0.000625, Loss: 63524.9821669, Val MAE: 895.5340903, Test MAE: 973.0511124, Time: 177.50866842269897\n",
      "ToTal Epoch: 051, LR: 0.000625, Loss: 62980.1811201, Val MAE: 1614.4722787, Test MAE: 973.0511124, Time: 146.12683820724487\n",
      "ToTal Epoch: 052, LR: 0.000625, Loss: 63729.9581002, Val MAE: 1625.1999746, Test MAE: 973.0511124, Time: 156.70672988891602\n",
      "ToTal Epoch: 053, LR: 0.000625, Loss: 63586.7319382, Val MAE: 4157.4407598, Test MAE: 973.0511124, Time: 170.7291088104248\n",
      "ToTal Epoch: 054, LR: 0.000625, Loss: 62547.2182218, Val MAE: 1762.9248028, Test MAE: 973.0511124, Time: 172.7472186088562\n",
      "ToTal Epoch: 055, LR: 0.000625, Loss: 62928.2133049, Val MAE: 731.2824679, Test MAE: 764.1811928, Time: 168.85995721817017\n",
      "ToTal Epoch: 056, LR: 0.000625, Loss: 62614.0811428, Val MAE: 12087.9964912, Test MAE: 764.1811928, Time: 147.02263379096985\n",
      "ToTal Epoch: 057, LR: 0.000625, Loss: 62437.0791699, Val MAE: 2854.5713703, Test MAE: 764.1811928, Time: 165.2869942188263\n",
      "ToTal Epoch: 058, LR: 0.000625, Loss: 62910.2242345, Val MAE: 1991.0591772, Test MAE: 764.1811928, Time: 135.00796508789062\n",
      "ToTal Epoch: 059, LR: 0.000625, Loss: 62155.2852697, Val MAE: 937.9621168, Test MAE: 764.1811928, Time: 154.79997777938843\n",
      "ToTal Epoch: 060, LR: 0.000625, Loss: 62491.1858140, Val MAE: 6097.4282496, Test MAE: 764.1811928, Time: 151.06833600997925\n",
      "ToTal Epoch: 061, LR: 0.000625, Loss: 62714.5115798, Val MAE: 1324.2663126, Test MAE: 764.1811928, Time: 132.12053322792053\n",
      "ToTal Epoch: 062, LR: 0.000313, Loss: 60778.6237287, Val MAE: 4261.2214091, Test MAE: 764.1811928, Time: 151.20028519630432\n",
      "ToTal Epoch: 063, LR: 0.000313, Loss: 61061.5196245, Val MAE: 3010.7946697, Test MAE: 764.1811928, Time: 147.22846579551697\n",
      "ToTal Epoch: 064, LR: 0.000313, Loss: 61180.8964912, Val MAE: 9449.6697730, Test MAE: 764.1811928, Time: 137.1036148071289\n",
      "ToTal Epoch: 065, LR: 0.000313, Loss: 61072.6116168, Val MAE: 1117.8016410, Test MAE: 764.1811928, Time: 152.4754776954651\n",
      "ToTal Epoch: 066, LR: 0.000313, Loss: 60894.7669153, Val MAE: 976.0581060, Test MAE: 764.1811928, Time: 141.07520008087158\n",
      "ToTal Epoch: 067, LR: 0.000313, Loss: 61151.1486642, Val MAE: 425.9420104, Test MAE: 460.4327764, Time: 153.99631214141846\n",
      "ToTal Epoch: 068, LR: 0.000313, Loss: 61113.8217695, Val MAE: 1112.9629306, Test MAE: 460.4327764, Time: 157.11456632614136\n",
      "ToTal Epoch: 069, LR: 0.000313, Loss: 61077.9565035, Val MAE: 1867.9239004, Test MAE: 460.4327764, Time: 153.8907344341278\n",
      "ToTal Epoch: 070, LR: 0.000313, Loss: 60594.1247504, Val MAE: 659.0073849, Test MAE: 460.4327764, Time: 149.15434384346008\n",
      "ToTal Epoch: 071, LR: 0.000313, Loss: 61322.4635704, Val MAE: 566.6169670, Test MAE: 460.4327764, Time: 149.48689794540405\n",
      "ToTal Epoch: 072, LR: 0.000313, Loss: 61019.2270822, Val MAE: 923.2086550, Test MAE: 460.4327764, Time: 137.78216218948364\n",
      "ToTal Epoch: 073, LR: 0.000313, Loss: 61214.7256852, Val MAE: 598.7210151, Test MAE: 460.4327764, Time: 150.78870558738708\n",
      "ToTal Epoch: 074, LR: 0.000156, Loss: 60549.6901406, Val MAE: 3030.7918040, Test MAE: 460.4327764, Time: 149.95538520812988\n",
      "ToTal Epoch: 075, LR: 0.000156, Loss: 60574.8081838, Val MAE: 1610.3512106, Test MAE: 460.4327764, Time: 145.45499730110168\n",
      "ToTal Epoch: 076, LR: 0.000156, Loss: 60386.3538528, Val MAE: 1223.8688504, Test MAE: 460.4327764, Time: 150.17957186698914\n",
      "ToTal Epoch: 077, LR: 0.000156, Loss: 60086.9666448, Val MAE: 2239.8271319, Test MAE: 460.4327764, Time: 171.3170485496521\n",
      "ToTal Epoch: 078, LR: 0.000156, Loss: 60590.9174070, Val MAE: 1479.5160302, Test MAE: 460.4327764, Time: 154.03747177124023\n",
      "ToTal Epoch: 079, LR: 0.000156, Loss: 60457.0645166, Val MAE: 5164.7805089, Test MAE: 460.4327764, Time: 142.0699498653412\n",
      "ToTal Epoch: 080, LR: 0.000078, Loss: 59884.8470573, Val MAE: 457.3217758, Test MAE: 460.4327764, Time: 133.30867791175842\n",
      "ToTal Epoch: 081, LR: 0.000078, Loss: 59999.3163343, Val MAE: 329.3464669, Test MAE: 356.2771374, Time: 168.83164715766907\n",
      "ToTal Epoch: 082, LR: 0.000078, Loss: 60205.4822087, Val MAE: 320.5081732, Test MAE: 348.7742797, Time: 156.9793381690979\n",
      "ToTal Epoch: 083, LR: 0.000078, Loss: 59699.6699727, Val MAE: 423.5103025, Test MAE: 348.7742797, Time: 146.1083676815033\n",
      "ToTal Epoch: 084, LR: 0.000078, Loss: 59745.1668836, Val MAE: 393.9511450, Test MAE: 348.7742797, Time: 141.52909350395203\n",
      "ToTal Epoch: 085, LR: 0.000078, Loss: 59766.3509417, Val MAE: 1010.0831676, Test MAE: 348.7742797, Time: 143.39892745018005\n",
      "ToTal Epoch: 086, LR: 0.000078, Loss: 59903.8302805, Val MAE: 484.9547768, Test MAE: 348.7742797, Time: 123.89972710609436\n",
      "ToTal Epoch: 087, LR: 0.000078, Loss: 59828.5704975, Val MAE: 349.0876708, Test MAE: 348.7742797, Time: 132.19141149520874\n",
      "ToTal Epoch: 088, LR: 0.000078, Loss: 59963.2002359, Val MAE: 389.3547509, Test MAE: 348.7742797, Time: 140.0511758327484\n",
      "ToTal Epoch: 089, LR: 0.000039, Loss: 59909.9891254, Val MAE: 440.5650511, Test MAE: 348.7742797, Time: 145.131742477417\n",
      "ToTal Epoch: 090, LR: 0.000039, Loss: 59709.1732325, Val MAE: 500.0114716, Test MAE: 348.7742797, Time: 145.33993935585022\n",
      "ToTal Epoch: 091, LR: 0.000039, Loss: 59225.8101365, Val MAE: 740.3915622, Test MAE: 348.7742797, Time: 153.950599193573\n",
      "ToTal Epoch: 092, LR: 0.000039, Loss: 60309.2363941, Val MAE: 624.9248095, Test MAE: 348.7742797, Time: 137.62846279144287\n",
      "ToTal Epoch: 093, LR: 0.000039, Loss: 59719.2861827, Val MAE: 309.0999801, Test MAE: 338.3537270, Time: 162.10599756240845\n",
      "ToTal Epoch: 094, LR: 0.000039, Loss: 60031.4086890, Val MAE: 730.6443477, Test MAE: 338.3537270, Time: 166.24019837379456\n",
      "ToTal Epoch: 095, LR: 0.000039, Loss: 59986.1274507, Val MAE: 676.7977161, Test MAE: 338.3537270, Time: 148.48816585540771\n",
      "ToTal Epoch: 096, LR: 0.000039, Loss: 59421.0940895, Val MAE: 348.6346780, Test MAE: 338.3537270, Time: 141.58697700500488\n",
      "ToTal Epoch: 097, LR: 0.000039, Loss: 59801.8938363, Val MAE: 317.6413143, Test MAE: 338.3537270, Time: 147.56689715385437\n",
      "ToTal Epoch: 098, LR: 0.000039, Loss: 59603.5058042, Val MAE: 306.6899781, Test MAE: 337.3325092, Time: 161.93241834640503\n",
      "ToTal Epoch: 099, LR: 0.000039, Loss: 59451.4300930, Val MAE: 489.1492492, Test MAE: 337.3325092, Time: 158.85637593269348\n",
      "ToTal Epoch: 100, LR: 0.000039, Loss: 59493.1668317, Val MAE: 477.2774716, Test MAE: 337.3325092, Time: 155.19362497329712\n",
      "ToTal Epoch: 101, LR: 0.000039, Loss: 59535.9301916, Val MAE: 578.9681061, Test MAE: 337.3325092, Time: 142.5156238079071\n",
      "ToTal Epoch: 102, LR: 0.000039, Loss: 59473.7804161, Val MAE: 710.1385689, Test MAE: 337.3325092, Time: 161.50576734542847\n",
      "ToTal Epoch: 103, LR: 0.000039, Loss: 59719.5066026, Val MAE: 545.9882677, Test MAE: 337.3325092, Time: 160.16044449806213\n",
      "ToTal Epoch: 104, LR: 0.000039, Loss: 59299.3764260, Val MAE: 349.4504463, Test MAE: 337.3325092, Time: 151.79752707481384\n",
      "ToTal Epoch: 105, LR: 0.000020, Loss: 59812.2407771, Val MAE: 345.3232434, Test MAE: 337.3325092, Time: 146.59539937973022\n",
      "ToTal Epoch: 106, LR: 0.000020, Loss: 60065.0051999, Val MAE: 306.1321443, Test MAE: 342.5866881, Time: 148.0851068496704\n",
      "ToTal Epoch: 107, LR: 0.000020, Loss: 59462.7426020, Val MAE: 317.8978123, Test MAE: 342.5866881, Time: 135.5406882762909\n",
      "ToTal Epoch: 108, LR: 0.000020, Loss: 59581.2889272, Val MAE: 373.0337205, Test MAE: 342.5866881, Time: 152.98293662071228\n",
      "ToTal Epoch: 109, LR: 0.000020, Loss: 59374.9812888, Val MAE: 302.2098021, Test MAE: 331.0880252, Time: 177.2827923297882\n",
      "ToTal Epoch: 110, LR: 0.000020, Loss: 59605.5839524, Val MAE: 408.5720388, Test MAE: 331.0880252, Time: 158.74235701560974\n",
      "ToTal Epoch: 111, LR: 0.000020, Loss: 59038.8071806, Val MAE: 382.6920824, Test MAE: 331.0880252, Time: 161.39627122879028\n",
      "ToTal Epoch: 112, LR: 0.000020, Loss: 59069.5435628, Val MAE: 606.7631732, Test MAE: 331.0880252, Time: 150.43632197380066\n",
      "ToTal Epoch: 113, LR: 0.000020, Loss: 59341.8260629, Val MAE: 514.5927163, Test MAE: 331.0880252, Time: 150.28717947006226\n",
      "ToTal Epoch: 114, LR: 0.000020, Loss: 59134.7003768, Val MAE: 580.1788625, Test MAE: 331.0880252, Time: 152.83278679847717\n",
      "ToTal Epoch: 115, LR: 0.000020, Loss: 59889.8897680, Val MAE: 382.5502731, Test MAE: 331.0880252, Time: 148.49015879631042\n",
      "ToTal Epoch: 116, LR: 0.000010, Loss: 59104.2857946, Val MAE: 313.4316122, Test MAE: 331.0880252, Time: 156.9789891242981\n",
      "ToTal Epoch: 117, LR: 0.000010, Loss: 59466.9679800, Val MAE: 519.7772746, Test MAE: 331.0880252, Time: 158.60917925834656\n",
      "ToTal Epoch: 118, LR: 0.000010, Loss: 59519.5818880, Val MAE: 305.9348305, Test MAE: 331.0880252, Time: 173.9666130542755\n",
      "ToTal Epoch: 119, LR: 0.000010, Loss: 59055.2372623, Val MAE: 326.6820344, Test MAE: 331.0880252, Time: 155.43662476539612\n",
      "ToTal Epoch: 120, LR: 0.000010, Loss: 59795.7946323, Val MAE: 579.1584500, Test MAE: 331.0880252, Time: 154.9061827659607\n",
      "ToTal Epoch: 121, LR: 0.000010, Loss: 59486.4477559, Val MAE: 367.4003691, Test MAE: 331.0880252, Time: 151.22713017463684\n",
      "ToTal Epoch: 122, LR: 0.000010, Loss: 59604.5497217, Val MAE: 369.4763162, Test MAE: 331.0880252, Time: 148.37449407577515\n",
      "ToTal Epoch: 123, LR: 0.000010, Loss: 59290.9203465, Val MAE: 320.1659092, Test MAE: 331.0880252, Time: 150.5164725780487\n",
      "ToTal Epoch: 124, LR: 0.000010, Loss: 59630.9669648, Val MAE: 314.7011649, Test MAE: 331.0880252, Time: 154.69268250465393\n",
      "ToTal Epoch: 125, LR: 0.000010, Loss: 59242.5773635, Val MAE: 399.4995353, Test MAE: 331.0880252, Time: 134.3851478099823\n",
      "ToTal Epoch: 126, LR: 0.000010, Loss: 59342.5363816, Val MAE: 308.8131896, Test MAE: 331.0880252, Time: 132.91848516464233\n",
      "ToTal Epoch: 127, LR: 0.000010, Loss: 59824.1942740, Val MAE: 300.5274346, Test MAE: 327.2499131, Time: 156.77744245529175\n",
      "ToTal Epoch: 128, LR: 0.000010, Loss: 59176.4319346, Val MAE: 303.7780736, Test MAE: 327.2499131, Time: 166.39067959785461\n",
      "ToTal Epoch: 129, LR: 0.000010, Loss: 59237.4043627, Val MAE: 340.1554164, Test MAE: 327.2499131, Time: 170.32588410377502\n",
      "ToTal Epoch: 130, LR: 0.000010, Loss: 59484.7522160, Val MAE: 362.9336645, Test MAE: 327.2499131, Time: 177.15158081054688\n",
      "ToTal Epoch: 131, LR: 0.000010, Loss: 58959.4608692, Val MAE: 365.8587339, Test MAE: 327.2499131, Time: 142.09273052215576\n",
      "ToTal Epoch: 132, LR: 0.000010, Loss: 59064.4919009, Val MAE: 371.9572042, Test MAE: 327.2499131, Time: 157.9181089401245\n",
      "ToTal Epoch: 133, LR: 0.000010, Loss: 59377.4969826, Val MAE: 297.7838626, Test MAE: 324.5583049, Time: 144.73391819000244\n",
      "ToTal Epoch: 134, LR: 0.000010, Loss: 59677.3596809, Val MAE: 422.6660986, Test MAE: 324.5583049, Time: 151.66956877708435\n",
      "ToTal Epoch: 135, LR: 0.000010, Loss: 59575.4137558, Val MAE: 508.4055969, Test MAE: 324.5583049, Time: 146.84688091278076\n",
      "ToTal Epoch: 136, LR: 0.000010, Loss: 59778.4751397, Val MAE: 585.4747227, Test MAE: 324.5583049, Time: 171.57025408744812\n",
      "ToTal Epoch: 137, LR: 0.000010, Loss: 59374.1130697, Val MAE: 298.3693376, Test MAE: 324.5583049, Time: 156.81142807006836\n",
      "ToTal Epoch: 138, LR: 0.000010, Loss: 59357.6648981, Val MAE: 299.8180638, Test MAE: 324.5583049, Time: 139.1382932662964\n",
      "ToTal Epoch: 139, LR: 0.000010, Loss: 59468.0920353, Val MAE: 532.5726018, Test MAE: 324.5583049, Time: 154.53352308273315\n",
      "ToTal Epoch: 140, LR: 0.000010, Loss: 59490.4286079, Val MAE: 347.5168148, Test MAE: 324.5583049, Time: 176.0724232196808\n",
      "ToTal Epoch: 141, LR: 0.000010, Loss: 59593.1133635, Val MAE: 312.4797233, Test MAE: 324.5583049, Time: 164.06970643997192\n",
      "ToTal Epoch: 142, LR: 0.000010, Loss: 59166.6135390, Val MAE: 322.2601197, Test MAE: 324.5583049, Time: 157.00767350196838\n",
      "ToTal Epoch: 143, LR: 0.000010, Loss: 59433.9151465, Val MAE: 307.5505870, Test MAE: 324.5583049, Time: 157.5693805217743\n",
      "ToTal Epoch: 144, LR: 0.000010, Loss: 59465.6355623, Val MAE: 483.6256579, Test MAE: 324.5583049, Time: 155.15176630020142\n",
      "ToTal Epoch: 145, LR: 0.000010, Loss: 59306.8710821, Val MAE: 572.9144606, Test MAE: 324.5583049, Time: 135.88859033584595\n",
      "ToTal Epoch: 146, LR: 0.000010, Loss: 58867.3041191, Val MAE: 306.8344540, Test MAE: 324.5583049, Time: 162.59685516357422\n",
      "ToTal Epoch: 147, LR: 0.000010, Loss: 59662.5338034, Val MAE: 297.7209710, Test MAE: 326.0154230, Time: 177.17867302894592\n",
      "ToTal Epoch: 148, LR: 0.000010, Loss: 59683.7708206, Val MAE: 354.0195018, Test MAE: 326.0154230, Time: 173.63137936592102\n",
      "ToTal Epoch: 149, LR: 0.000010, Loss: 59188.0659479, Val MAE: 325.6656460, Test MAE: 326.0154230, Time: 165.29771089553833\n",
      "ToTal Epoch: 150, LR: 0.000010, Loss: 59571.2836162, Val MAE: 347.5384583, Test MAE: 326.0154230, Time: 150.67558026313782\n",
      "ToTal Epoch: 151, LR: 0.000010, Loss: 59036.8297383, Val MAE: 296.5742972, Test MAE: 323.9060308, Time: 157.1278748512268\n",
      "ToTal Epoch: 152, LR: 0.000010, Loss: 59084.7166824, Val MAE: 349.7261050, Test MAE: 323.9060308, Time: 157.69232964515686\n",
      "ToTal Epoch: 153, LR: 0.000010, Loss: 59650.1721624, Val MAE: 766.8685660, Test MAE: 323.9060308, Time: 156.39421772956848\n",
      "ToTal Epoch: 154, LR: 0.000010, Loss: 59584.9359970, Val MAE: 382.1016059, Test MAE: 323.9060308, Time: 146.64920210838318\n",
      "ToTal Epoch: 155, LR: 0.000010, Loss: 59569.7263382, Val MAE: 456.8367433, Test MAE: 323.9060308, Time: 153.3197479248047\n",
      "ToTal Epoch: 156, LR: 0.000010, Loss: 59505.2609510, Val MAE: 342.8683342, Test MAE: 323.9060308, Time: 160.05839467048645\n",
      "ToTal Epoch: 157, LR: 0.000010, Loss: 59472.1662381, Val MAE: 309.0649637, Test MAE: 323.9060308, Time: 159.554292678833\n",
      "ToTal Epoch: 158, LR: 0.000010, Loss: 59338.7249068, Val MAE: 318.6460173, Test MAE: 323.9060308, Time: 161.83580875396729\n",
      "ToTal Epoch: 159, LR: 0.000010, Loss: 59949.9334657, Val MAE: 680.4793660, Test MAE: 323.9060308, Time: 155.5562300682068\n",
      "ToTal Epoch: 160, LR: 0.000010, Loss: 59784.6010761, Val MAE: 297.6558776, Test MAE: 323.9060308, Time: 147.68181037902832\n",
      "ToTal Epoch: 161, LR: 0.000010, Loss: 59584.3184995, Val MAE: 336.0595825, Test MAE: 323.9060308, Time: 129.90183019638062\n",
      "ToTal Epoch: 162, LR: 0.000010, Loss: 59525.6316378, Val MAE: 298.3210657, Test MAE: 323.9060308, Time: 167.8291003704071\n",
      "ToTal Epoch: 163, LR: 0.000010, Loss: 59828.1220173, Val MAE: 344.0860858, Test MAE: 323.9060308, Time: 172.10815739631653\n",
      "ToTal Epoch: 164, LR: 0.000010, Loss: 59958.1813267, Val MAE: 314.6265900, Test MAE: 323.9060308, Time: 155.956041097641\n",
      "ToTal Epoch: 165, LR: 0.000010, Loss: 59598.7194812, Val MAE: 318.7432346, Test MAE: 323.9060308, Time: 152.09740138053894\n",
      "ToTal Epoch: 166, LR: 0.000010, Loss: 59352.8558866, Val MAE: 372.3767115, Test MAE: 323.9060308, Time: 164.28989839553833\n",
      "ToTal Epoch: 167, LR: 0.000010, Loss: 58871.6329253, Val MAE: 361.5801227, Test MAE: 323.9060308, Time: 160.74101376533508\n",
      "ToTal Epoch: 168, LR: 0.000010, Loss: 59072.5394073, Val MAE: 311.9578301, Test MAE: 323.9060308, Time: 163.82274413108826\n",
      "ToTal Epoch: 169, LR: 0.000010, Loss: 59619.9279356, Val MAE: 324.5063159, Test MAE: 323.9060308, Time: 146.58185338974\n",
      "ToTal Epoch: 170, LR: 0.000010, Loss: 59346.6852452, Val MAE: 302.3482288, Test MAE: 323.9060308, Time: 139.1716718673706\n",
      "ToTal Epoch: 171, LR: 0.000010, Loss: 58987.2954113, Val MAE: 442.4023046, Test MAE: 323.9060308, Time: 167.97940492630005\n",
      "ToTal Epoch: 172, LR: 0.000010, Loss: 59477.3433365, Val MAE: 300.5626458, Test MAE: 323.9060308, Time: 157.49705696105957\n",
      "ToTal Epoch: 173, LR: 0.000010, Loss: 59426.4068815, Val MAE: 667.2001953, Test MAE: 323.9060308, Time: 167.64486694335938\n",
      "ToTal Epoch: 174, LR: 0.000010, Loss: 59340.7069209, Val MAE: 318.3637183, Test MAE: 323.9060308, Time: 177.39674258232117\n",
      "ToTal Epoch: 175, LR: 0.000010, Loss: 59299.0639379, Val MAE: 371.8472082, Test MAE: 323.9060308, Time: 162.33766674995422\n",
      "ToTal Epoch: 176, LR: 0.000010, Loss: 59135.6811350, Val MAE: 397.0707330, Test MAE: 323.9060308, Time: 157.5605354309082\n",
      "ToTal Epoch: 177, LR: 0.000010, Loss: 59195.3431818, Val MAE: 420.0029246, Test MAE: 323.9060308, Time: 147.29309105873108\n",
      "ToTal Epoch: 178, LR: 0.000010, Loss: 59645.5836795, Val MAE: 426.1112606, Test MAE: 323.9060308, Time: 138.0543918609619\n",
      "ToTal Epoch: 179, LR: 0.000010, Loss: 59234.0692716, Val MAE: 361.0226113, Test MAE: 323.9060308, Time: 157.90411567687988\n",
      "ToTal Epoch: 180, LR: 0.000010, Loss: 59486.6335081, Val MAE: 452.7049392, Test MAE: 323.9060308, Time: 139.30103421211243\n",
      "ToTal Epoch: 181, LR: 0.000010, Loss: 59684.6033464, Val MAE: 330.3378253, Test MAE: 323.9060308, Time: 162.4680802822113\n",
      "ToTal Epoch: 182, LR: 0.000010, Loss: 59468.8918436, Val MAE: 377.3575554, Test MAE: 323.9060308, Time: 142.02155590057373\n",
      "ToTal Epoch: 183, LR: 0.000010, Loss: 59148.8881855, Val MAE: 398.4986206, Test MAE: 323.9060308, Time: 154.96145844459534\n",
      "ToTal Epoch: 184, LR: 0.000010, Loss: 59024.3753141, Val MAE: 299.8970296, Test MAE: 323.9060308, Time: 130.57689690589905\n",
      "ToTal Epoch: 185, LR: 0.000010, Loss: 59371.2321460, Val MAE: 370.8400787, Test MAE: 323.9060308, Time: 149.85857272148132\n",
      "ToTal Epoch: 186, LR: 0.000010, Loss: 59266.6780854, Val MAE: 304.9010076, Test MAE: 323.9060308, Time: 135.95225930213928\n",
      "ToTal Epoch: 187, LR: 0.000010, Loss: 59558.8040701, Val MAE: 313.1497862, Test MAE: 323.9060308, Time: 139.2129077911377\n",
      "ToTal Epoch: 188, LR: 0.000010, Loss: 59759.3865601, Val MAE: 351.4418780, Test MAE: 323.9060308, Time: 155.45219898223877\n",
      "ToTal Epoch: 189, LR: 0.000010, Loss: 59296.6363744, Val MAE: 355.8193538, Test MAE: 323.9060308, Time: 160.0934112071991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31207/2957218169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_time_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to measure time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31207/1765397410.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (predicted value, true value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31207/3367470662.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31207/2130132424.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n\u001b[0m\u001b[1;32m    185\u001b[0m                              size=None)\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                         \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msegment_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n\u001b[0m\u001b[1;32m    386\u001b[0m                            reduce=self.aggr)\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#input parameters\n",
    "total_num_epoch = 0 #the total number of epoch that have run\n",
    "running_num_epoch = 300 #the number of epoch that run in this time\n",
    "\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "    \n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    epoch_time_end = time.time() # to measure time\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "\n",
    "total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total Time: {total_time_finish - total_time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddace62-b768-4693-b971-37ec46648859",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "503420b3-13cb-40d2-aad1-c96b5d8a0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOPCH = total_num_epoch\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EOPCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model/GCN_skip_ensemble_epoch300_20211212')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfde76-27c1-4468-a9ea-d7127b591588",
   "metadata": {},
   "source": [
    "To load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "432e800b-da88-4d12-b8d3-8a340d98d912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(11, 80)\n",
       "  (conv2): GCNConv(80, 80)\n",
       "  (conv3): GCNConv(80, 80)\n",
       "  (conv4): GCNConv(80, 80)\n",
       "  (conv5): GCNConv(80, 80)\n",
       "  (lin1): Linear(in_features=80, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = 'model/GCNConv01-2_epoch300_20211207'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8e2b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a08556",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b38a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "batch_size= 256\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d9e791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, SAGPooling, global_add_pool\n",
    "\n",
    "class GAT_Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.in_channels = in_channels # dim(node features)\n",
    "        self.hidden_channels = hidden_channels # dim(node embedding)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        #self.layers1 = [d_graph_layer for i in range(n_graph_layer+1)]\n",
    "        #self.GATConvList = nn.ModuleList([GAT_gate(self.layers1[i], self.layers1[i+1]) for i in range(len(self.layers1)-1)])\n",
    "        #self.depth = depth # the number of message passing\n",
    "        \n",
    "        # model structure\n",
    "        self.lin0 = Linear(self.in_channels, self.in_channels)\n",
    "        self.conv1 = GATConv(in_channels= self.in_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        self.MultiHeadAttention_lin1 = Linear(self.hidden_channels * self.heads, self.hidden_channels)\n",
    "        #self.pool1 = SAGPooling(in_channels= self.hidden_channels*self.heads , ratio=1/self.heads, GNN=GATConv, min_score=0.001)\n",
    "        self.conv2 = GATConv(in_channels= self.hidden_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        #self.MultiHeadAttention_lin2 = Linear(self.hidden_channels * self.heads, self.hidden_channels)\n",
    "        self.conv3 = GATConv(in_channels= self.hidden_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        #self.MultiHeadAttention_lin3 = Linear(self.hidden_channels * self.heads, self.hidden_channels)\n",
    "        #self.conv4 = GATConv(in_channels= self.hidden_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        #self.MultiHeadAttention_lin4 = Linear(self.hidden_channels * self.heads, self.hidden_channels)\n",
    "        #self.conv5 = GATConv(in_channels= self.hidden_channels, out_channels= self.hidden_channels, heads=1)\n",
    "        #self.skip = Linear(self.hidden_channels * 2, 1)\n",
    "        self.lin1 = Linear(self.hidden_channels, self.hidden_channels) # readout function\n",
    "        self.lin2 = Linear(self.hidden_channels, self.out_channels) # predictor\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "         \n",
    "        x = self.lin0(x)\n",
    "    \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.MultiHeadAttention_lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.MultiHeadAttention_lin1(x) #shared\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.MultiHeadAttention_lin1(x)\n",
    "        \n",
    "        #x = self.conv4(x, edge_index)\n",
    "        #x = F.elu(x)\n",
    "        #x = self.MultiHeadAttention_lin1(x)\n",
    "        \n",
    "        #x = self.conv5(x, edge_index)\n",
    "        #x = F.elu(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ecb309e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT_Net(\n",
       "  (lin0): Linear(in_features=11, out_features=11, bias=True)\n",
       "  (conv1): GATConv(11, 150, heads=15)\n",
       "  (MultiHeadAttention_lin1): Linear(in_features=2250, out_features=150, bias=True)\n",
       "  (conv2): GATConv(150, 150, heads=15)\n",
       "  (conv3): GATConv(150, 150, heads=15)\n",
       "  (lin1): Linear(in_features=150, out_features=150, bias=True)\n",
       "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAT_Net(in_channels= 11, hidden_channels= 150, out_channels= 1, heads=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a10528b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT_Net(in_channels= 11, hidden_channels= 150, out_channels= 1, heads=7).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=30, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9c710549-6673-44fc-ae5d-38676cab877f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "724f8bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT_Net(\n",
      "  (lin0): Linear(in_features=11, out_features=11, bias=True)\n",
      "  (conv1): GATConv(11, 150, heads=7)\n",
      "  (MultiHeadAttention_lin1): Linear(in_features=1050, out_features=150, bias=True)\n",
      "  (conv2): GATConv(150, 150, heads=7)\n",
      "  (conv3): GATConv(150, 150, heads=7)\n",
      "  (lin1): Linear(in_features=150, out_features=150, bias=True)\n",
      "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0dd25056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------+\n",
      "|            Modules             | Parameters |\n",
      "+--------------------------------+------------+\n",
      "|          lin0.weight           |    121     |\n",
      "|           lin0.bias            |     11     |\n",
      "|         conv1.att_src          |    1050    |\n",
      "|         conv1.att_dst          |    1050    |\n",
      "|           conv1.bias           |    1050    |\n",
      "|      conv1.lin_src.weight      |   11550    |\n",
      "| MultiHeadAttention_lin1.weight |   157500   |\n",
      "|  MultiHeadAttention_lin1.bias  |    150     |\n",
      "|         conv2.att_src          |    1050    |\n",
      "|         conv2.att_dst          |    1050    |\n",
      "|           conv2.bias           |    1050    |\n",
      "|      conv2.lin_src.weight      |   157500   |\n",
      "|         conv3.att_src          |    1050    |\n",
      "|         conv3.att_dst          |    1050    |\n",
      "|           conv3.bias           |    1050    |\n",
      "|      conv3.lin_src.weight      |   157500   |\n",
      "|          lin1.weight           |   22500    |\n",
      "|           lin1.bias            |    150     |\n",
      "|          lin2.weight           |    150     |\n",
      "|           lin2.bias            |     1      |\n",
      "+--------------------------------+------------+\n",
      "Total Trainable Params: 516583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "516583"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853fce0-0ef2-4881-b9fd-cf42aa01759d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f283948-c75c-4aa2-9d7d-75bcbb84a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, SAGPooling, global_add_pool\n",
    "\n",
    "class GAT_overfitting_Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.in_channels = in_channels # dim(node features)\n",
    "        self.hidden_channels = hidden_channels # dim(node embedding)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    " \n",
    "        # model structure\n",
    "        self.lin0 = Linear(self.in_channels, self.in_channels)\n",
    "        self.conv1 = GATConv(in_channels= self.in_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        self.MultiHeadAttention_lin1 = Linear(self.hidden_channels * self.heads, self.hidden_channels)\n",
    "        self.conv3 = GATConv(in_channels= self.hidden_channels, out_channels= self.hidden_channels, heads=self.heads)\n",
    "        self.lin1 = Linear(self.hidden_channels, self.hidden_channels) # readout function\n",
    "        self.lin2 = Linear(self.hidden_channels, self.out_channels) # predictor\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "         \n",
    "        x = self.lin0(x)\n",
    "    \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.MultiHeadAttention_lin1(x)   \n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.MultiHeadAttention_lin1(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb57a90-4d5b-4bba-b80f-d049dc21c194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "-------------------------------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT_overfitting_Net(in_channels= 11, hidden_channels= 10, out_channels= 1, heads=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=10, min_lr=0.0025)\n",
    "print(device)\n",
    "print('-------------------------------')\n",
    "print(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee648353-26a3-4ac4-a9ab-4dc4d95a7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0025, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=10, min_lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d356a31-f0b0-4189-a5dc-3ea69e644bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_single_batch(device, total_num_epoch, running_num_epoch, tf_board_directory, model_save_directory):\n",
    "    # load modules\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import time\n",
    "    writer = SummaryWriter(tf_board_directory)\n",
    "    ## logdir=./python/run/GAT_Net/run_02\n",
    "    \n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    epoch = int(running_num_epoch)\n",
    "    \n",
    "    for iteration in range(epoch):\n",
    "        # At iteration =0, load a single batch (without training)\n",
    "        if iteration == 0 :\n",
    "            for idx, batch in enumerate(train_loader):\n",
    "                batch = batch.to(device)\n",
    "                break\n",
    "        # At ieration >=1, train a single batch until overfitting\n",
    "        else :\n",
    "            epoch_time_start = time.time() # to measure time\n",
    "            lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            y = torch.index_select(batch.y.to(device), 1, torch.tensor(10).to(device)).to(device)\n",
    "            optimizer.zero_grad() #initialization\n",
    "            loss = F.mse_loss(model(batch), y.to(device)) # (predicted value, true value)\n",
    "            #print(model(batch))\n",
    "            #print(y.to(device))\n",
    "            #print(loss)\n",
    "            loss.backward() # compute gradient\n",
    "            loss_all += loss.item() * batch.num_graphs\n",
    "            #print(loss.item())\n",
    "            #print(batch.num_graphs)\n",
    "            #print(loss_all)\n",
    "            optimizer.step()  # to update the parameters\n",
    "            scheduler.step(loss_all)\n",
    "            epoch_time_end = time.time() # to measure time\n",
    "            total_num_epoch = total_num_epoch + 1\n",
    "            writer.add_scalar('loss in train', loss, total_num_epoch) #tensorboard\n",
    "            writer.add_scalar('learning rate', lr, total_num_epoch) #tensorboard\n",
    "        # print        \n",
    "        if iteration != 0 and iteration%1000 == 0:\n",
    "                print(f\"IDX: {idx:2d}\\tTotal Epoch: {total_num_epoch:10d}\\tLR: {lr:7f}\\tLoss: {loss:.6f}\\tTime: {epoch_time_end - epoch_time_start:.3f}\")\n",
    "        \n",
    "        # overfitting\n",
    "        if iteration != 0 and loss_all < 1e-1:\n",
    "            print(f\"overfitting is reached on epoch {iteration:10d}\")\n",
    "            break\n",
    "\n",
    "    writer.close() #tensorboard : if close() is not declared, the writer does not save any valeus.\n",
    "    \n",
    "    # model save\n",
    "    torch.save({\n",
    "            'epoch': total_num_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, model_save_directory)\n",
    "    \n",
    "    print(\"-------------------------done------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec986452-8463-4a73-9185-a3c62ff25d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDX:     0\tTotal Epoch:     666000\tLR: 0.000500\tLoss: 288749.562500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     667000\tLR: 0.000500\tLoss: 227174.515625\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     668000\tLR: 0.000500\tLoss: 208977.218750\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     669000\tLR: 0.000500\tLoss: 188319.656250\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     670000\tLR: 0.000500\tLoss: 186028.750000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     671000\tLR: 0.000500\tLoss: 185725.187500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     672000\tLR: 0.000500\tLoss: 180717.062500\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     673000\tLR: 0.000500\tLoss: 180471.531250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     674000\tLR: 0.000500\tLoss: 179364.875000\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     675000\tLR: 0.000500\tLoss: 178899.562500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     676000\tLR: 0.000500\tLoss: 179254.734375\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     677000\tLR: 0.000500\tLoss: 178804.015625\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     678000\tLR: 0.000500\tLoss: 179351.281250\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     679000\tLR: 0.000500\tLoss: 178438.156250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     680000\tLR: 0.000500\tLoss: 165529.812500\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     681000\tLR: 0.000500\tLoss: 164490.828125\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     682000\tLR: 0.000500\tLoss: 165470.546875\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     683000\tLR: 0.000500\tLoss: 160973.578125\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     684000\tLR: 0.000500\tLoss: 164121.140625\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     685000\tLR: 0.000500\tLoss: 164193.468750\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     686000\tLR: 0.000500\tLoss: 164034.187500\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     687000\tLR: 0.000500\tLoss: 164006.375000\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     688000\tLR: 0.000500\tLoss: 163657.593750\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     689000\tLR: 0.000500\tLoss: 163969.250000\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     690000\tLR: 0.000500\tLoss: 163814.031250\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     691000\tLR: 0.000500\tLoss: 156165.656250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     692000\tLR: 0.000500\tLoss: 149030.843750\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     693000\tLR: 0.000500\tLoss: 167086.781250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     694000\tLR: 0.000500\tLoss: 168154.406250\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     695000\tLR: 0.000500\tLoss: 167445.750000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     696000\tLR: 0.000500\tLoss: 166993.390625\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     697000\tLR: 0.000500\tLoss: 166362.546875\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     698000\tLR: 0.000500\tLoss: 184716.218750\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     699000\tLR: 0.000500\tLoss: 167126.968750\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     700000\tLR: 0.000500\tLoss: 167159.843750\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     701000\tLR: 0.000500\tLoss: 168112.906250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     702000\tLR: 0.000500\tLoss: 162111.687500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     703000\tLR: 0.000500\tLoss: 168736.250000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     704000\tLR: 0.000500\tLoss: 168716.250000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     705000\tLR: 0.000500\tLoss: 144611.984375\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     706000\tLR: 0.000500\tLoss: 138101.484375\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     707000\tLR: 0.000500\tLoss: 135614.812500\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     708000\tLR: 0.000500\tLoss: 136675.750000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     709000\tLR: 0.000500\tLoss: 135092.968750\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     710000\tLR: 0.000500\tLoss: 132758.312500\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     711000\tLR: 0.000500\tLoss: 132170.265625\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     712000\tLR: 0.000500\tLoss: 128870.914062\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     713000\tLR: 0.000500\tLoss: 128490.906250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     714000\tLR: 0.000500\tLoss: 130585.812500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     715000\tLR: 0.000500\tLoss: 128099.710938\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     716000\tLR: 0.000500\tLoss: 128735.312500\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     717000\tLR: 0.000500\tLoss: 127989.515625\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     718000\tLR: 0.000500\tLoss: 127955.406250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     719000\tLR: 0.000500\tLoss: 128279.750000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     720000\tLR: 0.000500\tLoss: 128095.820312\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     721000\tLR: 0.000500\tLoss: 127790.531250\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     722000\tLR: 0.000500\tLoss: 127788.867188\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     723000\tLR: 0.000500\tLoss: 127692.531250\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     724000\tLR: 0.000500\tLoss: 127869.687500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     725000\tLR: 0.000500\tLoss: 127229.234375\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     726000\tLR: 0.000500\tLoss: 127267.875000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     727000\tLR: 0.000500\tLoss: 127713.398438\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     728000\tLR: 0.000500\tLoss: 127774.882812\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     729000\tLR: 0.000500\tLoss: 126751.421875\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     730000\tLR: 0.000500\tLoss: 119307.984375\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     731000\tLR: 0.000500\tLoss: 118915.015625\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     732000\tLR: 0.000500\tLoss: 117749.054688\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     733000\tLR: 0.000500\tLoss: 117654.929688\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     734000\tLR: 0.000500\tLoss: 118133.867188\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     735000\tLR: 0.000500\tLoss: 118085.710938\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     736000\tLR: 0.000500\tLoss: 116377.406250\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     737000\tLR: 0.000500\tLoss: 119273.984375\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     738000\tLR: 0.000500\tLoss: 121632.460938\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     739000\tLR: 0.000500\tLoss: 117480.820312\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     740000\tLR: 0.000500\tLoss: 116003.718750\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     741000\tLR: 0.000500\tLoss: 115527.140625\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     742000\tLR: 0.000500\tLoss: 118440.820312\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     743000\tLR: 0.000500\tLoss: 113609.625000\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     744000\tLR: 0.000500\tLoss: 117654.726562\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     745000\tLR: 0.000500\tLoss: 112693.531250\tTime: 0.016\n",
      "IDX:     0\tTotal Epoch:     746000\tLR: 0.000500\tLoss: 124997.851562\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     747000\tLR: 0.000500\tLoss: 111900.500000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     748000\tLR: 0.000500\tLoss: 116591.312500\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     749000\tLR: 0.000500\tLoss: 111652.398438\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     750000\tLR: 0.000500\tLoss: 111718.843750\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     751000\tLR: 0.000500\tLoss: 114565.945312\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     752000\tLR: 0.000500\tLoss: 111300.117188\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     753000\tLR: 0.000500\tLoss: 118539.125000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     754000\tLR: 0.000500\tLoss: 110186.140625\tTime: 0.015\n",
      "IDX:     0\tTotal Epoch:     755000\tLR: 0.000500\tLoss: 112067.992188\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     756000\tLR: 0.000500\tLoss: 110040.640625\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     757000\tLR: 0.000500\tLoss: 116447.718750\tTime: 0.012\n",
      "IDX:     0\tTotal Epoch:     758000\tLR: 0.000500\tLoss: 108438.882812\tTime: 0.012\n",
      "IDX:     0\tTotal Epoch:     759000\tLR: 0.000500\tLoss: 108948.804688\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     760000\tLR: 0.000500\tLoss: 107906.875000\tTime: 0.014\n",
      "IDX:     0\tTotal Epoch:     761000\tLR: 0.000500\tLoss: 110624.710938\tTime: 0.013\n",
      "IDX:     0\tTotal Epoch:     762000\tLR: 0.000500\tLoss: 109129.265625\tTime: 0.014\n"
     ]
    }
   ],
   "source": [
    "overfit_single_batch(device='cuda', total_num_epoch=665000, running_num_epoch=1e10, tf_board_directory='run/GAT_overfiting_Net/run_01', model_save_directory='model/GAT_SingleOverfit_epoch1e_run01_20211219')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ce7f7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        #y = torch.index_select(batch.y.to(device), 1, torch.tensor(10).to(device)).to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10).to(device)).to(device)\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90135711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        #batch = data.to(device) #it trigger error!\n",
    "        out = model(batch).to(device)\n",
    "        y = torch.index_select(batch.y.to(device), 1, torch.tensor(10).to(device)).to(device)\n",
    "        tmp = (out.to(device) - y.to(device))**2\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "87d6127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125901953.06642208"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTal Epoch: 1501, LR: 0.000003, Loss: 209667.2944728, Val MAE: 3057.4786555, Test MAE: 3022.0600636, Time: 13.274787664413452\n",
      "ToTal Epoch: 1502, LR: 0.000003, Loss: 211572.6046720, Val MAE: 2839.5998242, Test MAE: 2804.4568620, Time: 13.883838176727295\n",
      "ToTal Epoch: 1503, LR: 0.000003, Loss: 208501.2974347, Val MAE: 3294.2207493, Test MAE: 2804.4568620, Time: 11.774081468582153\n",
      "ToTal Epoch: 1504, LR: 0.000003, Loss: 209348.4432427, Val MAE: 2524.8147166, Test MAE: 2496.3766863, Time: 13.53332781791687\n",
      "ToTal Epoch: 1505, LR: 0.000003, Loss: 210454.1956146, Val MAE: 2276.2463383, Test MAE: 2260.4902808, Time: 12.313076257705688\n",
      "ToTal Epoch: 1506, LR: 0.000003, Loss: 209481.4090288, Val MAE: 3012.9342037, Test MAE: 2260.4902808, Time: 11.753117084503174\n",
      "ToTal Epoch: 1507, LR: 0.000003, Loss: 210598.7201452, Val MAE: 2742.1497076, Test MAE: 2260.4902808, Time: 11.22588038444519\n",
      "ToTal Epoch: 1508, LR: 0.000003, Loss: 211213.5353365, Val MAE: 2722.0892045, Test MAE: 2260.4902808, Time: 10.87298035621643\n",
      "ToTal Epoch: 1509, LR: 0.000003, Loss: 211804.5131801, Val MAE: 3387.6437887, Test MAE: 2260.4902808, Time: 10.665250301361084\n",
      "ToTal Epoch: 1510, LR: 0.000003, Loss: 209285.8500549, Val MAE: 2051.9569408, Test MAE: 2026.6671730, Time: 12.960161924362183\n",
      "ToTal Epoch: 1511, LR: 0.000003, Loss: 210506.4354273, Val MAE: 2888.9028892, Test MAE: 2026.6671730, Time: 10.841697692871094\n",
      "ToTal Epoch: 1512, LR: 0.000003, Loss: 210254.0673960, Val MAE: 3362.2724241, Test MAE: 2026.6671730, Time: 11.131186246871948\n",
      "ToTal Epoch: 1513, LR: 0.000003, Loss: 210534.9136005, Val MAE: 3999.9215920, Test MAE: 2026.6671730, Time: 11.337837219238281\n",
      "ToTal Epoch: 1514, LR: 0.000003, Loss: 210008.1584484, Val MAE: 2043.4189597, Test MAE: 2017.4051895, Time: 13.071624755859375\n",
      "ToTal Epoch: 1515, LR: 0.000003, Loss: 212784.7456934, Val MAE: 2815.3198091, Test MAE: 2017.4051895, Time: 11.551842451095581\n",
      "ToTal Epoch: 1516, LR: 0.000003, Loss: 210463.0088759, Val MAE: 4491.9673813, Test MAE: 2017.4051895, Time: 11.190128564834595\n",
      "ToTal Epoch: 1517, LR: 0.000003, Loss: 208883.2948168, Val MAE: 2800.5112407, Test MAE: 2017.4051895, Time: 11.295620441436768\n",
      "ToTal Epoch: 1518, LR: 0.000003, Loss: 209577.7002054, Val MAE: 2909.0325852, Test MAE: 2017.4051895, Time: 11.360085725784302\n",
      "ToTal Epoch: 1519, LR: 0.000003, Loss: 210037.8419911, Val MAE: 3290.8810288, Test MAE: 2017.4051895, Time: 12.145836353302002\n",
      "ToTal Epoch: 1520, LR: 0.000003, Loss: 210566.2506091, Val MAE: 2415.8337394, Test MAE: 2017.4051895, Time: 12.38418197631836\n",
      "ToTal Epoch: 1521, LR: 0.000003, Loss: 210344.7025080, Val MAE: 2840.2922686, Test MAE: 2017.4051895, Time: 11.293312549591064\n",
      "ToTal Epoch: 1522, LR: 0.000003, Loss: 210429.5202026, Val MAE: 2907.5457703, Test MAE: 2017.4051895, Time: 12.789875507354736\n",
      "ToTal Epoch: 1523, LR: 0.000003, Loss: 210108.6270291, Val MAE: 2773.2331031, Test MAE: 2017.4051895, Time: 11.534959077835083\n",
      "ToTal Epoch: 1524, LR: 0.000003, Loss: 211265.7367792, Val MAE: 3413.1006507, Test MAE: 2017.4051895, Time: 12.085096597671509\n",
      "ToTal Epoch: 1525, LR: 0.000003, Loss: 209400.9112884, Val MAE: 4004.1028912, Test MAE: 2017.4051895, Time: 12.991882801055908\n",
      "ToTal Epoch: 1526, LR: 0.000003, Loss: 210497.2888740, Val MAE: 2870.7443629, Test MAE: 2017.4051895, Time: 11.881484508514404\n",
      "ToTal Epoch: 1527, LR: 0.000003, Loss: 208846.7630249, Val MAE: 2683.0156453, Test MAE: 2017.4051895, Time: 11.674003601074219\n",
      "ToTal Epoch: 1528, LR: 0.000003, Loss: 210717.6701285, Val MAE: 3425.8472445, Test MAE: 2017.4051895, Time: 11.361157417297363\n",
      "ToTal Epoch: 1529, LR: 0.000003, Loss: 209123.5773945, Val MAE: 2834.0610668, Test MAE: 2017.4051895, Time: 11.992292404174805\n",
      "ToTal Epoch: 1530, LR: 0.000003, Loss: 210600.3109349, Val MAE: 2581.9240522, Test MAE: 2017.4051895, Time: 11.336021900177002\n",
      "ToTal Epoch: 1531, LR: 0.000003, Loss: 210440.5546458, Val MAE: 2721.5551001, Test MAE: 2017.4051895, Time: 10.629817962646484\n",
      "ToTal Epoch: 1532, LR: 0.000003, Loss: 209736.9478622, Val MAE: 2482.5350264, Test MAE: 2017.4051895, Time: 11.087236881256104\n",
      "ToTal Epoch: 1533, LR: 0.000003, Loss: 209342.3057947, Val MAE: 2771.3063135, Test MAE: 2017.4051895, Time: 12.414382219314575\n",
      "ToTal Epoch: 1534, LR: 0.000003, Loss: 209139.4187169, Val MAE: 3030.5611289, Test MAE: 2017.4051895, Time: 12.64075517654419\n",
      "ToTal Epoch: 1535, LR: 0.000003, Loss: 209393.0713037, Val MAE: 3129.4586104, Test MAE: 2017.4051895, Time: 11.362025022506714\n",
      "ToTal Epoch: 1536, LR: 0.000003, Loss: 210281.0828453, Val MAE: 3083.7129672, Test MAE: 2017.4051895, Time: 12.322652339935303\n",
      "ToTal Epoch: 1537, LR: 0.000003, Loss: 210332.5815889, Val MAE: 3204.3037482, Test MAE: 2017.4051895, Time: 12.4555184841156\n",
      "ToTal Epoch: 1538, LR: 0.000003, Loss: 210642.7676492, Val MAE: 2894.6451741, Test MAE: 2017.4051895, Time: 12.083757877349854\n",
      "ToTal Epoch: 1539, LR: 0.000003, Loss: 211097.3667606, Val MAE: 2655.8387459, Test MAE: 2017.4051895, Time: 11.461428880691528\n",
      "ToTal Epoch: 1540, LR: 0.000003, Loss: 210015.4413796, Val MAE: 2650.1180492, Test MAE: 2017.4051895, Time: 12.347718477249146\n",
      "ToTal Epoch: 1541, LR: 0.000003, Loss: 210750.2245450, Val MAE: 2530.1501376, Test MAE: 2017.4051895, Time: 12.159277200698853\n",
      "ToTal Epoch: 1542, LR: 0.000003, Loss: 211149.1194573, Val MAE: 3218.8362331, Test MAE: 2017.4051895, Time: 10.85447359085083\n",
      "ToTal Epoch: 1543, LR: 0.000003, Loss: 210672.7058711, Val MAE: 3183.2709528, Test MAE: 2017.4051895, Time: 10.988905191421509\n",
      "ToTal Epoch: 1544, LR: 0.000003, Loss: 208465.0789471, Val MAE: 2533.3384593, Test MAE: 2017.4051895, Time: 11.830930233001709\n",
      "ToTal Epoch: 1545, LR: 0.000003, Loss: 208986.6131372, Val MAE: 2955.1664804, Test MAE: 2017.4051895, Time: 11.340564727783203\n",
      "ToTal Epoch: 1546, LR: 0.000003, Loss: 208527.1438590, Val MAE: 2515.9775520, Test MAE: 2017.4051895, Time: 11.05779218673706\n",
      "ToTal Epoch: 1547, LR: 0.000003, Loss: 210340.0872116, Val MAE: 4116.3814110, Test MAE: 2017.4051895, Time: 12.320781946182251\n",
      "ToTal Epoch: 1548, LR: 0.000003, Loss: 210084.8468160, Val MAE: 3459.0111309, Test MAE: 2017.4051895, Time: 11.949162006378174\n",
      "ToTal Epoch: 1549, LR: 0.000003, Loss: 208615.7012182, Val MAE: 3289.2939119, Test MAE: 2017.4051895, Time: 11.303545236587524\n",
      "ToTal Epoch: 1550, LR: 0.000003, Loss: 210371.4552907, Val MAE: 2468.8500583, Test MAE: 2017.4051895, Time: 12.913806200027466\n",
      "ToTal Epoch: 1551, LR: 0.000003, Loss: 210900.5486075, Val MAE: 3014.2356589, Test MAE: 2017.4051895, Time: 11.877046585083008\n",
      "ToTal Epoch: 1552, LR: 0.000003, Loss: 210652.3768595, Val MAE: 2559.2197222, Test MAE: 2017.4051895, Time: 12.690436124801636\n",
      "ToTal Epoch: 1553, LR: 0.000003, Loss: 211593.2766063, Val MAE: 2212.9615985, Test MAE: 2017.4051895, Time: 12.451730489730835\n",
      "ToTal Epoch: 1554, LR: 0.000003, Loss: 210703.6100702, Val MAE: 2895.7059782, Test MAE: 2017.4051895, Time: 10.834686279296875\n",
      "ToTal Epoch: 1555, LR: 0.000003, Loss: 210821.8492524, Val MAE: 3112.6417584, Test MAE: 2017.4051895, Time: 11.341205358505249\n",
      "ToTal Epoch: 1556, LR: 0.000003, Loss: 210582.6115320, Val MAE: 2257.7594278, Test MAE: 2017.4051895, Time: 11.500759840011597\n",
      "ToTal Epoch: 1557, LR: 0.000003, Loss: 209902.1105049, Val MAE: 2976.8724108, Test MAE: 2017.4051895, Time: 13.477658987045288\n",
      "ToTal Epoch: 1558, LR: 0.000003, Loss: 209401.4401758, Val MAE: 2743.5136962, Test MAE: 2017.4051895, Time: 10.528439283370972\n",
      "ToTal Epoch: 1559, LR: 0.000003, Loss: 209676.6616921, Val MAE: 3378.1778071, Test MAE: 2017.4051895, Time: 12.02564287185669\n",
      "ToTal Epoch: 1560, LR: 0.000003, Loss: 209283.7531553, Val MAE: 3422.6921004, Test MAE: 2017.4051895, Time: 12.790817260742188\n",
      "ToTal Epoch: 1561, LR: 0.000003, Loss: 211019.0196150, Val MAE: 3186.8749427, Test MAE: 2017.4051895, Time: 11.007970094680786\n",
      "ToTal Epoch: 1562, LR: 0.000003, Loss: 209923.7823914, Val MAE: 2929.8792947, Test MAE: 2017.4051895, Time: 11.35340428352356\n",
      "ToTal Epoch: 1563, LR: 0.000003, Loss: 210313.1098266, Val MAE: 3280.2088970, Test MAE: 2017.4051895, Time: 11.430075645446777\n",
      "ToTal Epoch: 1564, LR: 0.000003, Loss: 210576.6962022, Val MAE: 2681.6216177, Test MAE: 2017.4051895, Time: 11.390625238418579\n",
      "ToTal Epoch: 1565, LR: 0.000003, Loss: 209225.6280132, Val MAE: 3175.2505303, Test MAE: 2017.4051895, Time: 11.724624633789062\n",
      "ToTal Epoch: 1566, LR: 0.000002, Loss: 208867.5745665, Val MAE: 2649.6304794, Test MAE: 2017.4051895, Time: 10.72248649597168\n",
      "ToTal Epoch: 1567, LR: 0.000002, Loss: 210268.0689438, Val MAE: 3103.0957063, Test MAE: 2017.4051895, Time: 10.436302185058594\n",
      "ToTal Epoch: 1568, LR: 0.000002, Loss: 208675.5757512, Val MAE: 3259.8674664, Test MAE: 2017.4051895, Time: 12.221755743026733\n",
      "ToTal Epoch: 1569, LR: 0.000002, Loss: 210094.4788038, Val MAE: 2759.4427501, Test MAE: 2017.4051895, Time: 11.539191961288452\n",
      "ToTal Epoch: 1570, LR: 0.000002, Loss: 210109.1868342, Val MAE: 2809.3987379, Test MAE: 2017.4051895, Time: 12.594972610473633\n",
      "ToTal Epoch: 1571, LR: 0.000002, Loss: 209828.4937658, Val MAE: 3717.2587566, Test MAE: 2017.4051895, Time: 12.30849838256836\n",
      "ToTal Epoch: 1572, LR: 0.000002, Loss: 210316.9798882, Val MAE: 3414.2242700, Test MAE: 2017.4051895, Time: 11.738681554794312\n",
      "ToTal Epoch: 1573, LR: 0.000002, Loss: 211000.5386710, Val MAE: 2804.2693333, Test MAE: 2017.4051895, Time: 12.430029153823853\n",
      "ToTal Epoch: 1574, LR: 0.000002, Loss: 210282.0620456, Val MAE: 3147.5688537, Test MAE: 2017.4051895, Time: 10.822079181671143\n",
      "ToTal Epoch: 1575, LR: 0.000002, Loss: 210019.5161324, Val MAE: 2865.9406529, Test MAE: 2017.4051895, Time: 12.47189736366272\n",
      "ToTal Epoch: 1576, LR: 0.000002, Loss: 208495.6758420, Val MAE: 2327.6160499, Test MAE: 2017.4051895, Time: 11.368084192276001\n",
      "ToTal Epoch: 1577, LR: 0.000002, Loss: 209567.7595376, Val MAE: 3346.5622755, Test MAE: 2017.4051895, Time: 11.474475145339966\n",
      "ToTal Epoch: 1578, LR: 0.000002, Loss: 208368.7777576, Val MAE: 3476.1996771, Test MAE: 2017.4051895, Time: 11.821529865264893\n",
      "ToTal Epoch: 1579, LR: 0.000002, Loss: 209426.3568528, Val MAE: 2697.6722225, Test MAE: 2017.4051895, Time: 11.810428619384766\n",
      "ToTal Epoch: 1580, LR: 0.000002, Loss: 209987.8720107, Val MAE: 2581.3059075, Test MAE: 2017.4051895, Time: 10.931562662124634\n",
      "ToTal Epoch: 1581, LR: 0.000002, Loss: 209920.3241962, Val MAE: 3471.9060517, Test MAE: 2017.4051895, Time: 10.995802164077759\n",
      "ToTal Epoch: 1582, LR: 0.000002, Loss: 209496.4398414, Val MAE: 2702.2184228, Test MAE: 2017.4051895, Time: 10.815865278244019\n",
      "ToTal Epoch: 1583, LR: 0.000002, Loss: 210339.0488511, Val MAE: 3316.8019185, Test MAE: 2017.4051895, Time: 10.430869579315186\n",
      "ToTal Epoch: 1584, LR: 0.000002, Loss: 209462.9093584, Val MAE: 2946.5172504, Test MAE: 2017.4051895, Time: 11.855992555618286\n",
      "ToTal Epoch: 1585, LR: 0.000002, Loss: 210599.7440978, Val MAE: 2475.6434065, Test MAE: 2017.4051895, Time: 10.254186630249023\n",
      "ToTal Epoch: 1586, LR: 0.000002, Loss: 210153.7675823, Val MAE: 2566.2701359, Test MAE: 2017.4051895, Time: 10.923674583435059\n",
      "ToTal Epoch: 1587, LR: 0.000002, Loss: 210186.0057899, Val MAE: 2762.9041074, Test MAE: 2017.4051895, Time: 11.229059934616089\n",
      "ToTal Epoch: 1588, LR: 0.000002, Loss: 210810.1271676, Val MAE: 2534.8533880, Test MAE: 2017.4051895, Time: 10.836910724639893\n",
      "ToTal Epoch: 1589, LR: 0.000002, Loss: 211861.6906511, Val MAE: 3772.7028921, Test MAE: 2017.4051895, Time: 11.811991214752197\n",
      "ToTal Epoch: 1590, LR: 0.000002, Loss: 208767.2793388, Val MAE: 2832.4746140, Test MAE: 2017.4051895, Time: 11.238095045089722\n",
      "ToTal Epoch: 1591, LR: 0.000002, Loss: 210647.9156547, Val MAE: 3061.0173938, Test MAE: 2017.4051895, Time: 11.751448392868042\n",
      "ToTal Epoch: 1592, LR: 0.000002, Loss: 210384.8856829, Val MAE: 3290.7959184, Test MAE: 2017.4051895, Time: 11.87092900276184\n",
      "ToTal Epoch: 1593, LR: 0.000002, Loss: 210344.3924903, Val MAE: 2357.8954082, Test MAE: 2017.4051895, Time: 11.147815704345703\n",
      "ToTal Epoch: 1594, LR: 0.000002, Loss: 212210.3923566, Val MAE: 2985.6827467, Test MAE: 2017.4051895, Time: 11.207357168197632\n",
      "ToTal Epoch: 1595, LR: 0.000002, Loss: 209803.6430899, Val MAE: 3429.4104802, Test MAE: 2017.4051895, Time: 11.708343744277954\n",
      "ToTal Epoch: 1596, LR: 0.000002, Loss: 209761.6067644, Val MAE: 2428.3188537, Test MAE: 2017.4051895, Time: 11.386618375778198\n",
      "ToTal Epoch: 1597, LR: 0.000002, Loss: 211613.1613433, Val MAE: 3141.2652201, Test MAE: 2017.4051895, Time: 11.378536939620972\n",
      "ToTal Epoch: 1598, LR: 0.000002, Loss: 210446.1936655, Val MAE: 2473.1843662, Test MAE: 2017.4051895, Time: 12.35093069076538\n",
      "ToTal Epoch: 1599, LR: 0.000002, Loss: 208481.1618402, Val MAE: 3200.6570932, Test MAE: 2017.4051895, Time: 11.990681648254395\n",
      "ToTal Epoch: 1600, LR: 0.000002, Loss: 209641.7763722, Val MAE: 3504.4407389, Test MAE: 2017.4051895, Time: 11.144516944885254\n",
      "ToTal Epoch: 1601, LR: 0.000002, Loss: 208329.9752162, Val MAE: 2880.7424186, Test MAE: 2017.4051895, Time: 11.005594730377197\n",
      "ToTal Epoch: 1602, LR: 0.000002, Loss: 208499.9859361, Val MAE: 2581.7775166, Test MAE: 2017.4051895, Time: 12.469291925430298\n",
      "ToTal Epoch: 1603, LR: 0.000002, Loss: 210447.5870253, Val MAE: 2384.3807971, Test MAE: 2017.4051895, Time: 11.393958806991577\n",
      "ToTal Epoch: 1604, LR: 0.000002, Loss: 210497.1922228, Val MAE: 2851.8718662, Test MAE: 2017.4051895, Time: 11.48176646232605\n",
      "ToTal Epoch: 1605, LR: 0.000002, Loss: 209818.7671906, Val MAE: 3068.3495853, Test MAE: 2017.4051895, Time: 11.49552059173584\n",
      "ToTal Epoch: 1606, LR: 0.000002, Loss: 209164.3112024, Val MAE: 3124.1642590, Test MAE: 2017.4051895, Time: 11.739186763763428\n",
      "ToTal Epoch: 1607, LR: 0.000002, Loss: 209295.5200688, Val MAE: 2534.1843279, Test MAE: 2017.4051895, Time: 11.250536680221558\n",
      "ToTal Epoch: 1608, LR: 0.000002, Loss: 208770.4720394, Val MAE: 2983.6274364, Test MAE: 2017.4051895, Time: 11.463834762573242\n",
      "ToTal Epoch: 1609, LR: 0.000002, Loss: 208861.1284384, Val MAE: 3734.7421845, Test MAE: 2017.4051895, Time: 10.791760683059692\n",
      "ToTal Epoch: 1610, LR: 0.000002, Loss: 210757.1852673, Val MAE: 2651.8642418, Test MAE: 2017.4051895, Time: 11.068079471588135\n",
      "ToTal Epoch: 1611, LR: 0.000002, Loss: 209172.4561219, Val MAE: 2638.1449591, Test MAE: 2017.4051895, Time: 11.443963289260864\n",
      "ToTal Epoch: 1612, LR: 0.000002, Loss: 209507.8011179, Val MAE: 2634.4424062, Test MAE: 2017.4051895, Time: 11.077021837234497\n",
      "ToTal Epoch: 1613, LR: 0.000002, Loss: 209236.9280657, Val MAE: 2854.4400749, Test MAE: 2017.4051895, Time: 11.120763540267944\n",
      "ToTal Epoch: 1614, LR: 0.000002, Loss: 210772.6628386, Val MAE: 2807.8353111, Test MAE: 2017.4051895, Time: 11.469780206680298\n",
      "ToTal Epoch: 1615, LR: 0.000002, Loss: 210746.6102709, Val MAE: 3288.9752733, Test MAE: 2017.4051895, Time: 11.123494625091553\n",
      "ToTal Epoch: 1616, LR: 0.000002, Loss: 209930.0919314, Val MAE: 2879.1861863, Test MAE: 2017.4051895, Time: 11.007917642593384\n",
      "ToTal Epoch: 1617, LR: 0.000001, Loss: 209729.3713467, Val MAE: 3018.5448483, Test MAE: 2017.4051895, Time: 11.347851037979126\n",
      "ToTal Epoch: 1618, LR: 0.000001, Loss: 210040.7085081, Val MAE: 2815.8287711, Test MAE: 2017.4051895, Time: 12.64078164100647\n",
      "ToTal Epoch: 1619, LR: 0.000001, Loss: 211096.5626332, Val MAE: 3691.5227012, Test MAE: 2017.4051895, Time: 12.252063035964966\n",
      "ToTal Epoch: 1620, LR: 0.000001, Loss: 210579.8414752, Val MAE: 2698.0725082, Test MAE: 2017.4051895, Time: 11.594989776611328\n",
      "ToTal Epoch: 1621, LR: 0.000001, Loss: 211149.6546123, Val MAE: 3478.7773112, Test MAE: 2017.4051895, Time: 11.46481442451477\n",
      "ToTal Epoch: 1622, LR: 0.000001, Loss: 210210.0437395, Val MAE: 3028.1525023, Test MAE: 2017.4051895, Time: 10.869826078414917\n",
      "ToTal Epoch: 1623, LR: 0.000001, Loss: 209495.6378541, Val MAE: 3110.5618312, Test MAE: 2017.4051895, Time: 10.878600597381592\n",
      "ToTal Epoch: 1624, LR: 0.000001, Loss: 210123.0073473, Val MAE: 3224.6952343, Test MAE: 2017.4051895, Time: 10.72760796546936\n",
      "ToTal Epoch: 1625, LR: 0.000001, Loss: 208619.1667893, Val MAE: 3081.6147004, Test MAE: 2017.4051895, Time: 11.994572639465332\n",
      "ToTal Epoch: 1626, LR: 0.000001, Loss: 209414.7052023, Val MAE: 3098.4003382, Test MAE: 2017.4051895, Time: 11.492326498031616\n",
      "ToTal Epoch: 1627, LR: 0.000001, Loss: 210978.7391009, Val MAE: 2890.9491516, Test MAE: 2017.4051895, Time: 11.371135950088501\n",
      "ToTal Epoch: 1628, LR: 0.000001, Loss: 208636.9860794, Val MAE: 3151.1025902, Test MAE: 2017.4051895, Time: 11.108323335647583\n",
      "ToTal Epoch: 1629, LR: 0.000001, Loss: 209996.4272297, Val MAE: 3129.9493761, Test MAE: 2017.4051895, Time: 11.09677243232727\n",
      "ToTal Epoch: 1630, LR: 0.000001, Loss: 211116.9424354, Val MAE: 3034.6241783, Test MAE: 2017.4051895, Time: 10.55796217918396\n",
      "ToTal Epoch: 1631, LR: 0.000001, Loss: 210328.9350977, Val MAE: 2684.9916829, Test MAE: 2017.4051895, Time: 11.236164569854736\n",
      "ToTal Epoch: 1632, LR: 0.000001, Loss: 209791.4146658, Val MAE: 2493.1231799, Test MAE: 2017.4051895, Time: 11.903804302215576\n",
      "ToTal Epoch: 1633, LR: 0.000001, Loss: 208274.9742894, Val MAE: 2982.0809161, Test MAE: 2017.4051895, Time: 11.26776647567749\n",
      "ToTal Epoch: 1634, LR: 0.000001, Loss: 209861.3476520, Val MAE: 3307.5491573, Test MAE: 2017.4051895, Time: 10.39728832244873\n",
      "ToTal Epoch: 1635, LR: 0.000001, Loss: 210196.4103377, Val MAE: 2722.7873338, Test MAE: 2017.4051895, Time: 11.291409015655518\n",
      "ToTal Epoch: 1636, LR: 0.000001, Loss: 209862.1058424, Val MAE: 2913.0924530, Test MAE: 2017.4051895, Time: 10.74527645111084\n",
      "ToTal Epoch: 1637, LR: 0.000001, Loss: 210340.0754407, Val MAE: 3116.7008188, Test MAE: 2017.4051895, Time: 12.119389772415161\n",
      "ToTal Epoch: 1638, LR: 0.000001, Loss: 209742.0818421, Val MAE: 3266.8985946, Test MAE: 2017.4051895, Time: 10.79895830154419\n",
      "ToTal Epoch: 1639, LR: 0.000001, Loss: 210620.9196197, Val MAE: 3078.5591799, Test MAE: 2017.4051895, Time: 11.047640085220337\n",
      "ToTal Epoch: 1640, LR: 0.000001, Loss: 210558.2906607, Val MAE: 2858.1584212, Test MAE: 2017.4051895, Time: 11.096837282180786\n",
      "ToTal Epoch: 1641, LR: 0.000001, Loss: 210661.9833182, Val MAE: 3279.5660112, Test MAE: 2017.4051895, Time: 11.151117324829102\n",
      "ToTal Epoch: 1642, LR: 0.000001, Loss: 209340.7960636, Val MAE: 2199.0312381, Test MAE: 2017.4051895, Time: 11.874337434768677\n",
      "ToTal Epoch: 1643, LR: 0.000001, Loss: 208176.4687336, Val MAE: 3209.7128382, Test MAE: 2017.4051895, Time: 11.52056884765625\n",
      "ToTal Epoch: 1644, LR: 0.000001, Loss: 209789.0629341, Val MAE: 3198.8629280, Test MAE: 2017.4051895, Time: 13.065024614334106\n",
      "ToTal Epoch: 1645, LR: 0.000001, Loss: 210505.7434290, Val MAE: 3390.8453910, Test MAE: 2017.4051895, Time: 12.532003402709961\n",
      "ToTal Epoch: 1646, LR: 0.000001, Loss: 211072.5294989, Val MAE: 3153.9232640, Test MAE: 2017.4051895, Time: 11.81948184967041\n",
      "ToTal Epoch: 1647, LR: 0.000001, Loss: 209760.5046195, Val MAE: 2832.4799692, Test MAE: 2017.4051895, Time: 12.49732518196106\n",
      "ToTal Epoch: 1648, LR: 0.000001, Loss: 210243.6405293, Val MAE: 2810.7168128, Test MAE: 2017.4051895, Time: 13.007895231246948\n",
      "ToTal Epoch: 1649, LR: 0.000001, Loss: 209991.0912339, Val MAE: 2835.4990350, Test MAE: 2017.4051895, Time: 14.400612115859985\n",
      "ToTal Epoch: 1650, LR: 0.000001, Loss: 209273.5651842, Val MAE: 2931.0364309, Test MAE: 2017.4051895, Time: 12.909126043319702\n",
      "ToTal Epoch: 1651, LR: 0.000001, Loss: 210127.4055319, Val MAE: 2910.2337671, Test MAE: 2017.4051895, Time: 12.442751169204712\n",
      "ToTal Epoch: 1652, LR: 0.000001, Loss: 210192.7300244, Val MAE: 2650.6038896, Test MAE: 2017.4051895, Time: 12.6268470287323\n",
      "ToTal Epoch: 1653, LR: 0.000001, Loss: 209481.5761143, Val MAE: 3040.4067922, Test MAE: 2017.4051895, Time: 11.733985662460327\n",
      "ToTal Epoch: 1654, LR: 0.000001, Loss: 210664.0796828, Val MAE: 2600.7687218, Test MAE: 2017.4051895, Time: 11.839021682739258\n",
      "ToTal Epoch: 1655, LR: 0.000001, Loss: 208225.7520661, Val MAE: 3203.1709852, Test MAE: 2017.4051895, Time: 12.154060125350952\n",
      "ToTal Epoch: 1656, LR: 0.000001, Loss: 209204.6528639, Val MAE: 2971.3464324, Test MAE: 2017.4051895, Time: 11.549874305725098\n",
      "ToTal Epoch: 1657, LR: 0.000001, Loss: 208128.6320738, Val MAE: 2788.5379261, Test MAE: 2017.4051895, Time: 12.926154375076294\n",
      "ToTal Epoch: 1658, LR: 0.000001, Loss: 211131.4634692, Val MAE: 3159.1182212, Test MAE: 2017.4051895, Time: 10.780715942382812\n",
      "ToTal Epoch: 1659, LR: 0.000001, Loss: 209534.6162327, Val MAE: 2851.8876452, Test MAE: 2017.4051895, Time: 11.174275636672974\n",
      "ToTal Epoch: 1660, LR: 0.000001, Loss: 209645.4061625, Val MAE: 3108.8445263, Test MAE: 2017.4051895, Time: 13.044314622879028\n",
      "ToTal Epoch: 1661, LR: 0.000001, Loss: 211316.3856112, Val MAE: 3507.1471518, Test MAE: 2017.4051895, Time: 10.778926610946655\n",
      "ToTal Epoch: 1662, LR: 0.000001, Loss: 209866.5801557, Val MAE: 2620.3122229, Test MAE: 2017.4051895, Time: 10.937369585037231\n",
      "ToTal Epoch: 1663, LR: 0.000001, Loss: 208855.8897817, Val MAE: 2795.8934017, Test MAE: 2017.4051895, Time: 10.998104095458984\n",
      "ToTal Epoch: 1664, LR: 0.000001, Loss: 208513.3392825, Val MAE: 2456.2801202, Test MAE: 2017.4051895, Time: 11.079507827758789\n",
      "ToTal Epoch: 1665, LR: 0.000001, Loss: 210830.1525438, Val MAE: 2060.6790898, Test MAE: 2017.4051895, Time: 10.989034175872803\n",
      "ToTal Epoch: 1666, LR: 0.000001, Loss: 208773.4678450, Val MAE: 2554.6519577, Test MAE: 2017.4051895, Time: 11.420050859451294\n",
      "ToTal Epoch: 1667, LR: 0.000001, Loss: 209651.6854727, Val MAE: 2542.6423794, Test MAE: 2017.4051895, Time: 11.324358940124512\n",
      "ToTal Epoch: 1668, LR: 0.000001, Loss: 211413.7882196, Val MAE: 3174.7956078, Test MAE: 2017.4051895, Time: 10.395867347717285\n",
      "ToTal Epoch: 1669, LR: 0.000001, Loss: 210541.5939999, Val MAE: 3083.2863210, Test MAE: 2017.4051895, Time: 11.560349464416504\n",
      "ToTal Epoch: 1670, LR: 0.000001, Loss: 210299.2164334, Val MAE: 3016.5634124, Test MAE: 2017.4051895, Time: 11.352296352386475\n",
      "ToTal Epoch: 1671, LR: 0.000001, Loss: 209274.4150958, Val MAE: 2667.2877446, Test MAE: 2017.4051895, Time: 12.031347274780273\n",
      "ToTal Epoch: 1672, LR: 0.000001, Loss: 209975.4732910, Val MAE: 2971.8281214, Test MAE: 2017.4051895, Time: 12.717130661010742\n",
      "ToTal Epoch: 1673, LR: 0.000001, Loss: 210515.6473320, Val MAE: 3028.1643927, Test MAE: 2017.4051895, Time: 10.77827525138855\n",
      "ToTal Epoch: 1674, LR: 0.000001, Loss: 211364.3577509, Val MAE: 2843.2238688, Test MAE: 2017.4051895, Time: 11.409770011901855\n",
      "ToTal Epoch: 1675, LR: 0.000001, Loss: 210320.7061386, Val MAE: 2726.9337212, Test MAE: 2017.4051895, Time: 12.775076866149902\n",
      "ToTal Epoch: 1676, LR: 0.000001, Loss: 209525.6664597, Val MAE: 2672.8104315, Test MAE: 2017.4051895, Time: 11.028808116912842\n",
      "ToTal Epoch: 1677, LR: 0.000001, Loss: 210123.3341040, Val MAE: 3388.9167574, Test MAE: 2017.4051895, Time: 11.969183921813965\n",
      "ToTal Epoch: 1678, LR: 0.000001, Loss: 210295.2512492, Val MAE: 3129.6128134, Test MAE: 2017.4051895, Time: 11.54826045036316\n",
      "ToTal Epoch: 1679, LR: 0.000001, Loss: 209538.6733674, Val MAE: 2871.7777412, Test MAE: 2017.4051895, Time: 11.020514249801636\n",
      "ToTal Epoch: 1680, LR: 0.000001, Loss: 210215.0045192, Val MAE: 2525.3617409, Test MAE: 2017.4051895, Time: 10.807027816772461\n",
      "ToTal Epoch: 1681, LR: 0.000001, Loss: 209188.8691731, Val MAE: 3042.3295880, Test MAE: 2017.4051895, Time: 11.650943756103516\n",
      "ToTal Epoch: 1682, LR: 0.000001, Loss: 211088.5794487, Val MAE: 2697.5463292, Test MAE: 2017.4051895, Time: 11.883335828781128\n",
      "ToTal Epoch: 1683, LR: 0.000001, Loss: 210182.1523527, Val MAE: 3871.1599690, Test MAE: 2017.4051895, Time: 10.68887209892273\n",
      "ToTal Epoch: 1684, LR: 0.000001, Loss: 208627.6248603, Val MAE: 3481.6453891, Test MAE: 2017.4051895, Time: 11.661051750183105\n",
      "ToTal Epoch: 1685, LR: 0.000001, Loss: 209586.4545359, Val MAE: 3089.5679890, Test MAE: 2017.4051895, Time: 11.923835754394531\n",
      "ToTal Epoch: 1686, LR: 0.000001, Loss: 209579.3973917, Val MAE: 3095.8193983, Test MAE: 2017.4051895, Time: 13.195245265960693\n",
      "ToTal Epoch: 1687, LR: 0.000001, Loss: 210105.5369035, Val MAE: 2498.7981159, Test MAE: 2017.4051895, Time: 10.87619400024414\n",
      "ToTal Epoch: 1688, LR: 0.000001, Loss: 210517.4741126, Val MAE: 2818.5768316, Test MAE: 2017.4051895, Time: 11.851790189743042\n",
      "ToTal Epoch: 1689, LR: 0.000001, Loss: 209684.9621554, Val MAE: 3197.3861404, Test MAE: 2017.4051895, Time: 12.369186878204346\n",
      "ToTal Epoch: 1690, LR: 0.000001, Loss: 210708.4508862, Val MAE: 3064.9294743, Test MAE: 2017.4051895, Time: 12.654055833816528\n",
      "ToTal Epoch: 1691, LR: 0.000001, Loss: 209843.6032676, Val MAE: 2965.8975292, Test MAE: 2017.4051895, Time: 11.372877597808838\n",
      "ToTal Epoch: 1692, LR: 0.000001, Loss: 209477.3801366, Val MAE: 2997.0367223, Test MAE: 2017.4051895, Time: 10.862624168395996\n",
      "ToTal Epoch: 1693, LR: 0.000001, Loss: 211122.4992118, Val MAE: 2553.3984847, Test MAE: 2017.4051895, Time: 12.465513229370117\n",
      "ToTal Epoch: 1694, LR: 0.000001, Loss: 209034.1648115, Val MAE: 3182.9424922, Test MAE: 2017.4051895, Time: 11.768086671829224\n",
      "ToTal Epoch: 1695, LR: 0.000001, Loss: 210686.7556108, Val MAE: 2705.1802578, Test MAE: 2017.4051895, Time: 10.405423402786255\n",
      "ToTal Epoch: 1696, LR: 0.000001, Loss: 209813.9728467, Val MAE: 3133.0358624, Test MAE: 2017.4051895, Time: 12.3734450340271\n",
      "ToTal Epoch: 1697, LR: 0.000001, Loss: 209750.7853437, Val MAE: 3120.9959967, Test MAE: 2017.4051895, Time: 11.0947265625\n",
      "ToTal Epoch: 1698, LR: 0.000001, Loss: 209122.5098744, Val MAE: 3195.1005264, Test MAE: 2017.4051895, Time: 11.394206762313843\n",
      "ToTal Epoch: 1699, LR: 0.000001, Loss: 208990.6844122, Val MAE: 2830.7758876, Test MAE: 2017.4051895, Time: 11.6566162109375\n",
      "ToTal Epoch: 1700, LR: 0.000001, Loss: 209419.3023074, Val MAE: 3217.3749475, Test MAE: 2017.4051895, Time: 10.89030385017395\n",
      "ToTal Epoch: 1701, LR: 0.000001, Loss: 210794.9856017, Val MAE: 2565.2157810, Test MAE: 2017.4051895, Time: 11.285237550735474\n",
      "ToTal Epoch: 1702, LR: 0.000001, Loss: 210847.3691492, Val MAE: 2286.1423364, Test MAE: 2017.4051895, Time: 10.378404378890991\n",
      "ToTal Epoch: 1703, LR: 0.000001, Loss: 209929.7277218, Val MAE: 2744.7108796, Test MAE: 2017.4051895, Time: 11.507951021194458\n",
      "ToTal Epoch: 1704, LR: 0.000001, Loss: 208761.6511346, Val MAE: 2769.9780201, Test MAE: 2017.4051895, Time: 11.597396850585938\n",
      "ToTal Epoch: 1705, LR: 0.000001, Loss: 209333.1580184, Val MAE: 2854.2434505, Test MAE: 2017.4051895, Time: 11.459916591644287\n",
      "ToTal Epoch: 1706, LR: 0.000001, Loss: 207670.2250418, Val MAE: 3239.1326435, Test MAE: 2017.4051895, Time: 11.086519241333008\n",
      "ToTal Epoch: 1707, LR: 0.000001, Loss: 209200.1603592, Val MAE: 3334.4225952, Test MAE: 2017.4051895, Time: 10.573205947875977\n",
      "ToTal Epoch: 1708, LR: 0.000001, Loss: 210081.9004252, Val MAE: 3372.1272548, Test MAE: 2017.4051895, Time: 10.538337230682373\n",
      "ToTal Epoch: 1709, LR: 0.000001, Loss: 212093.7895954, Val MAE: 2988.2637726, Test MAE: 2017.4051895, Time: 11.711610555648804\n",
      "ToTal Epoch: 1710, LR: 0.000001, Loss: 209034.7160942, Val MAE: 3348.2961238, Test MAE: 2017.4051895, Time: 11.411113500595093\n",
      "ToTal Epoch: 1711, LR: 0.000001, Loss: 209160.2241437, Val MAE: 2929.1472569, Test MAE: 2017.4051895, Time: 10.916148662567139\n",
      "ToTal Epoch: 1712, LR: 0.000001, Loss: 210193.2024650, Val MAE: 3421.9347579, Test MAE: 2017.4051895, Time: 10.850461959838867\n",
      "ToTal Epoch: 1713, LR: 0.000001, Loss: 210440.0376057, Val MAE: 2796.0609713, Test MAE: 2017.4051895, Time: 11.055819272994995\n",
      "ToTal Epoch: 1714, LR: 0.000001, Loss: 210393.7598242, Val MAE: 2927.6149058, Test MAE: 2017.4051895, Time: 10.828325510025024\n",
      "ToTal Epoch: 1715, LR: 0.000001, Loss: 211712.5612573, Val MAE: 2597.4999092, Test MAE: 2017.4051895, Time: 11.245672702789307\n",
      "ToTal Epoch: 1716, LR: 0.000001, Loss: 210166.6256724, Val MAE: 3080.6520867, Test MAE: 2017.4051895, Time: 11.550009489059448\n",
      "ToTal Epoch: 1717, LR: 0.000001, Loss: 209548.6159843, Val MAE: 2619.3465470, Test MAE: 2017.4051895, Time: 12.125423192977905\n",
      "ToTal Epoch: 1718, LR: 0.000001, Loss: 210343.1019347, Val MAE: 3380.0028424, Test MAE: 2017.4051895, Time: 10.8611319065094\n",
      "ToTal Epoch: 1719, LR: 0.000001, Loss: 209631.1868915, Val MAE: 3212.6753134, Test MAE: 2017.4051895, Time: 11.407413005828857\n",
      "ToTal Epoch: 1720, LR: 0.000001, Loss: 210990.1443654, Val MAE: 3036.9336305, Test MAE: 2017.4051895, Time: 10.751623630523682\n",
      "ToTal Epoch: 1721, LR: 0.000001, Loss: 210557.1092533, Val MAE: 3487.7004366, Test MAE: 2017.4051895, Time: 10.897423028945923\n",
      "ToTal Epoch: 1722, LR: 0.000001, Loss: 210240.4989251, Val MAE: 3252.6290606, Test MAE: 2017.4051895, Time: 13.244125127792358\n",
      "ToTal Epoch: 1723, LR: 0.000001, Loss: 211107.5737257, Val MAE: 3451.2291380, Test MAE: 2017.4051895, Time: 10.944031476974487\n",
      "ToTal Epoch: 1724, LR: 0.000001, Loss: 210367.2216309, Val MAE: 2761.4558970, Test MAE: 2017.4051895, Time: 11.157602548599243\n",
      "ToTal Epoch: 1725, LR: 0.000001, Loss: 210211.8559977, Val MAE: 3173.6543941, Test MAE: 2017.4051895, Time: 12.382725477218628\n",
      "ToTal Epoch: 1726, LR: 0.000001, Loss: 210454.1164668, Val MAE: 2939.3498003, Test MAE: 2017.4051895, Time: 11.551372051239014\n",
      "ToTal Epoch: 1727, LR: 0.000001, Loss: 209527.0252711, Val MAE: 3135.7516911, Test MAE: 2017.4051895, Time: 10.879595518112183\n",
      "ToTal Epoch: 1728, LR: 0.000001, Loss: 209924.0486505, Val MAE: 3038.9731904, Test MAE: 2017.4051895, Time: 11.938924074172974\n",
      "ToTal Epoch: 1729, LR: 0.000001, Loss: 210001.6404338, Val MAE: 3116.2389265, Test MAE: 2017.4051895, Time: 11.808249711990356\n",
      "ToTal Epoch: 1730, LR: 0.000001, Loss: 208425.2668992, Val MAE: 2957.9971241, Test MAE: 2017.4051895, Time: 11.017462968826294\n",
      "ToTal Epoch: 1731, LR: 0.000001, Loss: 208853.2823007, Val MAE: 3157.5596623, Test MAE: 2017.4051895, Time: 12.30536961555481\n",
      "ToTal Epoch: 1732, LR: 0.000001, Loss: 210256.3588210, Val MAE: 2772.3590346, Test MAE: 2017.4051895, Time: 11.09102988243103\n",
      "ToTal Epoch: 1733, LR: 0.000001, Loss: 210273.2135480, Val MAE: 3139.8373844, Test MAE: 2017.4051895, Time: 11.62893033027649\n",
      "ToTal Epoch: 1734, LR: 0.000001, Loss: 209180.1595567, Val MAE: 2851.6654103, Test MAE: 2017.4051895, Time: 12.370736360549927\n",
      "ToTal Epoch: 1735, LR: 0.000001, Loss: 209981.1537381, Val MAE: 3054.4598764, Test MAE: 2017.4051895, Time: 10.686857461929321\n",
      "ToTal Epoch: 1736, LR: 0.000001, Loss: 209896.5328620, Val MAE: 3090.0490044, Test MAE: 2017.4051895, Time: 11.682282209396362\n",
      "ToTal Epoch: 1737, LR: 0.000001, Loss: 210515.2304591, Val MAE: 3103.4303677, Test MAE: 2017.4051895, Time: 10.47174072265625\n",
      "ToTal Epoch: 1738, LR: 0.000001, Loss: 209266.3098075, Val MAE: 2899.0049444, Test MAE: 2017.4051895, Time: 11.764323234558105\n",
      "ToTal Epoch: 1739, LR: 0.000001, Loss: 209297.3125305, Val MAE: 3108.8528291, Test MAE: 2017.4051895, Time: 10.715563774108887\n",
      "ToTal Epoch: 1740, LR: 0.000001, Loss: 210459.9546744, Val MAE: 2898.9191747, Test MAE: 2017.4051895, Time: 11.162450075149536\n",
      "ToTal Epoch: 1741, LR: 0.000001, Loss: 209640.8670329, Val MAE: 2902.6187801, Test MAE: 2017.4051895, Time: 10.766499757766724\n",
      "ToTal Epoch: 1742, LR: 0.000001, Loss: 209085.7581426, Val MAE: 2756.8396440, Test MAE: 2017.4051895, Time: 11.078697919845581\n",
      "ToTal Epoch: 1743, LR: 0.000001, Loss: 209742.2444943, Val MAE: 2933.4131507, Test MAE: 2017.4051895, Time: 11.09871244430542\n",
      "ToTal Epoch: 1744, LR: 0.000001, Loss: 210589.6447523, Val MAE: 2900.3469531, Test MAE: 2017.4051895, Time: 10.60667610168457\n",
      "ToTal Epoch: 1745, LR: 0.000001, Loss: 210291.2399369, Val MAE: 3023.1917804, Test MAE: 2017.4051895, Time: 11.215538501739502\n",
      "ToTal Epoch: 1746, LR: 0.000001, Loss: 208530.4540391, Val MAE: 2979.0588884, Test MAE: 2017.4051895, Time: 10.597429275512695\n",
      "ToTal Epoch: 1747, LR: 0.000001, Loss: 209457.3667606, Val MAE: 3190.9515688, Test MAE: 2017.4051895, Time: 11.324897289276123\n",
      "ToTal Epoch: 1748, LR: 0.000001, Loss: 208945.5164191, Val MAE: 2980.0035447, Test MAE: 2017.4051895, Time: 11.816035270690918\n",
      "ToTal Epoch: 1749, LR: 0.000001, Loss: 209597.8096212, Val MAE: 3646.3192263, Test MAE: 2017.4051895, Time: 10.951476812362671\n",
      "ToTal Epoch: 1750, LR: 0.000001, Loss: 210745.0482205, Val MAE: 3313.1406214, Test MAE: 2017.4051895, Time: 11.234574794769287\n",
      "ToTal Epoch: 1751, LR: 0.000001, Loss: 210717.9296995, Val MAE: 2905.3153854, Test MAE: 2017.4051895, Time: 10.885157823562622\n",
      "ToTal Epoch: 1752, LR: 0.000001, Loss: 209962.1588879, Val MAE: 3064.5831327, Test MAE: 2017.4051895, Time: 10.996121883392334\n",
      "ToTal Epoch: 1753, LR: 0.000001, Loss: 209619.4304113, Val MAE: 3178.5626959, Test MAE: 2017.4051895, Time: 10.076987981796265\n",
      "ToTal Epoch: 1754, LR: 0.000001, Loss: 210779.4146658, Val MAE: 2733.6633513, Test MAE: 2017.4051895, Time: 10.248902559280396\n",
      "ToTal Epoch: 1755, LR: 0.000001, Loss: 210208.1683084, Val MAE: 2456.7143956, Test MAE: 2017.4051895, Time: 10.9818274974823\n",
      "ToTal Epoch: 1756, LR: 0.000001, Loss: 210168.7096546, Val MAE: 3050.3469388, Test MAE: 2017.4051895, Time: 11.544477224349976\n",
      "ToTal Epoch: 1757, LR: 0.000001, Loss: 210221.4389911, Val MAE: 3122.0928495, Test MAE: 2017.4051895, Time: 10.554608821868896\n",
      "ToTal Epoch: 1758, LR: 0.000001, Loss: 208656.6848517, Val MAE: 3317.6708801, Test MAE: 2017.4051895, Time: 10.744986295700073\n",
      "ToTal Epoch: 1759, LR: 0.000001, Loss: 210393.0202838, Val MAE: 2917.5710703, Test MAE: 2017.4051895, Time: 11.310842752456665\n",
      "ToTal Epoch: 1760, LR: 0.000001, Loss: 211679.6323891, Val MAE: 3081.6343585, Test MAE: 2017.4051895, Time: 11.693894863128662\n",
      "ToTal Epoch: 1761, LR: 0.000001, Loss: 209790.3537954, Val MAE: 3113.4953184, Test MAE: 2017.4051895, Time: 11.619751214981079\n",
      "ToTal Epoch: 1762, LR: 0.000001, Loss: 211290.4903072, Val MAE: 2970.2001834, Test MAE: 2017.4051895, Time: 12.135418891906738\n",
      "ToTal Epoch: 1763, LR: 0.000001, Loss: 209487.6557015, Val MAE: 2691.9316336, Test MAE: 2017.4051895, Time: 11.07379937171936\n",
      "ToTal Epoch: 1764, LR: 0.000001, Loss: 211959.8808006, Val MAE: 2713.8757930, Test MAE: 2017.4051895, Time: 11.18128228187561\n",
      "ToTal Epoch: 1765, LR: 0.000001, Loss: 210888.9034921, Val MAE: 2665.9124580, Test MAE: 2017.4051895, Time: 11.110050201416016\n",
      "ToTal Epoch: 1766, LR: 0.000001, Loss: 209649.9812163, Val MAE: 2715.2488583, Test MAE: 2017.4051895, Time: 10.980295658111572\n",
      "ToTal Epoch: 1767, LR: 0.000001, Loss: 208892.9950222, Val MAE: 3157.7791218, Test MAE: 2017.4051895, Time: 11.03538179397583\n",
      "ToTal Epoch: 1768, LR: 0.000001, Loss: 211344.8179621, Val MAE: 2947.8000076, Test MAE: 2017.4051895, Time: 10.975666999816895\n",
      "ToTal Epoch: 1769, LR: 0.000001, Loss: 210369.7576840, Val MAE: 2974.8614041, Test MAE: 2017.4051895, Time: 10.932180643081665\n",
      "ToTal Epoch: 1770, LR: 0.000001, Loss: 209485.4981512, Val MAE: 2929.3916007, Test MAE: 2017.4051895, Time: 11.840794324874878\n",
      "ToTal Epoch: 1771, LR: 0.000001, Loss: 210524.5770028, Val MAE: 2863.8822231, Test MAE: 2017.4051895, Time: 11.266225814819336\n",
      "ToTal Epoch: 1772, LR: 0.000001, Loss: 211659.6146563, Val MAE: 3007.9708782, Test MAE: 2017.4051895, Time: 10.806395292282104\n",
      "ToTal Epoch: 1773, LR: 0.000001, Loss: 210399.9338461, Val MAE: 2724.0419199, Test MAE: 2017.4051895, Time: 10.832508325576782\n",
      "ToTal Epoch: 1774, LR: 0.000001, Loss: 209980.6019204, Val MAE: 3227.2060259, Test MAE: 2017.4051895, Time: 10.6721830368042\n",
      "ToTal Epoch: 1775, LR: 0.000001, Loss: 211370.2440740, Val MAE: 3417.2665482, Test MAE: 2017.4051895, Time: 10.933269262313843\n",
      "ToTal Epoch: 1776, LR: 0.000001, Loss: 209544.6353222, Val MAE: 3035.0810355, Test MAE: 2017.4051895, Time: 11.754586219787598\n",
      "ToTal Epoch: 1777, LR: 0.000001, Loss: 208367.6077390, Val MAE: 3037.1386532, Test MAE: 2017.4051895, Time: 11.50679874420166\n",
      "ToTal Epoch: 1778, LR: 0.000001, Loss: 210318.4236182, Val MAE: 3254.9055645, Test MAE: 2017.4051895, Time: 12.058403968811035\n",
      "ToTal Epoch: 1779, LR: 0.000001, Loss: 209804.7954522, Val MAE: 3105.0899975, Test MAE: 2017.4051895, Time: 11.039032220840454\n",
      "ToTal Epoch: 1780, LR: 0.000001, Loss: 210689.1727703, Val MAE: 2739.0091292, Test MAE: 2017.4051895, Time: 11.910706281661987\n",
      "ToTal Epoch: 1781, LR: 0.000001, Loss: 209992.4244399, Val MAE: 3106.8973190, Test MAE: 2017.4051895, Time: 12.57702088356018\n",
      "ToTal Epoch: 1782, LR: 0.000001, Loss: 209094.6567812, Val MAE: 3102.8572671, Test MAE: 2017.4051895, Time: 11.013986587524414\n",
      "ToTal Epoch: 1783, LR: 0.000001, Loss: 209018.9368748, Val MAE: 3269.8908221, Test MAE: 2017.4051895, Time: 10.715409994125366\n",
      "ToTal Epoch: 1784, LR: 0.000001, Loss: 211358.0325419, Val MAE: 3122.5309610, Test MAE: 2017.4051895, Time: 11.24682903289795\n",
      "ToTal Epoch: 1785, LR: 0.000001, Loss: 210504.5368748, Val MAE: 3063.8339974, Test MAE: 2017.4051895, Time: 11.661144256591797\n",
      "ToTal Epoch: 1786, LR: 0.000001, Loss: 211231.9894903, Val MAE: 3527.2055148, Test MAE: 2017.4051895, Time: 11.110947132110596\n",
      "ToTal Epoch: 1787, LR: 0.000001, Loss: 209929.2096498, Val MAE: 3185.3503879, Test MAE: 2017.4051895, Time: 10.916489601135254\n",
      "ToTal Epoch: 1788, LR: 0.000001, Loss: 209169.1858405, Val MAE: 2713.6281768, Test MAE: 2017.4051895, Time: 11.163673877716064\n",
      "ToTal Epoch: 1789, LR: 0.000001, Loss: 211974.5295944, Val MAE: 3398.2659128, Test MAE: 2017.4051895, Time: 10.69716763496399\n",
      "ToTal Epoch: 1790, LR: 0.000001, Loss: 209109.9367315, Val MAE: 3085.6929078, Test MAE: 2017.4051895, Time: 11.058629274368286\n",
      "ToTal Epoch: 1791, LR: 0.000001, Loss: 211457.3790665, Val MAE: 2924.4599862, Test MAE: 2017.4051895, Time: 11.622310876846313\n",
      "ToTal Epoch: 1792, LR: 0.000001, Loss: 210175.8494626, Val MAE: 3077.2520685, Test MAE: 2017.4051895, Time: 11.508840322494507\n",
      "ToTal Epoch: 1793, LR: 0.000001, Loss: 210667.8006975, Val MAE: 3064.2153510, Test MAE: 2017.4051895, Time: 11.260514259338379\n",
      "ToTal Epoch: 1794, LR: 0.000001, Loss: 211593.7123967, Val MAE: 2883.2048746, Test MAE: 2017.4051895, Time: 11.547730922698975\n",
      "ToTal Epoch: 1795, LR: 0.000001, Loss: 211078.5024602, Val MAE: 3296.3261437, Test MAE: 2017.4051895, Time: 12.78923773765564\n",
      "ToTal Epoch: 1796, LR: 0.000001, Loss: 211122.7765155, Val MAE: 3115.3760462, Test MAE: 2017.4051895, Time: 13.277710914611816\n",
      "ToTal Epoch: 1797, LR: 0.000001, Loss: 209828.1436583, Val MAE: 3304.3351200, Test MAE: 2017.4051895, Time: 12.618406772613525\n",
      "ToTal Epoch: 1798, LR: 0.000001, Loss: 210761.8243730, Val MAE: 2943.8977824, Test MAE: 2017.4051895, Time: 12.039440631866455\n",
      "ToTal Epoch: 1799, LR: 0.000001, Loss: 210058.1557159, Val MAE: 3060.1545660, Test MAE: 2017.4051895, Time: 10.394587278366089\n",
      "ToTal Epoch: 1800, LR: 0.000001, Loss: 209798.1483399, Val MAE: 3075.8765144, Test MAE: 2017.4051895, Time: 10.993279218673706\n",
      "ToTal Epoch: 1801, LR: 0.000001, Loss: 209732.2056466, Val MAE: 2884.8534596, Test MAE: 2017.4051895, Time: 10.571295499801636\n",
      "ToTal Epoch: 1802, LR: 0.000001, Loss: 210066.4301916, Val MAE: 2852.4744516, Test MAE: 2017.4051895, Time: 10.43724799156189\n",
      "ToTal Epoch: 1803, LR: 0.000001, Loss: 208999.6782497, Val MAE: 2739.5896631, Test MAE: 2017.4051895, Time: 10.898870468139648\n",
      "ToTal Epoch: 1804, LR: 0.000001, Loss: 209459.2368414, Val MAE: 2836.2439473, Test MAE: 2017.4051895, Time: 10.65203046798706\n",
      "ToTal Epoch: 1805, LR: 0.000001, Loss: 211388.5051928, Val MAE: 2961.5993083, Test MAE: 2017.4051895, Time: 12.731744527816772\n",
      "ToTal Epoch: 1806, LR: 0.000001, Loss: 210060.0247265, Val MAE: 2863.9636599, Test MAE: 2017.4051895, Time: 13.083266973495483\n",
      "ToTal Epoch: 1807, LR: 0.000001, Loss: 209974.3821144, Val MAE: 3014.2593681, Test MAE: 2017.4051895, Time: 10.883997201919556\n",
      "ToTal Epoch: 1808, LR: 0.000001, Loss: 210258.7055080, Val MAE: 2634.1877198, Test MAE: 2017.4051895, Time: 10.6123206615448\n",
      "ToTal Epoch: 1809, LR: 0.000001, Loss: 210175.2963837, Val MAE: 2972.3325642, Test MAE: 2017.4051895, Time: 11.70950984954834\n",
      "ToTal Epoch: 1810, LR: 0.000001, Loss: 210319.6826446, Val MAE: 3086.1437123, Test MAE: 2017.4051895, Time: 11.709462404251099\n",
      "ToTal Epoch: 1811, LR: 0.000001, Loss: 211833.3381360, Val MAE: 2929.2515144, Test MAE: 2017.4051895, Time: 10.695483207702637\n",
      "ToTal Epoch: 1812, LR: 0.000001, Loss: 210207.0700234, Val MAE: 2847.5440123, Test MAE: 2017.4051895, Time: 10.743999481201172\n",
      "ToTal Epoch: 1813, LR: 0.000001, Loss: 209427.3375436, Val MAE: 2797.1020934, Test MAE: 2017.4051895, Time: 10.442528486251831\n",
      "ToTal Epoch: 1814, LR: 0.000001, Loss: 211066.2626475, Val MAE: 2714.5277794, Test MAE: 2017.4051895, Time: 11.265328884124756\n",
      "ToTal Epoch: 1815, LR: 0.000001, Loss: 211753.4816032, Val MAE: 2677.5768985, Test MAE: 2017.4051895, Time: 11.627804279327393\n",
      "ToTal Epoch: 1816, LR: 0.000001, Loss: 209644.9254287, Val MAE: 3519.0963751, Test MAE: 2017.4051895, Time: 11.232136011123657\n",
      "ToTal Epoch: 1817, LR: 0.000001, Loss: 210388.6073855, Val MAE: 2916.6927310, Test MAE: 2017.4051895, Time: 11.596557378768921\n",
      "ToTal Epoch: 1818, LR: 0.000001, Loss: 209365.4759089, Val MAE: 2994.8956279, Test MAE: 2017.4051895, Time: 11.463871717453003\n",
      "ToTal Epoch: 1819, LR: 0.000001, Loss: 208871.1725219, Val MAE: 3372.3233203, Test MAE: 2017.4051895, Time: 11.806806325912476\n",
      "ToTal Epoch: 1820, LR: 0.000001, Loss: 210902.1299575, Val MAE: 2868.1157991, Test MAE: 2017.4051895, Time: 11.787869453430176\n",
      "ToTal Epoch: 1821, LR: 0.000000, Loss: 210429.7630344, Val MAE: 3004.9356512, Test MAE: 2017.4051895, Time: 10.585761308670044\n",
      "ToTal Epoch: 1822, LR: 0.000000, Loss: 209552.9182439, Val MAE: 3158.5751834, Test MAE: 2017.4051895, Time: 10.62532663345337\n",
      "ToTal Epoch: 1823, LR: 0.000000, Loss: 208934.5885635, Val MAE: 3092.7296109, Test MAE: 2017.4051895, Time: 10.80469799041748\n",
      "ToTal Epoch: 1824, LR: 0.000000, Loss: 209865.6546888, Val MAE: 3229.4161603, Test MAE: 2017.4051895, Time: 11.107927560806274\n",
      "ToTal Epoch: 1825, LR: 0.000000, Loss: 208974.6374051, Val MAE: 2930.6844283, Test MAE: 2017.4051895, Time: 11.08773422241211\n",
      "ToTal Epoch: 1826, LR: 0.000000, Loss: 210713.3545311, Val MAE: 2714.0128124, Test MAE: 2017.4051895, Time: 10.764243602752686\n",
      "ToTal Epoch: 1827, LR: 0.000000, Loss: 209126.2014618, Val MAE: 3305.3986901, Test MAE: 2017.4051895, Time: 11.102071046829224\n",
      "ToTal Epoch: 1828, LR: 0.000000, Loss: 211590.5404863, Val MAE: 2867.2293148, Test MAE: 2017.4051895, Time: 12.049999713897705\n",
      "ToTal Epoch: 1829, LR: 0.000000, Loss: 209981.4705967, Val MAE: 3374.4478140, Test MAE: 2017.4051895, Time: 11.494349241256714\n",
      "ToTal Epoch: 1830, LR: 0.000000, Loss: 210517.3771557, Val MAE: 3089.0656864, Test MAE: 2017.4051895, Time: 10.83750867843628\n",
      "ToTal Epoch: 1831, LR: 0.000000, Loss: 210707.2658100, Val MAE: 3414.5549711, Test MAE: 2017.4051895, Time: 10.548482894897461\n",
      "ToTal Epoch: 1832, LR: 0.000000, Loss: 210706.7718531, Val MAE: 2977.3509421, Test MAE: 2017.4051895, Time: 10.53952670097351\n",
      "ToTal Epoch: 1833, LR: 0.000000, Loss: 209057.4604309, Val MAE: 2869.8729649, Test MAE: 2017.4051895, Time: 11.324086666107178\n",
      "ToTal Epoch: 1834, LR: 0.000000, Loss: 209800.6506855, Val MAE: 3166.6582588, Test MAE: 2017.4051895, Time: 10.809564113616943\n",
      "ToTal Epoch: 1835, LR: 0.000000, Loss: 209601.3004538, Val MAE: 2689.3666447, Test MAE: 2017.4051895, Time: 10.458071231842041\n",
      "ToTal Epoch: 1836, LR: 0.000000, Loss: 210367.1294511, Val MAE: 2931.0585302, Test MAE: 2017.4051895, Time: 12.357247591018677\n",
      "ToTal Epoch: 1837, LR: 0.000000, Loss: 212144.4426695, Val MAE: 3286.7602662, Test MAE: 2017.4051895, Time: 11.726505279541016\n",
      "ToTal Epoch: 1838, LR: 0.000000, Loss: 211748.8322171, Val MAE: 2976.1333028, Test MAE: 2017.4051895, Time: 10.932530403137207\n",
      "ToTal Epoch: 1839, LR: 0.000000, Loss: 210870.8794344, Val MAE: 3118.8660714, Test MAE: 2017.4051895, Time: 11.08833122253418\n",
      "ToTal Epoch: 1840, LR: 0.000000, Loss: 211271.8739216, Val MAE: 2974.8074696, Test MAE: 2017.4051895, Time: 10.942176342010498\n",
      "ToTal Epoch: 1841, LR: 0.000000, Loss: 211200.5952324, Val MAE: 3050.4779485, Test MAE: 2017.4051895, Time: 10.591135740280151\n",
      "ToTal Epoch: 1842, LR: 0.000000, Loss: 208957.1785793, Val MAE: 3225.4747000, Test MAE: 2017.4051895, Time: 11.29810881614685\n",
      "ToTal Epoch: 1843, LR: 0.000000, Loss: 207977.3556394, Val MAE: 3192.5644778, Test MAE: 2017.4051895, Time: 12.67166543006897\n",
      "ToTal Epoch: 1844, LR: 0.000000, Loss: 211249.3022118, Val MAE: 3220.4751634, Test MAE: 2017.4051895, Time: 11.368468284606934\n",
      "ToTal Epoch: 1845, LR: 0.000000, Loss: 209765.4385325, Val MAE: 3048.5554059, Test MAE: 2017.4051895, Time: 11.236651182174683\n",
      "ToTal Epoch: 1846, LR: 0.000000, Loss: 209754.1054985, Val MAE: 3105.3078566, Test MAE: 2017.4051895, Time: 11.501206398010254\n",
      "ToTal Epoch: 1847, LR: 0.000000, Loss: 211881.1618784, Val MAE: 2972.3287281, Test MAE: 2017.4051895, Time: 11.037795782089233\n",
      "ToTal Epoch: 1848, LR: 0.000000, Loss: 209851.3073138, Val MAE: 3382.5117471, Test MAE: 2017.4051895, Time: 15.22922134399414\n",
      "ToTal Epoch: 1849, LR: 0.000000, Loss: 210403.6748101, Val MAE: 3219.8798154, Test MAE: 2017.4051895, Time: 13.572033166885376\n",
      "ToTal Epoch: 1850, LR: 0.000000, Loss: 210239.2622558, Val MAE: 2987.8544103, Test MAE: 2017.4051895, Time: 11.347213506698608\n",
      "ToTal Epoch: 1851, LR: 0.000000, Loss: 211216.4318922, Val MAE: 2914.4672141, Test MAE: 2017.4051895, Time: 11.223956108093262\n",
      "ToTal Epoch: 1852, LR: 0.000000, Loss: 211297.5676683, Val MAE: 2877.6554355, Test MAE: 2017.4051895, Time: 10.787160873413086\n",
      "ToTal Epoch: 1853, LR: 0.000000, Loss: 208828.3747576, Val MAE: 2733.4267274, Test MAE: 2017.4051895, Time: 10.735568284988403\n",
      "ToTal Epoch: 1854, LR: 0.000000, Loss: 210758.4450580, Val MAE: 3281.7778415, Test MAE: 2017.4051895, Time: 10.482569217681885\n",
      "ToTal Epoch: 1855, LR: 0.000000, Loss: 212030.8622367, Val MAE: 2741.7879357, Test MAE: 2017.4051895, Time: 11.659673929214478\n",
      "ToTal Epoch: 1856, LR: 0.000000, Loss: 209979.0618449, Val MAE: 3067.3384545, Test MAE: 2017.4051895, Time: 10.987439155578613\n",
      "ToTal Epoch: 1857, LR: 0.000000, Loss: 210389.2690393, Val MAE: 2774.1749073, Test MAE: 2017.4051895, Time: 10.46456003189087\n",
      "ToTal Epoch: 1858, LR: 0.000000, Loss: 210339.9768022, Val MAE: 3253.6105729, Test MAE: 2017.4051895, Time: 11.351659774780273\n",
      "ToTal Epoch: 1859, LR: 0.000000, Loss: 210665.9565280, Val MAE: 3306.5821868, Test MAE: 2017.4051895, Time: 11.368592500686646\n",
      "ToTal Epoch: 1860, LR: 0.000000, Loss: 210848.8206373, Val MAE: 3017.4815218, Test MAE: 2017.4051895, Time: 10.771908044815063\n",
      "ToTal Epoch: 1861, LR: 0.000000, Loss: 208353.1801080, Val MAE: 3240.8920020, Test MAE: 2017.4051895, Time: 10.254186868667603\n",
      "ToTal Epoch: 1862, LR: 0.000000, Loss: 209845.8074046, Val MAE: 2902.5039221, Test MAE: 2017.4051895, Time: 10.620286226272583\n",
      "ToTal Epoch: 1863, LR: 0.000000, Loss: 209875.4983997, Val MAE: 3007.3648915, Test MAE: 2017.4051895, Time: 11.200466871261597\n",
      "ToTal Epoch: 1864, LR: 0.000000, Loss: 212064.2028567, Val MAE: 3059.0737455, Test MAE: 2017.4051895, Time: 11.024675130844116\n",
      "ToTal Epoch: 1865, LR: 0.000000, Loss: 208483.6741604, Val MAE: 3104.7270313, Test MAE: 2017.4051895, Time: 11.124598503112793\n",
      "ToTal Epoch: 1866, LR: 0.000000, Loss: 209291.9118330, Val MAE: 3285.5814607, Test MAE: 2017.4051895, Time: 10.992807388305664\n",
      "ToTal Epoch: 1867, LR: 0.000000, Loss: 207950.6889600, Val MAE: 3088.8905450, Test MAE: 2017.4051895, Time: 11.314422845840454\n",
      "ToTal Epoch: 1868, LR: 0.000000, Loss: 208872.7318588, Val MAE: 3033.8848601, Test MAE: 2017.4051895, Time: 11.70200228691101\n",
      "ToTal Epoch: 1869, LR: 0.000000, Loss: 209904.1128935, Val MAE: 3320.9627045, Test MAE: 2017.4051895, Time: 11.807204246520996\n",
      "ToTal Epoch: 1870, LR: 0.000000, Loss: 210281.2471027, Val MAE: 3194.2340872, Test MAE: 2017.4051895, Time: 10.934939622879028\n",
      "ToTal Epoch: 1871, LR: 0.000000, Loss: 211433.0366407, Val MAE: 2810.2145341, Test MAE: 2017.4051895, Time: 10.85344672203064\n",
      "ToTal Epoch: 1872, LR: 0.000000, Loss: 209486.1687097, Val MAE: 2959.7673125, Test MAE: 2017.4051895, Time: 10.421655654907227\n",
      "ToTal Epoch: 1873, LR: 0.000000, Loss: 210286.6259399, Val MAE: 2848.1550724, Test MAE: 2017.4051895, Time: 10.809149503707886\n",
      "ToTal Epoch: 1874, LR: 0.000000, Loss: 211621.5333875, Val MAE: 2694.9827448, Test MAE: 2017.4051895, Time: 11.802070140838623\n",
      "ToTal Epoch: 1875, LR: 0.000000, Loss: 209587.2680648, Val MAE: 2933.2465318, Test MAE: 2017.4051895, Time: 10.872615575790405\n",
      "ToTal Epoch: 1876, LR: 0.000000, Loss: 210415.4265896, Val MAE: 2827.3474404, Test MAE: 2017.4051895, Time: 10.883106708526611\n",
      "ToTal Epoch: 1877, LR: 0.000000, Loss: 210403.0102900, Val MAE: 2729.8900195, Test MAE: 2017.4051895, Time: 10.666874647140503\n",
      "ToTal Epoch: 1878, LR: 0.000000, Loss: 210091.0876033, Val MAE: 2912.6031014, Test MAE: 2017.4051895, Time: 11.176429033279419\n",
      "ToTal Epoch: 1879, LR: 0.000000, Loss: 208566.7624898, Val MAE: 3210.5658393, Test MAE: 2017.4051895, Time: 10.44902753829956\n",
      "ToTal Epoch: 1880, LR: 0.000000, Loss: 208751.6734725, Val MAE: 3321.0007835, Test MAE: 2017.4051895, Time: 11.873011112213135\n",
      "ToTal Epoch: 1881, LR: 0.000000, Loss: 208851.5932547, Val MAE: 3171.1217611, Test MAE: 2017.4051895, Time: 11.595008373260498\n",
      "ToTal Epoch: 1882, LR: 0.000000, Loss: 208521.9401328, Val MAE: 2997.1945464, Test MAE: 2017.4051895, Time: 11.271811962127686\n",
      "ToTal Epoch: 1883, LR: 0.000000, Loss: 210915.3338365, Val MAE: 3049.4827305, Test MAE: 2017.4051895, Time: 11.230789184570312\n",
      "ToTal Epoch: 1884, LR: 0.000000, Loss: 209585.0567047, Val MAE: 3050.1884077, Test MAE: 2017.4051895, Time: 11.531678199768066\n",
      "ToTal Epoch: 1885, LR: 0.000000, Loss: 208133.3907228, Val MAE: 3250.3234063, Test MAE: 2017.4051895, Time: 12.58562159538269\n",
      "ToTal Epoch: 1886, LR: 0.000000, Loss: 210383.3000525, Val MAE: 3273.8562304, Test MAE: 2017.4051895, Time: 11.716540813446045\n",
      "ToTal Epoch: 1887, LR: 0.000000, Loss: 211196.9435819, Val MAE: 3030.2183798, Test MAE: 2017.4051895, Time: 11.27884292602539\n",
      "ToTal Epoch: 1888, LR: 0.000000, Loss: 209941.9805666, Val MAE: 2864.7588904, Test MAE: 2017.4051895, Time: 11.019570350646973\n",
      "ToTal Epoch: 1889, LR: 0.000000, Loss: 211254.4480008, Val MAE: 2751.2422705, Test MAE: 2017.4051895, Time: 12.156264781951904\n",
      "ToTal Epoch: 1890, LR: 0.000000, Loss: 210327.9244829, Val MAE: 2884.6234092, Test MAE: 2017.4051895, Time: 11.219611167907715\n",
      "ToTal Epoch: 1891, LR: 0.000000, Loss: 210662.9273205, Val MAE: 2876.8837136, Test MAE: 2017.4051895, Time: 10.61739182472229\n",
      "ToTal Epoch: 1892, LR: 0.000000, Loss: 210897.6144461, Val MAE: 3008.4705343, Test MAE: 2017.4051895, Time: 11.281553268432617\n",
      "ToTal Epoch: 1893, LR: 0.000000, Loss: 210245.1429991, Val MAE: 3008.7515383, Test MAE: 2017.4051895, Time: 11.175369501113892\n",
      "ToTal Epoch: 1894, LR: 0.000000, Loss: 210304.4025032, Val MAE: 2886.4899727, Test MAE: 2017.4051895, Time: 11.161081552505493\n",
      "ToTal Epoch: 1895, LR: 0.000000, Loss: 210038.2648259, Val MAE: 3022.5541017, Test MAE: 2017.4051895, Time: 12.321026802062988\n",
      "ToTal Epoch: 1896, LR: 0.000000, Loss: 209402.0856638, Val MAE: 2999.6314014, Test MAE: 2017.4051895, Time: 11.763208866119385\n",
      "ToTal Epoch: 1897, LR: 0.000000, Loss: 209396.2986672, Val MAE: 3180.4353933, Test MAE: 2017.4051895, Time: 11.672856569290161\n",
      "ToTal Epoch: 1898, LR: 0.000000, Loss: 209215.3000525, Val MAE: 2871.4023590, Test MAE: 2017.4051895, Time: 10.595849752426147\n",
      "ToTal Epoch: 1899, LR: 0.000000, Loss: 209759.2570582, Val MAE: 2909.5816374, Test MAE: 2017.4051895, Time: 10.384238243103027\n",
      "ToTal Epoch: 1900, LR: 0.000000, Loss: 209913.2957913, Val MAE: 3079.0973353, Test MAE: 2017.4051895, Time: 11.95145320892334\n",
      "ToTal Epoch: 1901, LR: 0.000000, Loss: 210465.2516505, Val MAE: 2971.3439100, Test MAE: 2017.4051895, Time: 13.63746953010559\n",
      "ToTal Epoch: 1902, LR: 0.000000, Loss: 210983.3868820, Val MAE: 2901.3764188, Test MAE: 2017.4051895, Time: 11.26718544960022\n",
      "ToTal Epoch: 1903, LR: 0.000000, Loss: 209125.9146802, Val MAE: 3244.6200269, Test MAE: 2017.4051895, Time: 10.789916515350342\n",
      "ToTal Epoch: 1904, LR: 0.000000, Loss: 210432.5880094, Val MAE: 3234.3721194, Test MAE: 2017.4051895, Time: 10.997143983840942\n",
      "ToTal Epoch: 1905, LR: 0.000000, Loss: 210058.4646252, Val MAE: 2992.7186377, Test MAE: 2017.4051895, Time: 12.419523477554321\n",
      "ToTal Epoch: 1906, LR: 0.000000, Loss: 208326.3378971, Val MAE: 3118.1330735, Test MAE: 2017.4051895, Time: 10.862857341766357\n",
      "ToTal Epoch: 1907, LR: 0.000000, Loss: 209258.2136913, Val MAE: 3126.8506172, Test MAE: 2017.4051895, Time: 11.051941633224487\n",
      "ToTal Epoch: 1908, LR: 0.000000, Loss: 211268.0249558, Val MAE: 3063.4568381, Test MAE: 2017.4051895, Time: 11.367781639099121\n",
      "ToTal Epoch: 1909, LR: 0.000000, Loss: 210709.8046912, Val MAE: 2981.8432173, Test MAE: 2017.4051895, Time: 10.498900651931763\n",
      "ToTal Epoch: 1910, LR: 0.000000, Loss: 209492.4110257, Val MAE: 3059.3133312, Test MAE: 2017.4051895, Time: 12.130029439926147\n",
      "ToTal Epoch: 1911, LR: 0.000000, Loss: 211558.5187025, Val MAE: 3020.6730585, Test MAE: 2017.4051895, Time: 11.528956413269043\n",
      "ToTal Epoch: 1912, LR: 0.000000, Loss: 209982.9560598, Val MAE: 3220.0621035, Test MAE: 2017.4051895, Time: 11.698221445083618\n",
      "ToTal Epoch: 1913, LR: 0.000000, Loss: 211367.3116706, Val MAE: 3079.8309400, Test MAE: 2017.4051895, Time: 10.70958161354065\n",
      "ToTal Epoch: 1914, LR: 0.000000, Loss: 210525.8305260, Val MAE: 3062.3150367, Test MAE: 2017.4051895, Time: 10.411247730255127\n",
      "ToTal Epoch: 1915, LR: 0.000000, Loss: 209731.0969665, Val MAE: 3011.7750564, Test MAE: 2017.4051895, Time: 9.994361639022827\n",
      "ToTal Epoch: 1916, LR: 0.000000, Loss: 211788.8788038, Val MAE: 3086.3573913, Test MAE: 2017.4051895, Time: 10.294543504714966\n",
      "ToTal Epoch: 1917, LR: 0.000000, Loss: 211415.1182153, Val MAE: 3082.8322441, Test MAE: 2017.4051895, Time: 10.761488676071167\n",
      "ToTal Epoch: 1918, LR: 0.000000, Loss: 210624.9359385, Val MAE: 3223.4645437, Test MAE: 2017.4051895, Time: 10.208148002624512\n",
      "ToTal Epoch: 1919, LR: 0.000000, Loss: 209919.0697177, Val MAE: 3268.3974289, Test MAE: 2017.4051895, Time: 10.65580415725708\n",
      "ToTal Epoch: 1920, LR: 0.000000, Loss: 207742.8918550, Val MAE: 3188.1416963, Test MAE: 2017.4051895, Time: 11.933273077011108\n",
      "ToTal Epoch: 1921, LR: 0.000000, Loss: 209002.7312282, Val MAE: 3322.5740751, Test MAE: 2017.4051895, Time: 11.669295310974121\n",
      "ToTal Epoch: 1922, LR: 0.000000, Loss: 211582.5382315, Val MAE: 2957.2486146, Test MAE: 2017.4051895, Time: 10.569034099578857\n",
      "ToTal Epoch: 1923, LR: 0.000000, Loss: 211205.2511155, Val MAE: 2994.9624560, Test MAE: 2017.4051895, Time: 10.95665431022644\n",
      "ToTal Epoch: 1924, LR: 0.000000, Loss: 210144.3467062, Val MAE: 3038.3223649, Test MAE: 2017.4051895, Time: 10.83407473564148\n",
      "ToTal Epoch: 1925, LR: 0.000000, Loss: 210470.1305307, Val MAE: 2819.3740207, Test MAE: 2017.4051895, Time: 10.485925674438477\n",
      "ToTal Epoch: 1926, LR: 0.000000, Loss: 210256.2980557, Val MAE: 3083.2847541, Test MAE: 2017.4051895, Time: 10.733437299728394\n",
      "ToTal Epoch: 1927, LR: 0.000000, Loss: 209833.3462380, Val MAE: 3040.9630675, Test MAE: 2017.4051895, Time: 10.659387588500977\n",
      "ToTal Epoch: 1928, LR: 0.000000, Loss: 210341.2057135, Val MAE: 3465.9328279, Test MAE: 2017.4051895, Time: 11.084206819534302\n",
      "ToTal Epoch: 1929, LR: 0.000000, Loss: 209118.8188602, Val MAE: 3207.6136876, Test MAE: 2017.4051895, Time: 11.987763166427612\n",
      "ToTal Epoch: 1930, LR: 0.000000, Loss: 209507.4119906, Val MAE: 3082.7665148, Test MAE: 2017.4051895, Time: 11.66929030418396\n",
      "ToTal Epoch: 1931, LR: 0.000000, Loss: 209570.6578130, Val MAE: 3263.9005341, Test MAE: 2017.4051895, Time: 11.707644701004028\n",
      "ToTal Epoch: 1932, LR: 0.000000, Loss: 207975.4349592, Val MAE: 3239.2483614, Test MAE: 2017.4051895, Time: 11.72869610786438\n",
      "ToTal Epoch: 1933, LR: 0.000000, Loss: 210041.0340037, Val MAE: 3028.4668797, Test MAE: 2017.4051895, Time: 12.426056623458862\n",
      "ToTal Epoch: 1934, LR: 0.000000, Loss: 212256.3647447, Val MAE: 3037.6710330, Test MAE: 2017.4051895, Time: 11.069978713989258\n",
      "ToTal Epoch: 1935, LR: 0.000000, Loss: 210995.9664453, Val MAE: 3009.7785963, Test MAE: 2017.4051895, Time: 12.768192768096924\n",
      "ToTal Epoch: 1936, LR: 0.000000, Loss: 210713.0490231, Val MAE: 3110.0376777, Test MAE: 2017.4051895, Time: 12.103317975997925\n",
      "ToTal Epoch: 1937, LR: 0.000000, Loss: 210377.0161181, Val MAE: 3083.3194508, Test MAE: 2017.4051895, Time: 12.949564695358276\n",
      "ToTal Epoch: 1938, LR: 0.000000, Loss: 209705.1493431, Val MAE: 3248.8345324, Test MAE: 2017.4051895, Time: 12.46164345741272\n",
      "ToTal Epoch: 1939, LR: 0.000000, Loss: 209762.4530836, Val MAE: 3077.1483796, Test MAE: 2017.4051895, Time: 11.004271268844604\n",
      "ToTal Epoch: 1940, LR: 0.000000, Loss: 209780.0687145, Val MAE: 3051.3485009, Test MAE: 2017.4051895, Time: 12.250473737716675\n",
      "ToTal Epoch: 1941, LR: 0.000000, Loss: 211103.8704438, Val MAE: 3189.7309008, Test MAE: 2017.4051895, Time: 11.506490468978882\n",
      "ToTal Epoch: 1942, LR: 0.000000, Loss: 209375.6570009, Val MAE: 3124.5661832, Test MAE: 2017.4051895, Time: 14.248519897460938\n",
      "ToTal Epoch: 1943, LR: 0.000000, Loss: 209348.4746955, Val MAE: 3083.3216627, Test MAE: 2017.4051895, Time: 11.327016830444336\n",
      "ToTal Epoch: 1944, LR: 0.000000, Loss: 209136.2288062, Val MAE: 3152.2967448, Test MAE: 2017.4051895, Time: 10.960403203964233\n",
      "ToTal Epoch: 1945, LR: 0.000000, Loss: 211025.8531887, Val MAE: 2825.6184791, Test MAE: 2017.4051895, Time: 11.441162586212158\n",
      "ToTal Epoch: 1946, LR: 0.000000, Loss: 210982.3580758, Val MAE: 3050.8826626, Test MAE: 2017.4051895, Time: 12.433260202407837\n",
      "ToTal Epoch: 1947, LR: 0.000000, Loss: 209189.3645822, Val MAE: 3021.7766711, Test MAE: 2017.4051895, Time: 10.659924745559692\n",
      "ToTal Epoch: 1948, LR: 0.000000, Loss: 210059.2980270, Val MAE: 3124.7500287, Test MAE: 2017.4051895, Time: 11.068589925765991\n",
      "ToTal Epoch: 1949, LR: 0.000000, Loss: 209057.9682606, Val MAE: 3409.9370318, Test MAE: 2017.4051895, Time: 10.554584980010986\n",
      "ToTal Epoch: 1950, LR: 0.000000, Loss: 211911.3591745, Val MAE: 3117.9848611, Test MAE: 2017.4051895, Time: 12.28504729270935\n",
      "ToTal Epoch: 1951, LR: 0.000000, Loss: 209340.9968566, Val MAE: 3109.4654705, Test MAE: 2017.4051895, Time: 11.592138528823853\n",
      "ToTal Epoch: 1952, LR: 0.000000, Loss: 211389.3692830, Val MAE: 3081.0670765, Test MAE: 2017.4051895, Time: 11.687892198562622\n",
      "ToTal Epoch: 1953, LR: 0.000000, Loss: 210050.7896240, Val MAE: 3162.7852175, Test MAE: 2017.4051895, Time: 11.034755229949951\n",
      "ToTal Epoch: 1954, LR: 0.000000, Loss: 210099.2398223, Val MAE: 2988.8672227, Test MAE: 2017.4051895, Time: 11.29771614074707\n",
      "ToTal Epoch: 1955, LR: 0.000000, Loss: 210872.9682320, Val MAE: 3037.3837757, Test MAE: 2017.4051895, Time: 10.872065305709839\n",
      "ToTal Epoch: 1956, LR: 0.000000, Loss: 210239.3547031, Val MAE: 3029.7671788, Test MAE: 2017.4051895, Time: 11.177068710327148\n",
      "ToTal Epoch: 1957, LR: 0.000000, Loss: 208869.4611188, Val MAE: 3272.7067473, Test MAE: 2017.4051895, Time: 11.269692659378052\n",
      "ToTal Epoch: 1958, LR: 0.000000, Loss: 209877.0293794, Val MAE: 3127.4408823, Test MAE: 2017.4051895, Time: 12.770757913589478\n",
      "ToTal Epoch: 1959, LR: 0.000000, Loss: 209935.4392013, Val MAE: 3209.2469713, Test MAE: 2017.4051895, Time: 12.32157588005066\n",
      "ToTal Epoch: 1960, LR: 0.000000, Loss: 211156.9061291, Val MAE: 3132.2646755, Test MAE: 2017.4051895, Time: 11.959167957305908\n",
      "ToTal Epoch: 1961, LR: 0.000000, Loss: 210708.9148044, Val MAE: 2940.9465337, Test MAE: 2017.4051895, Time: 11.625570297241211\n",
      "ToTal Epoch: 1962, LR: 0.000000, Loss: 208574.9840348, Val MAE: 3012.7383962, Test MAE: 2017.4051895, Time: 10.331309080123901\n",
      "ToTal Epoch: 1963, LR: 0.000000, Loss: 208360.2586920, Val MAE: 3148.5103426, Test MAE: 2017.4051895, Time: 11.144112586975098\n",
      "ToTal Epoch: 1964, LR: 0.000000, Loss: 211819.6650265, Val MAE: 3125.0712470, Test MAE: 2017.4051895, Time: 12.774559020996094\n",
      "ToTal Epoch: 1965, LR: 0.000000, Loss: 211718.9938948, Val MAE: 3217.2751376, Test MAE: 2017.4051895, Time: 11.190105676651001\n",
      "ToTal Epoch: 1966, LR: 0.000000, Loss: 210044.4049109, Val MAE: 3331.6399908, Test MAE: 2017.4051895, Time: 12.887817144393921\n",
      "ToTal Epoch: 1967, LR: 0.000000, Loss: 211866.0425548, Val MAE: 3136.3730605, Test MAE: 2017.4051895, Time: 13.04353928565979\n",
      "ToTal Epoch: 1968, LR: 0.000000, Loss: 209644.0530454, Val MAE: 3027.5004013, Test MAE: 2017.4051895, Time: 12.462908029556274\n",
      "ToTal Epoch: 1969, LR: 0.000000, Loss: 210592.7964458, Val MAE: 3037.6628831, Test MAE: 2017.4051895, Time: 14.483251810073853\n",
      "ToTal Epoch: 1970, LR: 0.000000, Loss: 210686.3237950, Val MAE: 2970.3898762, Test MAE: 2017.4051895, Time: 12.579160690307617\n",
      "ToTal Epoch: 1971, LR: 0.000000, Loss: 210791.4528066, Val MAE: 3040.0651896, Test MAE: 2017.4051895, Time: 11.298947095870972\n",
      "ToTal Epoch: 1972, LR: 0.000000, Loss: 211749.3324034, Val MAE: 3051.1116955, Test MAE: 2017.4051895, Time: 11.156651735305786\n",
      "ToTal Epoch: 1973, LR: 0.000000, Loss: 209711.0690680, Val MAE: 3115.1087146, Test MAE: 2017.4051895, Time: 11.684664726257324\n",
      "ToTal Epoch: 1974, LR: 0.000000, Loss: 210523.6629246, Val MAE: 3060.2322288, Test MAE: 2017.4051895, Time: 13.314213991165161\n",
      "ToTal Epoch: 1975, LR: 0.000000, Loss: 210617.6167773, Val MAE: 3284.0225818, Test MAE: 2017.4051895, Time: 11.579111337661743\n",
      "ToTal Epoch: 1976, LR: 0.000000, Loss: 209603.7993599, Val MAE: 3244.6595295, Test MAE: 2017.4051895, Time: 12.535596370697021\n",
      "ToTal Epoch: 1977, LR: 0.000000, Loss: 209083.5955477, Val MAE: 3195.1239538, Test MAE: 2017.4051895, Time: 12.106795310974121\n",
      "ToTal Epoch: 1978, LR: 0.000000, Loss: 210882.1829265, Val MAE: 3061.4745423, Test MAE: 2017.4051895, Time: 12.323033094406128\n",
      "ToTal Epoch: 1979, LR: 0.000000, Loss: 210142.1555630, Val MAE: 3090.6892532, Test MAE: 2017.4051895, Time: 12.59075379371643\n",
      "ToTal Epoch: 1980, LR: 0.000000, Loss: 211386.9123776, Val MAE: 3084.9099929, Test MAE: 2017.4051895, Time: 11.349841594696045\n",
      "ToTal Epoch: 1981, LR: 0.000000, Loss: 210745.3225434, Val MAE: 3013.9111872, Test MAE: 2017.4051895, Time: 12.258788347244263\n",
      "ToTal Epoch: 1982, LR: 0.000000, Loss: 210322.3919744, Val MAE: 3056.2656835, Test MAE: 2017.4051895, Time: 10.86741304397583\n",
      "ToTal Epoch: 1983, LR: 0.000000, Loss: 210049.9371901, Val MAE: 2989.0679890, Test MAE: 2017.4051895, Time: 11.289363622665405\n",
      "ToTal Epoch: 1984, LR: 0.000000, Loss: 208818.4951608, Val MAE: 3046.9544065, Test MAE: 2017.4051895, Time: 11.21386432647705\n",
      "ToTal Epoch: 1985, LR: 0.000000, Loss: 209304.7194764, Val MAE: 3128.9916208, Test MAE: 2017.4051895, Time: 11.4089937210083\n",
      "ToTal Epoch: 1986, LR: 0.000000, Loss: 209837.1912674, Val MAE: 3104.2038237, Test MAE: 2017.4051895, Time: 12.290033340454102\n",
      "ToTal Epoch: 1987, LR: 0.000000, Loss: 210442.7455596, Val MAE: 2974.5891663, Test MAE: 2017.4051895, Time: 11.678457021713257\n",
      "ToTal Epoch: 1988, LR: 0.000000, Loss: 210310.6290355, Val MAE: 3080.0899736, Test MAE: 2017.4051895, Time: 12.735014200210571\n",
      "ToTal Epoch: 1989, LR: 0.000000, Loss: 209661.9738403, Val MAE: 3131.3585808, Test MAE: 2017.4051895, Time: 11.351487159729004\n",
      "ToTal Epoch: 1990, LR: 0.000000, Loss: 210558.9165050, Val MAE: 2993.0532609, Test MAE: 2017.4051895, Time: 11.535870552062988\n",
      "ToTal Epoch: 1991, LR: 0.000000, Loss: 211066.4208666, Val MAE: 3024.2337862, Test MAE: 2017.4051895, Time: 11.565672636032104\n",
      "ToTal Epoch: 1992, LR: 0.000000, Loss: 209176.9870348, Val MAE: 3033.8579263, Test MAE: 2017.4051895, Time: 12.650506258010864\n",
      "ToTal Epoch: 1993, LR: 0.000000, Loss: 209684.3686046, Val MAE: 3136.5056371, Test MAE: 2017.4051895, Time: 11.865042924880981\n",
      "ToTal Epoch: 1994, LR: 0.000000, Loss: 210499.9076291, Val MAE: 3080.0203270, Test MAE: 2017.4051895, Time: 12.344541788101196\n",
      "ToTal Epoch: 1995, LR: 0.000000, Loss: 210088.9843214, Val MAE: 3227.0535858, Test MAE: 2017.4051895, Time: 12.090026140213013\n",
      "ToTal Epoch: 1996, LR: 0.000000, Loss: 210398.6204748, Val MAE: 3069.2551833, Test MAE: 2017.4051895, Time: 11.0947265625\n",
      "ToTal Epoch: 1997, LR: 0.000000, Loss: 208444.7788277, Val MAE: 3077.3765144, Test MAE: 2017.4051895, Time: 13.270612239837646\n",
      "ToTal Epoch: 1998, LR: 0.000000, Loss: 210769.7702193, Val MAE: 3008.1751080, Test MAE: 2017.4051895, Time: 12.191049575805664\n",
      "ToTal Epoch: 1999, LR: 0.000000, Loss: 210262.4491091, Val MAE: 3060.8729458, Test MAE: 2017.4051895, Time: 10.99133825302124\n",
      "ToTal Epoch: 2000, LR: 0.000000, Loss: 210524.3104763, Val MAE: 2883.1244793, Test MAE: 2017.4051895, Time: 11.25098991394043\n",
      "ToTal Epoch: 2001, LR: 0.000000, Loss: 209989.0932021, Val MAE: 2959.1006077, Test MAE: 2017.4051895, Time: 11.863927841186523\n",
      "ToTal Epoch: 2002, LR: 0.000000, Loss: 210384.4889887, Val MAE: 3064.2918530, Test MAE: 2017.4051895, Time: 10.598322868347168\n",
      "ToTal Epoch: 2003, LR: 0.000000, Loss: 208446.6887689, Val MAE: 3206.3042546, Test MAE: 2017.4051895, Time: 11.085735321044922\n",
      "ToTal Epoch: 2004, LR: 0.000000, Loss: 210587.8152582, Val MAE: 3009.7311396, Test MAE: 2017.4051895, Time: 11.755221128463745\n",
      "ToTal Epoch: 2005, LR: 0.000000, Loss: 210738.5661300, Val MAE: 2895.1029007, Test MAE: 2017.4051895, Time: 11.744948148727417\n",
      "ToTal Epoch: 2006, LR: 0.000000, Loss: 210341.7620408, Val MAE: 3099.0083696, Test MAE: 2017.4051895, Time: 12.845921754837036\n",
      "ToTal Epoch: 2007, LR: 0.000000, Loss: 210324.4985048, Val MAE: 3349.8024010, Test MAE: 2017.4051895, Time: 12.9209303855896\n",
      "ToTal Epoch: 2008, LR: 0.000000, Loss: 209651.4153155, Val MAE: 3075.6359398, Test MAE: 2017.4051895, Time: 11.92631459236145\n",
      "ToTal Epoch: 2009, LR: 0.000000, Loss: 210635.1672861, Val MAE: 2848.4598525, Test MAE: 2017.4051895, Time: 13.123153924942017\n",
      "ToTal Epoch: 2010, LR: 0.000000, Loss: 208989.5243682, Val MAE: 3167.0847999, Test MAE: 2017.4051895, Time: 11.296880960464478\n",
      "ToTal Epoch: 2011, LR: 0.000000, Loss: 209158.3908661, Val MAE: 3187.8339783, Test MAE: 2017.4051895, Time: 11.789928674697876\n",
      "ToTal Epoch: 2012, LR: 0.000000, Loss: 209733.6857211, Val MAE: 2960.9925619, Test MAE: 2017.4051895, Time: 11.447259426116943\n",
      "ToTal Epoch: 2013, LR: 0.000000, Loss: 208596.8733387, Val MAE: 2944.1594531, Test MAE: 2017.4051895, Time: 11.026617050170898\n",
      "ToTal Epoch: 2014, LR: 0.000000, Loss: 209798.1810156, Val MAE: 2981.5713856, Test MAE: 2017.4051895, Time: 12.903714656829834\n",
      "ToTal Epoch: 2015, LR: 0.000000, Loss: 209828.3609612, Val MAE: 2945.0522290, Test MAE: 2017.4051895, Time: 12.18816065788269\n",
      "ToTal Epoch: 2016, LR: 0.000000, Loss: 209571.5292791, Val MAE: 3006.5723315, Test MAE: 2017.4051895, Time: 11.351134300231934\n",
      "ToTal Epoch: 2017, LR: 0.000000, Loss: 209461.6214016, Val MAE: 3138.7890010, Test MAE: 2017.4051895, Time: 11.913078308105469\n",
      "ToTal Epoch: 2018, LR: 0.000000, Loss: 209994.8026943, Val MAE: 3127.3601047, Test MAE: 2017.4051895, Time: 11.623409748077393\n",
      "ToTal Epoch: 2019, LR: 0.000000, Loss: 209890.0030000, Val MAE: 3067.3954034, Test MAE: 2017.4051895, Time: 11.386157274246216\n",
      "ToTal Epoch: 2020, LR: 0.000000, Loss: 209774.6665265, Val MAE: 2978.6496073, Test MAE: 2017.4051895, Time: 12.61470103263855\n",
      "ToTal Epoch: 2021, LR: 0.000000, Loss: 210855.4964888, Val MAE: 2884.7601372, Test MAE: 2017.4051895, Time: 12.238312482833862\n",
      "ToTal Epoch: 2022, LR: 0.000000, Loss: 210304.2976735, Val MAE: 3068.6791638, Test MAE: 2017.4051895, Time: 11.875355005264282\n",
      "ToTal Epoch: 2023, LR: 0.000000, Loss: 209731.3831749, Val MAE: 3184.3224031, Test MAE: 2017.4051895, Time: 13.035646677017212\n",
      "ToTal Epoch: 2024, LR: 0.000000, Loss: 210470.7009220, Val MAE: 3122.9358137, Test MAE: 2017.4051895, Time: 12.18279767036438\n",
      "ToTal Epoch: 2025, LR: 0.000000, Loss: 210367.1023551, Val MAE: 3021.0216741, Test MAE: 2017.4051895, Time: 11.2662935256958\n",
      "ToTal Epoch: 2026, LR: 0.000000, Loss: 211114.8987341, Val MAE: 3050.5328336, Test MAE: 2017.4051895, Time: 10.680359840393066\n",
      "ToTal Epoch: 2027, LR: 0.000000, Loss: 210095.3027278, Val MAE: 2990.1972550, Test MAE: 2017.4051895, Time: 11.417850732803345\n",
      "ToTal Epoch: 2028, LR: 0.000000, Loss: 210239.5352028, Val MAE: 3039.5058807, Test MAE: 2017.4051895, Time: 11.220051527023315\n",
      "ToTal Epoch: 2029, LR: 0.000000, Loss: 210393.2684278, Val MAE: 3039.8849366, Test MAE: 2017.4051895, Time: 11.578885078430176\n",
      "ToTal Epoch: 2030, LR: 0.000000, Loss: 210508.6261501, Val MAE: 2978.3655173, Test MAE: 2017.4051895, Time: 12.15532374382019\n",
      "ToTal Epoch: 2031, LR: 0.000000, Loss: 211282.1942388, Val MAE: 2987.5412702, Test MAE: 2017.4051895, Time: 11.937400102615356\n",
      "ToTal Epoch: 2032, LR: 0.000000, Loss: 210266.0500454, Val MAE: 3133.7520351, Test MAE: 2017.4051895, Time: 12.226521492004395\n",
      "ToTal Epoch: 2033, LR: 0.000000, Loss: 210905.8012516, Val MAE: 3087.6962232, Test MAE: 2017.4051895, Time: 12.10505986213684\n",
      "ToTal Epoch: 2034, LR: 0.000000, Loss: 211401.7470597, Val MAE: 2968.6119726, Test MAE: 2017.4051895, Time: 12.218852519989014\n",
      "ToTal Epoch: 2035, LR: 0.000000, Loss: 209319.7757417, Val MAE: 3005.4651695, Test MAE: 2017.4051895, Time: 10.737861156463623\n",
      "ToTal Epoch: 2036, LR: 0.000000, Loss: 209574.7785793, Val MAE: 3002.3108853, Test MAE: 2017.4051895, Time: 12.409903287887573\n",
      "ToTal Epoch: 2037, LR: 0.000000, Loss: 208260.4198538, Val MAE: 3113.9885443, Test MAE: 2017.4051895, Time: 11.048232555389404\n",
      "ToTal Epoch: 2038, LR: 0.000000, Loss: 210098.9053838, Val MAE: 3115.5305693, Test MAE: 2017.4051895, Time: 12.25495719909668\n",
      "ToTal Epoch: 2039, LR: 0.000000, Loss: 211284.4988869, Val MAE: 3234.5260882, Test MAE: 2017.4051895, Time: 13.010358095169067\n",
      "ToTal Epoch: 2040, LR: 0.000000, Loss: 209645.2732432, Val MAE: 3347.4546788, Test MAE: 2017.4051895, Time: 12.121091842651367\n",
      "ToTal Epoch: 2041, LR: 0.000000, Loss: 209909.1553432, Val MAE: 3105.3949878, Test MAE: 2017.4051895, Time: 12.459036350250244\n",
      "ToTal Epoch: 2042, LR: 0.000000, Loss: 209006.3159222, Val MAE: 3155.5230022, Test MAE: 2017.4051895, Time: 12.737918853759766\n",
      "ToTal Epoch: 2043, LR: 0.000000, Loss: 210945.9778149, Val MAE: 3067.2362560, Test MAE: 2017.4051895, Time: 11.29280400276184\n",
      "ToTal Epoch: 2044, LR: 0.000000, Loss: 208973.2011274, Val MAE: 3176.5162711, Test MAE: 2017.4051895, Time: 10.963160753250122\n",
      "ToTal Epoch: 2045, LR: 0.000000, Loss: 210396.6474753, Val MAE: 3102.0014523, Test MAE: 2017.4051895, Time: 12.559046268463135\n",
      "ToTal Epoch: 2046, LR: 0.000000, Loss: 211023.8692591, Val MAE: 3062.0269434, Test MAE: 2017.4051895, Time: 12.0269193649292\n",
      "ToTal Epoch: 2047, LR: 0.000000, Loss: 209857.4992595, Val MAE: 3003.7308912, Test MAE: 2017.4051895, Time: 12.338321208953857\n",
      "ToTal Epoch: 2048, LR: 0.000000, Loss: 209657.6089046, Val MAE: 3005.8555903, Test MAE: 2017.4051895, Time: 12.028239011764526\n",
      "ToTal Epoch: 2049, LR: 0.000000, Loss: 212189.7167152, Val MAE: 2989.6866927, Test MAE: 2017.4051895, Time: 11.402296543121338\n",
      "ToTal Epoch: 2050, LR: 0.000000, Loss: 210076.2028950, Val MAE: 3032.4265650, Test MAE: 2017.4051895, Time: 11.889533042907715\n",
      "ToTal Epoch: 2051, LR: 0.000000, Loss: 210002.9293842, Val MAE: 2924.1751796, Test MAE: 2017.4051895, Time: 11.044026851654053\n",
      "ToTal Epoch: 2052, LR: 0.000000, Loss: 211482.4938232, Val MAE: 2883.5014332, Test MAE: 2017.4051895, Time: 11.510408163070679\n",
      "ToTal Epoch: 2053, LR: 0.000000, Loss: 211088.5639325, Val MAE: 2972.7676374, Test MAE: 2017.4051895, Time: 11.294739723205566\n",
      "ToTal Epoch: 2054, LR: 0.000000, Loss: 208187.2649692, Val MAE: 3039.6748261, Test MAE: 2017.4051895, Time: 11.828675508499146\n",
      "ToTal Epoch: 2055, LR: 0.000000, Loss: 208971.9123680, Val MAE: 3087.7521641, Test MAE: 2017.4051895, Time: 11.598032474517822\n",
      "ToTal Epoch: 2056, LR: 0.000000, Loss: 210545.6580519, Val MAE: 3226.9392580, Test MAE: 2017.4051895, Time: 12.335587978363037\n",
      "ToTal Epoch: 2057, LR: 0.000000, Loss: 210672.8189557, Val MAE: 3175.8032466, Test MAE: 2017.4051895, Time: 12.434006929397583\n",
      "ToTal Epoch: 2058, LR: 0.000000, Loss: 210540.4825300, Val MAE: 3199.9884488, Test MAE: 2017.4051895, Time: 10.443196773529053\n",
      "ToTal Epoch: 2059, LR: 0.000000, Loss: 210765.5241007, Val MAE: 3184.3155001, Test MAE: 2017.4051895, Time: 11.73617172241211\n",
      "ToTal Epoch: 2060, LR: 0.000000, Loss: 210517.1953948, Val MAE: 3171.4762812, Test MAE: 2017.4051895, Time: 10.703847885131836\n",
      "ToTal Epoch: 2061, LR: 0.000000, Loss: 211120.8556442, Val MAE: 3095.5919371, Test MAE: 2017.4051895, Time: 12.056024312973022\n",
      "ToTal Epoch: 2062, LR: 0.000000, Loss: 210106.3257058, Val MAE: 3132.4023112, Test MAE: 2017.4051895, Time: 10.860194206237793\n",
      "ToTal Epoch: 2063, LR: 0.000000, Loss: 210985.2488224, Val MAE: 3065.3274383, Test MAE: 2017.4051895, Time: 10.499379634857178\n",
      "ToTal Epoch: 2064, LR: 0.000000, Loss: 209872.5488750, Val MAE: 3013.0156883, Test MAE: 2017.4051895, Time: 10.481427192687988\n",
      "ToTal Epoch: 2065, LR: 0.000000, Loss: 210780.1540916, Val MAE: 3206.6060489, Test MAE: 2017.4051895, Time: 10.901308298110962\n",
      "ToTal Epoch: 2066, LR: 0.000000, Loss: 209705.4795395, Val MAE: 3155.9046664, Test MAE: 2017.4051895, Time: 11.014820337295532\n",
      "ToTal Epoch: 2067, LR: 0.000000, Loss: 209099.2801796, Val MAE: 3128.6406692, Test MAE: 2017.4051895, Time: 11.076486825942993\n",
      "ToTal Epoch: 2068, LR: 0.000000, Loss: 209240.7491329, Val MAE: 3161.5893000, Test MAE: 2017.4051895, Time: 11.498154401779175\n",
      "ToTal Epoch: 2069, LR: 0.000000, Loss: 209270.2965843, Val MAE: 3084.9248691, Test MAE: 2017.4051895, Time: 11.049727201461792\n",
      "ToTal Epoch: 2070, LR: 0.000000, Loss: 209622.9642383, Val MAE: 2990.7641166, Test MAE: 2017.4051895, Time: 11.530675172805786\n",
      "ToTal Epoch: 2071, LR: 0.000000, Loss: 210092.6976162, Val MAE: 2997.0381411, Test MAE: 2017.4051895, Time: 11.765032052993774\n",
      "ToTal Epoch: 2072, LR: 0.000000, Loss: 208847.3675823, Val MAE: 3092.3410628, Test MAE: 2017.4051895, Time: 11.423206329345703\n",
      "ToTal Epoch: 2073, LR: 0.000000, Loss: 210412.2645392, Val MAE: 3004.6364987, Test MAE: 2017.4051895, Time: 11.629672527313232\n",
      "ToTal Epoch: 2074, LR: 0.000000, Loss: 211058.7566808, Val MAE: 2914.5805864, Test MAE: 2017.4051895, Time: 11.392795085906982\n",
      "ToTal Epoch: 2075, LR: 0.000000, Loss: 210003.0389528, Val MAE: 3077.2577104, Test MAE: 2017.4051895, Time: 11.793187379837036\n",
      "ToTal Epoch: 2076, LR: 0.000000, Loss: 210245.7677734, Val MAE: 3151.5324801, Test MAE: 2017.4051895, Time: 11.871008396148682\n",
      "ToTal Epoch: 2077, LR: 0.000000, Loss: 209986.9536903, Val MAE: 3093.8593834, Test MAE: 2017.4051895, Time: 12.804967164993286\n",
      "ToTal Epoch: 2078, LR: 0.000000, Loss: 211109.9888597, Val MAE: 3064.6689119, Test MAE: 2017.4051895, Time: 11.46295952796936\n",
      "ToTal Epoch: 2079, LR: 0.000000, Loss: 208801.4013854, Val MAE: 3136.7315505, Test MAE: 2017.4051895, Time: 11.215139627456665\n",
      "ToTal Epoch: 2080, LR: 0.000000, Loss: 210957.4523289, Val MAE: 3057.5561368, Test MAE: 2017.4051895, Time: 12.177116632461548\n",
      "ToTal Epoch: 2081, LR: 0.000000, Loss: 211915.5287059, Val MAE: 2997.8435470, Test MAE: 2017.4051895, Time: 11.14688515663147\n",
      "ToTal Epoch: 2082, LR: 0.000000, Loss: 208154.2261119, Val MAE: 3010.5977079, Test MAE: 2017.4051895, Time: 11.1802396774292\n",
      "ToTal Epoch: 2083, LR: 0.000000, Loss: 208014.2980748, Val MAE: 3015.4328231, Test MAE: 2017.4051895, Time: 11.141608715057373\n",
      "ToTal Epoch: 2084, LR: 0.000000, Loss: 208573.2464530, Val MAE: 3124.5693553, Test MAE: 2017.4051895, Time: 10.992005586624146\n",
      "ToTal Epoch: 2085, LR: 0.000000, Loss: 210060.5517795, Val MAE: 3040.0929785, Test MAE: 2017.4051895, Time: 12.195501327514648\n",
      "ToTal Epoch: 2086, LR: 0.000000, Loss: 210854.3734009, Val MAE: 3155.1436502, Test MAE: 2017.4051895, Time: 13.186393737792969\n",
      "ToTal Epoch: 2087, LR: 0.000000, Loss: 210243.9149668, Val MAE: 3152.4768593, Test MAE: 2017.4051895, Time: 10.636735916137695\n",
      "ToTal Epoch: 2088, LR: 0.000000, Loss: 209444.8166627, Val MAE: 3018.8736433, Test MAE: 2017.4051895, Time: 11.72099757194519\n",
      "ToTal Epoch: 2089, LR: 0.000000, Loss: 209907.5671141, Val MAE: 3182.4751682, Test MAE: 2017.4051895, Time: 10.913378953933716\n",
      "ToTal Epoch: 2090, LR: 0.000000, Loss: 210381.8975971, Val MAE: 3166.6021364, Test MAE: 2017.4051895, Time: 11.837548971176147\n",
      "ToTal Epoch: 2091, LR: 0.000000, Loss: 209729.7905890, Val MAE: 3029.7645370, Test MAE: 2017.4051895, Time: 11.709269285202026\n",
      "ToTal Epoch: 2092, LR: 0.000000, Loss: 210581.9481202, Val MAE: 3022.2852127, Test MAE: 2017.4051895, Time: 11.038527727127075\n",
      "ToTal Epoch: 2093, LR: 0.000000, Loss: 208998.8033440, Val MAE: 3039.3725302, Test MAE: 2017.4051895, Time: 11.153505802154541\n",
      "ToTal Epoch: 2094, LR: 0.000000, Loss: 210205.2102995, Val MAE: 3044.7447260, Test MAE: 2017.4051895, Time: 10.956284523010254\n",
      "ToTal Epoch: 2095, LR: 0.000000, Loss: 208923.2297330, Val MAE: 2951.7959996, Test MAE: 2017.4051895, Time: 11.234434127807617\n",
      "ToTal Epoch: 2096, LR: 0.000000, Loss: 210683.1386997, Val MAE: 2904.4360143, Test MAE: 2017.4051895, Time: 11.806004762649536\n",
      "ToTal Epoch: 2097, LR: 0.000000, Loss: 210002.3973248, Val MAE: 2962.7508981, Test MAE: 2017.4051895, Time: 11.645821809768677\n",
      "ToTal Epoch: 2098, LR: 0.000000, Loss: 210609.0392395, Val MAE: 2901.1091541, Test MAE: 2017.4051895, Time: 11.44469428062439\n",
      "ToTal Epoch: 2099, LR: 0.000000, Loss: 208415.9481011, Val MAE: 2992.3684361, Test MAE: 2017.4051895, Time: 11.733300924301147\n",
      "ToTal Epoch: 2100, LR: 0.000000, Loss: 211731.6630392, Val MAE: 2974.1928648, Test MAE: 2017.4051895, Time: 13.190447807312012\n",
      "ToTal Epoch: 2101, LR: 0.000000, Loss: 211673.4953614, Val MAE: 2908.1583926, Test MAE: 2017.4051895, Time: 12.788670778274536\n",
      "ToTal Epoch: 2102, LR: 0.000000, Loss: 210552.1323843, Val MAE: 2959.1809839, Test MAE: 2017.4051895, Time: 11.6775643825531\n",
      "ToTal Epoch: 2103, LR: 0.000000, Loss: 210428.1122438, Val MAE: 2988.6157609, Test MAE: 2017.4051895, Time: 11.65702772140503\n",
      "ToTal Epoch: 2104, LR: 0.000000, Loss: 209826.2600105, Val MAE: 3022.7641118, Test MAE: 2017.4051895, Time: 12.251518249511719\n",
      "ToTal Epoch: 2105, LR: 0.000000, Loss: 210457.0945779, Val MAE: 3135.0694222, Test MAE: 2017.4051895, Time: 12.607232570648193\n",
      "ToTal Epoch: 2106, LR: 0.000000, Loss: 211166.2799217, Val MAE: 3160.3726687, Test MAE: 2017.4051895, Time: 12.762899398803711\n",
      "ToTal Epoch: 2107, LR: 0.000000, Loss: 209625.2920461, Val MAE: 3039.0703728, Test MAE: 2017.4051895, Time: 12.417229175567627\n",
      "ToTal Epoch: 2108, LR: 0.000000, Loss: 209941.7535948, Val MAE: 2999.6861098, Test MAE: 2017.4051895, Time: 13.461735010147095\n",
      "ToTal Epoch: 2109, LR: 0.000000, Loss: 209347.2322171, Val MAE: 3019.8533211, Test MAE: 2017.4051895, Time: 10.936932563781738\n",
      "ToTal Epoch: 2110, LR: 0.000000, Loss: 210126.5118234, Val MAE: 2991.4833897, Test MAE: 2017.4051895, Time: 11.056628704071045\n",
      "ToTal Epoch: 2111, LR: 0.000000, Loss: 209948.9041800, Val MAE: 3056.5233557, Test MAE: 2017.4051895, Time: 11.933645725250244\n",
      "ToTal Epoch: 2112, LR: 0.000000, Loss: 210402.4191850, Val MAE: 3070.9342706, Test MAE: 2017.4051895, Time: 12.736377477645874\n",
      "ToTal Epoch: 2113, LR: 0.000000, Loss: 209451.5739168, Val MAE: 3159.1183454, Test MAE: 2017.4051895, Time: 11.209757089614868\n",
      "ToTal Epoch: 2114, LR: 0.000000, Loss: 211657.2162996, Val MAE: 3182.9462327, Test MAE: 2017.4051895, Time: 11.42364764213562\n",
      "ToTal Epoch: 2115, LR: 0.000000, Loss: 210195.8662017, Val MAE: 3125.1954540, Test MAE: 2017.4051895, Time: 12.450151681900024\n",
      "ToTal Epoch: 2116, LR: 0.000000, Loss: 210148.3578656, Val MAE: 3182.4349776, Test MAE: 2017.4051895, Time: 11.303415536880493\n",
      "ToTal Epoch: 2117, LR: 0.000000, Loss: 210086.7021067, Val MAE: 3093.9773657, Test MAE: 2017.4051895, Time: 10.867899894714355\n",
      "ToTal Epoch: 2118, LR: 0.000000, Loss: 210595.2565996, Val MAE: 3144.9920651, Test MAE: 2017.4051895, Time: 11.967544078826904\n",
      "ToTal Epoch: 2119, LR: 0.000000, Loss: 211474.0273062, Val MAE: 3060.6748787, Test MAE: 2017.4051895, Time: 13.672460556030273\n",
      "ToTal Epoch: 2120, LR: 0.000000, Loss: 211600.7878087, Val MAE: 3035.6468079, Test MAE: 2017.4051895, Time: 11.925308465957642\n",
      "ToTal Epoch: 2121, LR: 0.000000, Loss: 209464.2928199, Val MAE: 3217.0898829, Test MAE: 2017.4051895, Time: 11.297886610031128\n",
      "ToTal Epoch: 2122, LR: 0.000000, Loss: 209850.1483017, Val MAE: 3207.4258771, Test MAE: 2017.4051895, Time: 11.796301364898682\n",
      "ToTal Epoch: 2123, LR: 0.000000, Loss: 209630.8036880, Val MAE: 3139.9456403, Test MAE: 2017.4051895, Time: 11.024619102478027\n",
      "ToTal Epoch: 2124, LR: 0.000000, Loss: 209247.5381837, Val MAE: 3078.3407523, Test MAE: 2017.4051895, Time: 12.206580638885498\n",
      "ToTal Epoch: 2125, LR: 0.000000, Loss: 209915.7262504, Val MAE: 3013.5316250, Test MAE: 2017.4051895, Time: 11.147403001785278\n",
      "ToTal Epoch: 2126, LR: 0.000000, Loss: 209869.7855061, Val MAE: 2977.0729477, Test MAE: 2017.4051895, Time: 11.80289340019226\n",
      "ToTal Epoch: 2127, LR: 0.000000, Loss: 209161.8792529, Val MAE: 3033.4592123, Test MAE: 2017.4051895, Time: 11.920968770980835\n",
      "ToTal Epoch: 2128, LR: 0.000000, Loss: 209527.0689915, Val MAE: 2964.1559753, Test MAE: 2017.4051895, Time: 11.562742948532104\n",
      "ToTal Epoch: 2129, LR: 0.000000, Loss: 208929.3259447, Val MAE: 3011.0224289, Test MAE: 2017.4051895, Time: 11.940587043762207\n",
      "ToTal Epoch: 2130, LR: 0.000000, Loss: 210273.4058568, Val MAE: 3014.3582655, Test MAE: 2017.4051895, Time: 12.320145845413208\n",
      "ToTal Epoch: 2131, LR: 0.000000, Loss: 208530.0427077, Val MAE: 3048.6661412, Test MAE: 2017.4051895, Time: 11.255117654800415\n",
      "ToTal Epoch: 2132, LR: 0.000000, Loss: 209829.2532938, Val MAE: 3035.2861968, Test MAE: 2017.4051895, Time: 11.838619709014893\n",
      "ToTal Epoch: 2133, LR: 0.000000, Loss: 209941.1235083, Val MAE: 3040.1030106, Test MAE: 2017.4051895, Time: 11.982505083084106\n",
      "ToTal Epoch: 2134, LR: 0.000000, Loss: 209838.1356901, Val MAE: 3077.2548632, Test MAE: 2017.4051895, Time: 10.98929500579834\n",
      "ToTal Epoch: 2135, LR: 0.000000, Loss: 209205.9276740, Val MAE: 3089.7945616, Test MAE: 2017.4051895, Time: 10.902233839035034\n",
      "ToTal Epoch: 2136, LR: 0.000000, Loss: 210838.1962261, Val MAE: 3222.1274364, Test MAE: 2017.4051895, Time: 12.732107639312744\n",
      "ToTal Epoch: 2137, LR: 0.000000, Loss: 208754.8660202, Val MAE: 3267.8423766, Test MAE: 2017.4051895, Time: 12.35504674911499\n",
      "ToTal Epoch: 2138, LR: 0.000000, Loss: 209850.7112406, Val MAE: 3138.4759946, Test MAE: 2017.4051895, Time: 12.289984464645386\n",
      "ToTal Epoch: 2139, LR: 0.000000, Loss: 209981.5071705, Val MAE: 3133.0132185, Test MAE: 2017.4051895, Time: 12.034345149993896\n",
      "ToTal Epoch: 2140, LR: 0.000000, Loss: 209323.2634405, Val MAE: 3115.6518478, Test MAE: 2017.4051895, Time: 12.305081605911255\n",
      "ToTal Epoch: 2141, LR: 0.000000, Loss: 210114.0265418, Val MAE: 3133.7068190, Test MAE: 2017.4051895, Time: 11.069984436035156\n",
      "ToTal Epoch: 2142, LR: 0.000000, Loss: 210906.9363015, Val MAE: 3034.4745232, Test MAE: 2017.4051895, Time: 10.861346960067749\n",
      "ToTal Epoch: 2143, LR: 0.000000, Loss: 210276.3946687, Val MAE: 3055.8379338, Test MAE: 2017.4051895, Time: 10.91387677192688\n",
      "ToTal Epoch: 2144, LR: 0.000000, Loss: 210366.2459084, Val MAE: 3009.2251156, Test MAE: 2017.4051895, Time: 11.204923629760742\n",
      "ToTal Epoch: 2145, LR: 0.000000, Loss: 210853.8778770, Val MAE: 3021.8092563, Test MAE: 2017.4051895, Time: 11.904480934143066\n",
      "ToTal Epoch: 2146, LR: 0.000000, Loss: 208699.0000096, Val MAE: 3085.1091111, Test MAE: 2017.4051895, Time: 12.65099287033081\n",
      "ToTal Epoch: 2147, LR: 0.000000, Loss: 210940.3240052, Val MAE: 3079.3024966, Test MAE: 2017.4051895, Time: 11.181035041809082\n",
      "ToTal Epoch: 2148, LR: 0.000000, Loss: 211260.4548990, Val MAE: 3189.5509679, Test MAE: 2017.4051895, Time: 11.373732805252075\n",
      "ToTal Epoch: 2149, LR: 0.000000, Loss: 210344.4357139, Val MAE: 3108.2960569, Test MAE: 2017.4051895, Time: 13.624583721160889\n",
      "ToTal Epoch: 2150, LR: 0.000000, Loss: 210241.6952372, Val MAE: 3197.1522252, Test MAE: 2017.4051895, Time: 11.648654460906982\n",
      "ToTal Epoch: 2151, LR: 0.000000, Loss: 213564.6051689, Val MAE: 3031.6387583, Test MAE: 2017.4051895, Time: 12.049804210662842\n",
      "ToTal Epoch: 2152, LR: 0.000000, Loss: 209764.6745330, Val MAE: 3101.2439282, Test MAE: 2017.4051895, Time: 12.10026216506958\n",
      "ToTal Epoch: 2153, LR: 0.000000, Loss: 211593.7007405, Val MAE: 3066.0518278, Test MAE: 2017.4051895, Time: 12.946077823638916\n",
      "ToTal Epoch: 2154, LR: 0.000000, Loss: 208488.6224430, Val MAE: 3092.2907399, Test MAE: 2017.4051895, Time: 11.085718631744385\n",
      "ToTal Epoch: 2155, LR: 0.000000, Loss: 208846.5995318, Val MAE: 3177.0068935, Test MAE: 2017.4051895, Time: 11.95828104019165\n",
      "ToTal Epoch: 2156, LR: 0.000000, Loss: 210071.0902403, Val MAE: 3133.1015822, Test MAE: 2017.4051895, Time: 12.009677648544312\n",
      "ToTal Epoch: 2157, LR: 0.000000, Loss: 209442.6871638, Val MAE: 3132.0191852, Test MAE: 2017.4051895, Time: 10.946837663650513\n",
      "ToTal Epoch: 2158, LR: 0.000000, Loss: 210040.4007452, Val MAE: 3174.1896450, Test MAE: 2017.4051895, Time: 11.80647349357605\n",
      "ToTal Epoch: 2159, LR: 0.000000, Loss: 210765.5687384, Val MAE: 3062.5108586, Test MAE: 2017.4051895, Time: 11.893119096755981\n",
      "ToTal Epoch: 2160, LR: 0.000000, Loss: 210500.8357713, Val MAE: 3078.3619965, Test MAE: 2017.4051895, Time: 11.28958535194397\n",
      "ToTal Epoch: 2161, LR: 0.000000, Loss: 209354.9039698, Val MAE: 3123.0535571, Test MAE: 2017.4051895, Time: 11.839101314544678\n",
      "ToTal Epoch: 2162, LR: 0.000000, Loss: 209807.0379592, Val MAE: 3139.4897386, Test MAE: 2017.4051895, Time: 11.529569387435913\n",
      "ToTal Epoch: 2163, LR: 0.000000, Loss: 211110.0494721, Val MAE: 3180.1378650, Test MAE: 2017.4051895, Time: 11.398197889328003\n",
      "ToTal Epoch: 2164, LR: 0.000000, Loss: 209206.9704295, Val MAE: 3106.8424434, Test MAE: 2017.4051895, Time: 11.543089389801025\n",
      "ToTal Epoch: 2165, LR: 0.000000, Loss: 209307.6597143, Val MAE: 3129.2806266, Test MAE: 2017.4051895, Time: 12.928516864776611\n",
      "ToTal Epoch: 2166, LR: 0.000000, Loss: 210100.6463288, Val MAE: 3180.8598372, Test MAE: 2017.4051895, Time: 12.07979679107666\n",
      "ToTal Epoch: 2167, LR: 0.000000, Loss: 210501.9172407, Val MAE: 3148.2638682, Test MAE: 2017.4051895, Time: 12.18892502784729\n",
      "ToTal Epoch: 2168, LR: 0.000000, Loss: 210025.1699422, Val MAE: 3169.7252637, Test MAE: 2017.4051895, Time: 11.162809610366821\n",
      "ToTal Epoch: 2169, LR: 0.000000, Loss: 208716.5863278, Val MAE: 3175.4850665, Test MAE: 2017.4051895, Time: 11.143393516540527\n",
      "ToTal Epoch: 2170, LR: 0.000000, Loss: 209398.9378302, Val MAE: 3027.1333649, Test MAE: 2017.4051895, Time: 11.781921148300171\n",
      "ToTal Epoch: 2171, LR: 0.000000, Loss: 210195.4396599, Val MAE: 3072.2496990, Test MAE: 2017.4051895, Time: 11.299274206161499\n",
      "ToTal Epoch: 2172, LR: 0.000000, Loss: 209472.2343859, Val MAE: 3044.9442358, Test MAE: 2017.4051895, Time: 11.673510074615479\n",
      "ToTal Epoch: 2173, LR: 0.000000, Loss: 210233.5892610, Val MAE: 3012.8268316, Test MAE: 2017.4051895, Time: 12.472187280654907\n",
      "ToTal Epoch: 2174, LR: 0.000000, Loss: 211731.9367506, Val MAE: 3136.5483404, Test MAE: 2017.4051895, Time: 11.016778945922852\n",
      "ToTal Epoch: 2175, LR: 0.000000, Loss: 210148.7063297, Val MAE: 3139.6542078, Test MAE: 2017.4051895, Time: 11.522084951400757\n",
      "ToTal Epoch: 2176, LR: 0.000000, Loss: 209966.5514546, Val MAE: 3075.7281205, Test MAE: 2017.4051895, Time: 12.157477140426636\n",
      "ToTal Epoch: 2177, LR: 0.000000, Loss: 211795.7230402, Val MAE: 3093.6969732, Test MAE: 2017.4051895, Time: 11.144503831863403\n",
      "ToTal Epoch: 2178, LR: 0.000000, Loss: 210277.2690011, Val MAE: 3092.1071381, Test MAE: 2017.4051895, Time: 11.276289701461792\n",
      "ToTal Epoch: 2179, LR: 0.000000, Loss: 210215.7009124, Val MAE: 3126.9376099, Test MAE: 2017.4051895, Time: 11.759517669677734\n",
      "ToTal Epoch: 2180, LR: 0.000000, Loss: 211348.4410261, Val MAE: 3085.2853034, Test MAE: 2017.4051895, Time: 11.984687328338623\n",
      "ToTal Epoch: 2181, LR: 0.000000, Loss: 209597.6964984, Val MAE: 3106.0315868, Test MAE: 2017.4051895, Time: 11.327646493911743\n",
      "ToTal Epoch: 2182, LR: 0.000000, Loss: 209491.7687097, Val MAE: 3066.2553361, Test MAE: 2017.4051895, Time: 10.95966100692749\n",
      "ToTal Epoch: 2183, LR: 0.000000, Loss: 210545.7293269, Val MAE: 3118.0409931, Test MAE: 2017.4051895, Time: 13.79494571685791\n",
      "ToTal Epoch: 2184, LR: 0.000000, Loss: 207714.6539531, Val MAE: 3096.0066069, Test MAE: 2017.4051895, Time: 11.240428686141968\n",
      "ToTal Epoch: 2185, LR: 0.000000, Loss: 209381.7527540, Val MAE: 3094.6039039, Test MAE: 2017.4051895, Time: 13.188462018966675\n",
      "ToTal Epoch: 2186, LR: 0.000000, Loss: 211877.7554292, Val MAE: 3065.9804135, Test MAE: 2017.4051895, Time: 12.159674644470215\n",
      "ToTal Epoch: 2187, LR: 0.000000, Loss: 208895.7555248, Val MAE: 3071.6197833, Test MAE: 2017.4051895, Time: 11.07071590423584\n",
      "ToTal Epoch: 2188, LR: 0.000000, Loss: 209635.1118712, Val MAE: 3031.2213368, Test MAE: 2017.4051895, Time: 12.966300010681152\n",
      "ToTal Epoch: 2189, LR: 0.000000, Loss: 210247.4626666, Val MAE: 3068.9591837, Test MAE: 2017.4051895, Time: 12.10984492301941\n",
      "ToTal Epoch: 2190, LR: 0.000000, Loss: 209952.4536760, Val MAE: 3138.7915902, Test MAE: 2017.4051895, Time: 11.133153200149536\n",
      "ToTal Epoch: 2191, LR: 0.000000, Loss: 210175.1589548, Val MAE: 3039.9714658, Test MAE: 2017.4051895, Time: 11.975414752960205\n",
      "ToTal Epoch: 2192, LR: 0.000000, Loss: 210418.6810108, Val MAE: 3046.6267676, Test MAE: 2017.4051895, Time: 12.579990148544312\n",
      "ToTal Epoch: 2193, LR: 0.000000, Loss: 210192.0558735, Val MAE: 3092.5815419, Test MAE: 2017.4051895, Time: 11.815240383148193\n",
      "ToTal Epoch: 2194, LR: 0.000000, Loss: 210756.0904218, Val MAE: 3118.1040998, Test MAE: 2017.4051895, Time: 12.17906641960144\n",
      "ToTal Epoch: 2195, LR: 0.000000, Loss: 209599.1522285, Val MAE: 3052.8815543, Test MAE: 2017.4051895, Time: 11.738801002502441\n",
      "ToTal Epoch: 2196, LR: 0.000000, Loss: 209894.0780204, Val MAE: 3054.6621427, Test MAE: 2017.4051895, Time: 10.904313325881958\n",
      "ToTal Epoch: 2197, LR: 0.000000, Loss: 210844.0648545, Val MAE: 3045.5454789, Test MAE: 2017.4051895, Time: 11.278449535369873\n",
      "ToTal Epoch: 2198, LR: 0.000000, Loss: 210921.4785841, Val MAE: 3069.6873949, Test MAE: 2017.4051895, Time: 10.974241733551025\n",
      "ToTal Epoch: 2199, LR: 0.000000, Loss: 208630.0319304, Val MAE: 3064.4450097, Test MAE: 2017.4051895, Time: 12.266929388046265\n",
      "ToTal Epoch: 2200, LR: 0.000000, Loss: 209427.5263746, Val MAE: 3112.4932212, Test MAE: 2017.4051895, Time: 11.443256855010986\n",
      "ToTal Epoch: 2201, LR: 0.000000, Loss: 209611.7964936, Val MAE: 3092.4095009, Test MAE: 2017.4051895, Time: 11.116033792495728\n",
      "ToTal Epoch: 2202, LR: 0.000000, Loss: 210437.2626953, Val MAE: 3145.0322508, Test MAE: 2017.4051895, Time: 10.92969799041748\n",
      "ToTal Epoch: 2203, LR: 0.000000, Loss: 209807.4417618, Val MAE: 3126.0817330, Test MAE: 2017.4051895, Time: 11.91759729385376\n",
      "ToTal Epoch: 2204, LR: 0.000000, Loss: 210876.1493909, Val MAE: 3040.7798049, Test MAE: 2017.4051895, Time: 12.081148862838745\n",
      "ToTal Epoch: 2205, LR: 0.000000, Loss: 210087.3856590, Val MAE: 3101.8677578, Test MAE: 2017.4051895, Time: 11.139593124389648\n",
      "ToTal Epoch: 2206, LR: 0.000000, Loss: 209644.5473081, Val MAE: 3121.7261188, Test MAE: 2017.4051895, Time: 11.340800523757935\n",
      "ToTal Epoch: 2207, LR: 0.000000, Loss: 211179.1503559, Val MAE: 3140.7277048, Test MAE: 2017.4051895, Time: 11.391665935516357\n",
      "ToTal Epoch: 2208, LR: 0.000000, Loss: 210775.4905651, Val MAE: 3138.4114882, Test MAE: 2017.4051895, Time: 11.184990882873535\n",
      "ToTal Epoch: 2209, LR: 0.000000, Loss: 209937.3165433, Val MAE: 3157.5053791, Test MAE: 2017.4051895, Time: 11.321484804153442\n",
      "ToTal Epoch: 2210, LR: 0.000000, Loss: 209699.4080925, Val MAE: 3140.8396249, Test MAE: 2017.4051895, Time: 11.334651470184326\n",
      "ToTal Epoch: 2211, LR: 0.000000, Loss: 209937.1643625, Val MAE: 3192.9171683, Test MAE: 2017.4051895, Time: 11.986982345581055\n",
      "ToTal Epoch: 2212, LR: 0.000000, Loss: 209214.8537907, Val MAE: 3179.5784702, Test MAE: 2017.4051895, Time: 12.515077352523804\n",
      "ToTal Epoch: 2213, LR: 0.000000, Loss: 208174.4544977, Val MAE: 3199.9419810, Test MAE: 2017.4051895, Time: 10.998510360717773\n",
      "ToTal Epoch: 2214, LR: 0.000000, Loss: 208037.0076339, Val MAE: 3153.5915310, Test MAE: 2017.4051895, Time: 11.867379426956177\n",
      "ToTal Epoch: 2215, LR: 0.000000, Loss: 209316.1672383, Val MAE: 3192.6281482, Test MAE: 2017.4051895, Time: 11.97843050956726\n",
      "ToTal Epoch: 2216, LR: 0.000000, Loss: 209881.8133665, Val MAE: 3159.5362923, Test MAE: 2017.4051895, Time: 11.927654266357422\n",
      "ToTal Epoch: 2217, LR: 0.000000, Loss: 210609.6849950, Val MAE: 3132.3043358, Test MAE: 2017.4051895, Time: 13.437481880187988\n",
      "ToTal Epoch: 2218, LR: 0.000000, Loss: 210173.6793771, Val MAE: 3109.5849385, Test MAE: 2017.4051895, Time: 11.564888000488281\n",
      "ToTal Epoch: 2219, LR: 0.000000, Loss: 211983.8304686, Val MAE: 3059.6188183, Test MAE: 2017.4051895, Time: 11.187501192092896\n",
      "ToTal Epoch: 2220, LR: 0.000000, Loss: 209595.3626905, Val MAE: 3066.5185881, Test MAE: 2017.4051895, Time: 11.114258050918579\n",
      "ToTal Epoch: 2221, LR: 0.000000, Loss: 210918.4526633, Val MAE: 3064.3004328, Test MAE: 2017.4051895, Time: 10.850807189941406\n",
      "ToTal Epoch: 2222, LR: 0.000000, Loss: 209562.7548846, Val MAE: 3032.3936406, Test MAE: 2017.4051895, Time: 11.3589346408844\n",
      "ToTal Epoch: 2223, LR: 0.000000, Loss: 209840.9727798, Val MAE: 3012.3026303, Test MAE: 2017.4051895, Time: 11.648948431015015\n",
      "ToTal Epoch: 2224, LR: 0.000000, Loss: 207974.7144509, Val MAE: 2993.1809696, Test MAE: 2017.4051895, Time: 11.143140077590942\n",
      "ToTal Epoch: 2225, LR: 0.000000, Loss: 211116.7225338, Val MAE: 2932.9765583, Test MAE: 2017.4051895, Time: 12.463298320770264\n",
      "ToTal Epoch: 2226, LR: 0.000000, Loss: 209698.4886256, Val MAE: 3044.8647004, Test MAE: 2017.4051895, Time: 13.924038410186768\n",
      "ToTal Epoch: 2227, LR: 0.000000, Loss: 208881.6806382, Val MAE: 3067.8083677, Test MAE: 2017.4051895, Time: 11.485460758209229\n",
      "ToTal Epoch: 2228, LR: 0.000000, Loss: 209918.0454975, Val MAE: 3041.6854601, Test MAE: 2017.4051895, Time: 10.77896523475647\n",
      "ToTal Epoch: 2229, LR: 0.000000, Loss: 209217.0955334, Val MAE: 3050.7044972, Test MAE: 2017.4051895, Time: 13.923958778381348\n",
      "ToTal Epoch: 2230, LR: 0.000000, Loss: 211706.5915827, Val MAE: 3036.6379414, Test MAE: 2017.4051895, Time: 10.647778034210205\n",
      "ToTal Epoch: 2231, LR: 0.000000, Loss: 208672.9295944, Val MAE: 3139.7129672, Test MAE: 2017.4051895, Time: 11.49526834487915\n",
      "ToTal Epoch: 2232, LR: 0.000000, Loss: 210838.5362442, Val MAE: 3099.6970639, Test MAE: 2017.4051895, Time: 11.14136552810669\n",
      "ToTal Epoch: 2233, LR: 0.000000, Loss: 210167.3876845, Val MAE: 3070.6518383, Test MAE: 2017.4051895, Time: 12.06002140045166\n",
      "ToTal Epoch: 2234, LR: 0.000000, Loss: 209163.4266660, Val MAE: 3036.7376366, Test MAE: 2017.4051895, Time: 11.10299825668335\n",
      "ToTal Epoch: 2235, LR: 0.000000, Loss: 209862.5921177, Val MAE: 3057.6403300, Test MAE: 2017.4051895, Time: 11.211960077285767\n",
      "ToTal Epoch: 2236, LR: 0.000000, Loss: 209944.1585248, Val MAE: 3059.6096748, Test MAE: 2017.4051895, Time: 11.030050039291382\n",
      "ToTal Epoch: 2237, LR: 0.000000, Loss: 210897.5948789, Val MAE: 3118.3473257, Test MAE: 2017.4051895, Time: 10.82473111152649\n",
      "ToTal Epoch: 2238, LR: 0.000000, Loss: 210424.6207233, Val MAE: 3095.3865321, Test MAE: 2017.4051895, Time: 10.654134273529053\n",
      "ToTal Epoch: 2239, LR: 0.000000, Loss: 209421.6660393, Val MAE: 3109.4223229, Test MAE: 2017.4051895, Time: 11.165225267410278\n",
      "ToTal Epoch: 2240, LR: 0.000000, Loss: 209129.6170831, Val MAE: 3127.1938680, Test MAE: 2017.4051895, Time: 12.411516666412354\n",
      "ToTal Epoch: 2241, LR: 0.000000, Loss: 209643.7662256, Val MAE: 3141.8237981, Test MAE: 2017.4051895, Time: 12.494067668914795\n",
      "ToTal Epoch: 2242, LR: 0.000000, Loss: 208597.6480772, Val MAE: 3038.7031644, Test MAE: 2017.4051895, Time: 12.11850094795227\n",
      "ToTal Epoch: 2243, LR: 0.000000, Loss: 209444.4020828, Val MAE: 3049.5112407, Test MAE: 2017.4051895, Time: 10.669148445129395\n",
      "ToTal Epoch: 2244, LR: 0.000000, Loss: 208929.5253237, Val MAE: 3095.7919724, Test MAE: 2017.4051895, Time: 10.685602903366089\n",
      "ToTal Epoch: 2245, LR: 0.000000, Loss: 209850.2879473, Val MAE: 3160.7808559, Test MAE: 2017.4051895, Time: 11.597289323806763\n",
      "ToTal Epoch: 2246, LR: 0.000000, Loss: 210010.4513257, Val MAE: 3073.0664603, Test MAE: 2017.4051895, Time: 11.803346395492554\n",
      "ToTal Epoch: 2247, LR: 0.000000, Loss: 208936.8578226, Val MAE: 3071.5102757, Test MAE: 2017.4051895, Time: 11.044612169265747\n",
      "ToTal Epoch: 2248, LR: 0.000000, Loss: 210748.9336072, Val MAE: 3026.8374131, Test MAE: 2017.4051895, Time: 12.137439012527466\n",
      "ToTal Epoch: 2249, LR: 0.000000, Loss: 210896.0435676, Val MAE: 3021.4694642, Test MAE: 2017.4051895, Time: 10.72292160987854\n",
      "ToTal Epoch: 2250, LR: 0.000000, Loss: 210000.2923231, Val MAE: 3023.8106273, Test MAE: 2017.4051895, Time: 11.050763607025146\n",
      "ToTal Epoch: 2251, LR: 0.000000, Loss: 208645.4460230, Val MAE: 3060.5538103, Test MAE: 2017.4051895, Time: 12.024771690368652\n",
      "ToTal Epoch: 2252, LR: 0.000000, Loss: 210252.4582239, Val MAE: 3052.9890172, Test MAE: 2017.4051895, Time: 11.904502868652344\n",
      "ToTal Epoch: 2253, LR: 0.000000, Loss: 209788.4603640, Val MAE: 3106.9772080, Test MAE: 2017.4051895, Time: 11.367426633834839\n",
      "ToTal Epoch: 2254, LR: 0.000000, Loss: 210969.6377968, Val MAE: 3148.1609054, Test MAE: 2017.4051895, Time: 10.40737009048462\n",
      "ToTal Epoch: 2255, LR: 0.000000, Loss: 210561.8363350, Val MAE: 3138.0801278, Test MAE: 2017.4051895, Time: 10.977016687393188\n",
      "ToTal Epoch: 2256, LR: 0.000000, Loss: 209770.8267329, Val MAE: 3129.5277459, Test MAE: 2017.4051895, Time: 11.456417560577393\n",
      "ToTal Epoch: 2257, LR: 0.000000, Loss: 210190.1901495, Val MAE: 3159.0539011, Test MAE: 2017.4051895, Time: 12.44612455368042\n",
      "ToTal Epoch: 2258, LR: 0.000000, Loss: 211218.3374385, Val MAE: 3148.4957340, Test MAE: 2017.4051895, Time: 11.533429384231567\n",
      "ToTal Epoch: 2259, LR: 0.000000, Loss: 210678.2164429, Val MAE: 3169.9981369, Test MAE: 2017.4051895, Time: 11.033138275146484\n",
      "ToTal Epoch: 2260, LR: 0.000000, Loss: 211474.0626188, Val MAE: 3127.1915511, Test MAE: 2017.4051895, Time: 11.187711715698242\n",
      "ToTal Epoch: 2261, LR: 0.000000, Loss: 210620.6815650, Val MAE: 3104.6564053, Test MAE: 2017.4051895, Time: 11.054091453552246\n",
      "ToTal Epoch: 2262, LR: 0.000000, Loss: 211556.6417427, Val MAE: 3114.7476114, Test MAE: 2017.4051895, Time: 11.644653081893921\n",
      "ToTal Epoch: 2263, LR: 0.000000, Loss: 209860.5663020, Val MAE: 3149.3034090, Test MAE: 2017.4051895, Time: 11.087321996688843\n",
      "ToTal Epoch: 2264, LR: 0.000000, Loss: 210590.6393923, Val MAE: 3113.6031300, Test MAE: 2017.4051895, Time: 11.522818803787231\n",
      "ToTal Epoch: 2265, LR: 0.000000, Loss: 210434.7191134, Val MAE: 3068.9972627, Test MAE: 2017.4051895, Time: 10.788879632949829\n",
      "ToTal Epoch: 2266, LR: 0.000000, Loss: 208578.0793962, Val MAE: 3100.4264265, Test MAE: 2017.4051895, Time: 11.482910871505737\n",
      "ToTal Epoch: 2267, LR: 0.000000, Loss: 208411.9896431, Val MAE: 3078.5712614, Test MAE: 2017.4051895, Time: 11.01878571510315\n",
      "ToTal Epoch: 2268, LR: 0.000000, Loss: 210381.4947881, Val MAE: 3169.3769252, Test MAE: 2017.4051895, Time: 11.785098552703857\n",
      "ToTal Epoch: 2269, LR: 0.000000, Loss: 210064.3071514, Val MAE: 3181.6446820, Test MAE: 2017.4051895, Time: 10.974908828735352\n",
      "ToTal Epoch: 2270, LR: 0.000000, Loss: 209675.0196532, Val MAE: 3217.7426336, Test MAE: 2017.4051895, Time: 10.672743558883667\n",
      "ToTal Epoch: 2271, LR: 0.000000, Loss: 209949.9486935, Val MAE: 3203.1381803, Test MAE: 2017.4051895, Time: 10.657539367675781\n",
      "ToTal Epoch: 2272, LR: 0.000000, Loss: 208269.7702575, Val MAE: 3147.7148351, Test MAE: 2017.4051895, Time: 11.078932046890259\n",
      "ToTal Epoch: 2273, LR: 0.000000, Loss: 209789.4336789, Val MAE: 3095.5860038, Test MAE: 2017.4051895, Time: 11.547762870788574\n",
      "ToTal Epoch: 2274, LR: 0.000000, Loss: 209209.9882864, Val MAE: 3126.7556419, Test MAE: 2017.4051895, Time: 10.670513153076172\n",
      "ToTal Epoch: 2275, LR: 0.000000, Loss: 209779.4046912, Val MAE: 3107.6259745, Test MAE: 2017.4051895, Time: 11.632551670074463\n",
      "ToTal Epoch: 2276, LR: 0.000000, Loss: 210827.9060240, Val MAE: 3136.2706518, Test MAE: 2017.4051895, Time: 11.00473690032959\n",
      "ToTal Epoch: 2277, LR: 0.000000, Loss: 208788.4111785, Val MAE: 3063.5985248, Test MAE: 2017.4051895, Time: 12.026460647583008\n",
      "ToTal Epoch: 2278, LR: 0.000000, Loss: 209550.6491759, Val MAE: 3193.5428849, Test MAE: 2017.4051895, Time: 11.049826383590698\n",
      "ToTal Epoch: 2279, LR: 0.000000, Loss: 210225.1562987, Val MAE: 3173.9482391, Test MAE: 2017.4051895, Time: 11.673067092895508\n",
      "ToTal Epoch: 2280, LR: 0.000000, Loss: 211575.3521808, Val MAE: 3181.6480261, Test MAE: 2017.4051895, Time: 10.800300121307373\n",
      "ToTal Epoch: 2281, LR: 0.000000, Loss: 208710.8574977, Val MAE: 3146.9006726, Test MAE: 2017.4051895, Time: 10.944653034210205\n",
      "ToTal Epoch: 2282, LR: 0.000000, Loss: 209593.4081880, Val MAE: 3070.4032332, Test MAE: 2017.4051895, Time: 10.915921926498413\n",
      "ToTal Epoch: 2283, LR: 0.000000, Loss: 211200.1024602, Val MAE: 3034.8148552, Test MAE: 2017.4051895, Time: 10.911497116088867\n",
      "ToTal Epoch: 2284, LR: 0.000000, Loss: 210876.2176086, Val MAE: 3021.1995767, Test MAE: 2017.4051895, Time: 12.046138763427734\n",
      "ToTal Epoch: 2285, LR: 0.000000, Loss: 209297.2638800, Val MAE: 3023.3643182, Test MAE: 2017.4051895, Time: 12.741516590118408\n",
      "ToTal Epoch: 2286, LR: 0.000000, Loss: 209678.9453972, Val MAE: 3007.3805272, Test MAE: 2017.4051895, Time: 10.465188026428223\n",
      "ToTal Epoch: 2287, LR: 0.000000, Loss: 210099.1027373, Val MAE: 3080.7306380, Test MAE: 2017.4051895, Time: 10.545211791992188\n",
      "ToTal Epoch: 2288, LR: 0.000000, Loss: 210389.8333158, Val MAE: 3066.1531042, Test MAE: 2017.4051895, Time: 11.147139072418213\n",
      "ToTal Epoch: 2289, LR: 0.000000, Loss: 208115.9524961, Val MAE: 3101.0812839, Test MAE: 2017.4051895, Time: 11.466514110565186\n",
      "ToTal Epoch: 2290, LR: 0.000000, Loss: 209017.8174939, Val MAE: 3064.5458276, Test MAE: 2017.4051895, Time: 11.393288135528564\n",
      "ToTal Epoch: 2291, LR: 0.000000, Loss: 209747.2241915, Val MAE: 3011.0850913, Test MAE: 2017.4051895, Time: 11.867300033569336\n",
      "ToTal Epoch: 2292, LR: 0.000000, Loss: 210325.0589213, Val MAE: 3057.1053084, Test MAE: 2017.4051895, Time: 11.050123691558838\n",
      "ToTal Epoch: 2293, LR: 0.000000, Loss: 210761.9441456, Val MAE: 3069.3066814, Test MAE: 2017.4051895, Time: 11.137766122817993\n",
      "ToTal Epoch: 2294, LR: 0.000000, Loss: 209635.3164095, Val MAE: 3083.8282838, Test MAE: 2017.4051895, Time: 11.512992143630981\n",
      "ToTal Epoch: 2295, LR: 0.000000, Loss: 209036.6469785, Val MAE: 3203.8641988, Test MAE: 2017.4051895, Time: 12.185089826583862\n",
      "ToTal Epoch: 2296, LR: 0.000000, Loss: 209920.7675918, Val MAE: 3097.6669055, Test MAE: 2017.4051895, Time: 11.864060401916504\n",
      "ToTal Epoch: 2297, LR: 0.000000, Loss: 209963.8449147, Val MAE: 3090.6221146, Test MAE: 2017.4051895, Time: 11.736487865447998\n",
      "ToTal Epoch: 2298, LR: 0.000000, Loss: 209259.6433192, Val MAE: 3027.0533469, Test MAE: 2017.4051895, Time: 11.348766326904297\n",
      "ToTal Epoch: 2299, LR: 0.000000, Loss: 207220.3676109, Val MAE: 3026.7947575, Test MAE: 2017.4051895, Time: 11.238819360733032\n",
      "ToTal Epoch: 2300, LR: 0.000000, Loss: 210378.6833421, Val MAE: 3085.2784482, Test MAE: 2017.4051895, Time: 11.179621696472168\n",
      "ToTal Epoch: 2301, LR: 0.000000, Loss: 209980.3712416, Val MAE: 3134.9908899, Test MAE: 2017.4051895, Time: 11.467458963394165\n",
      "ToTal Epoch: 2302, LR: 0.000000, Loss: 209623.6920461, Val MAE: 3057.4649259, Test MAE: 2017.4051895, Time: 12.088266134262085\n",
      "ToTal Epoch: 2303, LR: 0.000000, Loss: 211385.4211819, Val MAE: 2999.1381516, Test MAE: 2017.4051895, Time: 11.327740669250488\n",
      "ToTal Epoch: 2304, LR: 0.000000, Loss: 210284.5799838, Val MAE: 3125.6896402, Test MAE: 2017.4051895, Time: 12.164631366729736\n",
      "ToTal Epoch: 2305, LR: 0.000000, Loss: 209258.4804854, Val MAE: 3092.7802922, Test MAE: 2017.4051895, Time: 10.72066044807434\n",
      "ToTal Epoch: 2306, LR: 0.000000, Loss: 210611.7416519, Val MAE: 3046.1071476, Test MAE: 2017.4051895, Time: 11.189147472381592\n",
      "ToTal Epoch: 2307, LR: 0.000000, Loss: 209389.9991401, Val MAE: 3050.0045049, Test MAE: 2017.4051895, Time: 12.128042697906494\n",
      "ToTal Epoch: 2308, LR: 0.000000, Loss: 209791.5209860, Val MAE: 3093.7577343, Test MAE: 2017.4051895, Time: 10.352725744247437\n",
      "ToTal Epoch: 2309, LR: 0.000000, Loss: 210030.4018344, Val MAE: 3061.8992681, Test MAE: 2017.4051895, Time: 11.704168796539307\n",
      "ToTal Epoch: 2310, LR: 0.000000, Loss: 210785.9121196, Val MAE: 3063.6188183, Test MAE: 2017.4051895, Time: 10.74951171875\n",
      "ToTal Epoch: 2311, LR: 0.000000, Loss: 210056.6656667, Val MAE: 3048.8372554, Test MAE: 2017.4051895, Time: 10.834814310073853\n",
      "ToTal Epoch: 2312, LR: 0.000000, Loss: 211666.8604022, Val MAE: 3092.5005685, Test MAE: 2017.4051895, Time: 13.53802490234375\n",
      "ToTal Epoch: 2313, LR: 0.000000, Loss: 210068.9756843, Val MAE: 3141.6630694, Test MAE: 2017.4051895, Time: 11.974790096282959\n",
      "ToTal Epoch: 2314, LR: 0.000000, Loss: 209694.0037262, Val MAE: 3095.9565323, Test MAE: 2017.4051895, Time: 11.84661054611206\n",
      "ToTal Epoch: 2315, LR: 0.000000, Loss: 210508.7054507, Val MAE: 3069.2339343, Test MAE: 2017.4051895, Time: 10.878958702087402\n",
      "ToTal Epoch: 2316, LR: 0.000000, Loss: 210576.1344480, Val MAE: 3037.9300380, Test MAE: 2017.4051895, Time: 11.32528042793274\n",
      "ToTal Epoch: 2317, LR: 0.000000, Loss: 210440.3451393, Val MAE: 3086.1183788, Test MAE: 2017.4051895, Time: 10.888836860656738\n",
      "ToTal Epoch: 2318, LR: 0.000000, Loss: 209037.5597573, Val MAE: 3145.7487149, Test MAE: 2017.4051895, Time: 10.504799365997314\n",
      "ToTal Epoch: 2319, LR: 0.000000, Loss: 210378.1654230, Val MAE: 3147.3451569, Test MAE: 2017.4051895, Time: 11.24715256690979\n",
      "ToTal Epoch: 2320, LR: 0.000000, Loss: 210841.8876224, Val MAE: 3142.1671778, Test MAE: 2017.4051895, Time: 11.802074432373047\n",
      "ToTal Epoch: 2321, LR: 0.000000, Loss: 209112.3139923, Val MAE: 3015.2013156, Test MAE: 2017.4051895, Time: 12.081358909606934\n",
      "ToTal Epoch: 2322, LR: 0.000000, Loss: 210439.4349209, Val MAE: 3028.6668769, Test MAE: 2017.4051895, Time: 12.072202682495117\n",
      "ToTal Epoch: 2323, LR: 0.000000, Loss: 210682.3368652, Val MAE: 3049.0316393, Test MAE: 2017.4051895, Time: 11.81782078742981\n",
      "ToTal Epoch: 2324, LR: 0.000000, Loss: 209400.1535566, Val MAE: 3105.4571916, Test MAE: 2017.4051895, Time: 10.6992347240448\n",
      "ToTal Epoch: 2325, LR: 0.000000, Loss: 212532.9980796, Val MAE: 3116.4914775, Test MAE: 2017.4051895, Time: 12.350878715515137\n",
      "ToTal Epoch: 2326, LR: 0.000000, Loss: 211140.9410214, Val MAE: 3038.7809419, Test MAE: 2017.4051895, Time: 11.268403768539429\n",
      "ToTal Epoch: 2327, LR: 0.000000, Loss: 209703.7340849, Val MAE: 3007.7981732, Test MAE: 2017.4051895, Time: 12.130125284194946\n",
      "ToTal Epoch: 2328, LR: 0.000000, Loss: 209617.7907419, Val MAE: 2986.8035045, Test MAE: 2017.4051895, Time: 11.250778436660767\n",
      "ToTal Epoch: 2329, LR: 0.000000, Loss: 209843.5482348, Val MAE: 3006.9922323, Test MAE: 2017.4051895, Time: 11.226577997207642\n",
      "ToTal Epoch: 2330, LR: 0.000000, Loss: 209309.4033727, Val MAE: 3032.1286880, Test MAE: 2017.4051895, Time: 11.5930335521698\n",
      "ToTal Epoch: 2331, LR: 0.000000, Loss: 211066.8144270, Val MAE: 3041.6026045, Test MAE: 2017.4051895, Time: 11.153021574020386\n",
      "ToTal Epoch: 2332, LR: 0.000000, Loss: 208527.4750872, Val MAE: 3100.4494716, Test MAE: 2017.4051895, Time: 11.364858150482178\n",
      "ToTal Epoch: 2333, LR: 0.000000, Loss: 209501.9944776, Val MAE: 3085.3421807, Test MAE: 2017.4051895, Time: 11.686840772628784\n",
      "ToTal Epoch: 2334, LR: 0.000000, Loss: 209228.1731238, Val MAE: 3060.4768067, Test MAE: 2017.4051895, Time: 11.66164255142212\n",
      "ToTal Epoch: 2335, LR: 0.000000, Loss: 210552.6705584, Val MAE: 3061.1115618, Test MAE: 2017.4051895, Time: 11.504704475402832\n",
      "ToTal Epoch: 2336, LR: 0.000000, Loss: 209839.2925620, Val MAE: 3037.8937170, Test MAE: 2017.4051895, Time: 11.13757848739624\n",
      "ToTal Epoch: 2337, LR: 0.000000, Loss: 209801.1076482, Val MAE: 3101.5560938, Test MAE: 2017.4051895, Time: 12.51507306098938\n",
      "ToTal Epoch: 2338, LR: 0.000000, Loss: 210839.7331295, Val MAE: 3018.6456136, Test MAE: 2017.4051895, Time: 11.136276245117188\n",
      "ToTal Epoch: 2339, LR: 0.000000, Loss: 208549.2794344, Val MAE: 3098.9140058, Test MAE: 2017.4051895, Time: 11.699604272842407\n",
      "ToTal Epoch: 2340, LR: 0.000000, Loss: 210265.2797783, Val MAE: 3099.4557919, Test MAE: 2017.4051895, Time: 11.794588565826416\n",
      "ToTal Epoch: 2341, LR: 0.000000, Loss: 208979.1605981, Val MAE: 3120.9432756, Test MAE: 2017.4051895, Time: 11.043203353881836\n",
      "ToTal Epoch: 2342, LR: 0.000000, Loss: 211313.3108871, Val MAE: 3012.8432269, Test MAE: 2017.4051895, Time: 11.263078927993774\n",
      "ToTal Epoch: 2343, LR: 0.000000, Loss: 210710.5936082, Val MAE: 3103.0932651, Test MAE: 2017.4051895, Time: 10.859525680541992\n",
      "ToTal Epoch: 2344, LR: 0.000000, Loss: 210792.6284049, Val MAE: 3117.1624150, Test MAE: 2017.4051895, Time: 14.674718618392944\n",
      "ToTal Epoch: 2345, LR: 0.000000, Loss: 211225.1812927, Val MAE: 3100.1910160, Test MAE: 2017.4051895, Time: 12.189472913742065\n",
      "ToTal Epoch: 2346, LR: 0.000000, Loss: 211795.6466058, Val MAE: 3123.2819355, Test MAE: 2017.4051895, Time: 13.713326692581177\n",
      "ToTal Epoch: 2347, LR: 0.000000, Loss: 209633.7658243, Val MAE: 3110.6834442, Test MAE: 2017.4051895, Time: 12.870272159576416\n",
      "ToTal Epoch: 2348, LR: 0.000000, Loss: 210661.6502938, Val MAE: 3127.1861385, Test MAE: 2017.4051895, Time: 12.580727577209473\n",
      "ToTal Epoch: 2349, LR: 0.000000, Loss: 209735.2915301, Val MAE: 3052.2495653, Test MAE: 2017.4051895, Time: 11.58943796157837\n",
      "ToTal Epoch: 2350, LR: 0.000000, Loss: 209151.5702862, Val MAE: 3061.1419829, Test MAE: 2017.4051895, Time: 14.166777610778809\n",
      "ToTal Epoch: 2351, LR: 0.000000, Loss: 209594.9077151, Val MAE: 3032.6026905, Test MAE: 2017.4051895, Time: 11.42275333404541\n",
      "ToTal Epoch: 2352, LR: 0.000000, Loss: 210096.3474705, Val MAE: 3066.9076664, Test MAE: 2017.4051895, Time: 10.773664712905884\n",
      "ToTal Epoch: 2353, LR: 0.000000, Loss: 209712.7907514, Val MAE: 3057.6428715, Test MAE: 2017.4051895, Time: 11.320633888244629\n",
      "ToTal Epoch: 2354, LR: 0.000000, Loss: 209911.2504467, Val MAE: 3107.8689664, Test MAE: 2017.4051895, Time: 11.915394067764282\n",
      "ToTal Epoch: 2355, LR: 0.000000, Loss: 210054.4489180, Val MAE: 3086.9430654, Test MAE: 2017.4051895, Time: 10.564233779907227\n",
      "ToTal Epoch: 2356, LR: 0.000000, Loss: 210353.0745139, Val MAE: 3007.5462241, Test MAE: 2017.4051895, Time: 11.679988622665405\n",
      "ToTal Epoch: 2357, LR: 0.000000, Loss: 209944.9120527, Val MAE: 2989.7036995, Test MAE: 2017.4051895, Time: 10.665467977523804\n",
      "ToTal Epoch: 2358, LR: 0.000000, Loss: 209139.5577891, Val MAE: 3086.2562199, Test MAE: 2017.4051895, Time: 11.650083780288696\n",
      "ToTal Epoch: 2359, LR: 0.000000, Loss: 208845.0894568, Val MAE: 3020.3152994, Test MAE: 2017.4051895, Time: 10.641855239868164\n",
      "ToTal Epoch: 2360, LR: 0.000000, Loss: 208623.1080495, Val MAE: 3032.0531606, Test MAE: 2017.4051895, Time: 11.036013841629028\n",
      "ToTal Epoch: 2361, LR: 0.000000, Loss: 208546.7411265, Val MAE: 3044.5099127, Test MAE: 2017.4051895, Time: 10.968873500823975\n",
      "ToTal Epoch: 2362, LR: 0.000000, Loss: 209505.7120527, Val MAE: 3049.7644176, Test MAE: 2017.4051895, Time: 11.579314231872559\n",
      "ToTal Epoch: 2363, LR: 0.000000, Loss: 211757.2214589, Val MAE: 3112.2224882, Test MAE: 2017.4051895, Time: 11.138405323028564\n",
      "ToTal Epoch: 2364, LR: 0.000000, Loss: 210987.5563369, Val MAE: 3108.3365770, Test MAE: 2017.4051895, Time: 11.185383081436157\n",
      "ToTal Epoch: 2365, LR: 0.000000, Loss: 210542.7695600, Val MAE: 3130.7836553, Test MAE: 2017.4051895, Time: 11.28130030632019\n",
      "ToTal Epoch: 2366, LR: 0.000000, Loss: 210696.4823007, Val MAE: 3083.7131440, Test MAE: 2017.4051895, Time: 11.142716884613037\n",
      "ToTal Epoch: 2367, LR: 0.000000, Loss: 211383.0651698, Val MAE: 3026.7075021, Test MAE: 2017.4051895, Time: 11.473477363586426\n",
      "ToTal Epoch: 2368, LR: 0.000000, Loss: 210671.1708403, Val MAE: 3021.5878143, Test MAE: 2017.4051895, Time: 10.708884954452515\n",
      "ToTal Epoch: 2369, LR: 0.000000, Loss: 209206.5574165, Val MAE: 3003.9020580, Test MAE: 2017.4051895, Time: 11.784180879592896\n",
      "ToTal Epoch: 2370, LR: 0.000000, Loss: 210340.8189175, Val MAE: 3030.2512946, Test MAE: 2017.4051895, Time: 11.756794691085815\n",
      "ToTal Epoch: 2371, LR: 0.000000, Loss: 208681.5350308, Val MAE: 3076.8191546, Test MAE: 2017.4051895, Time: 11.617424011230469\n",
      "ToTal Epoch: 2372, LR: 0.000000, Loss: 210009.9999045, Val MAE: 3039.5417335, Test MAE: 2017.4051895, Time: 10.271844387054443\n",
      "ToTal Epoch: 2373, LR: 0.000000, Loss: 209641.7046004, Val MAE: 3083.9926718, Test MAE: 2017.4051895, Time: 11.043354988098145\n",
      "ToTal Epoch: 2374, LR: 0.000000, Loss: 208986.1927483, Val MAE: 2955.7260854, Test MAE: 2017.4051895, Time: 10.37414264678955\n",
      "ToTal Epoch: 2375, LR: 0.000000, Loss: 211312.1032628, Val MAE: 2982.1204760, Test MAE: 2017.4051895, Time: 11.643530368804932\n",
      "ToTal Epoch: 2376, LR: 0.000000, Loss: 208872.3656619, Val MAE: 3018.9386465, Test MAE: 2017.4051895, Time: 10.373961448669434\n",
      "ToTal Epoch: 2377, LR: 0.000000, Loss: 210193.0922849, Val MAE: 3016.0374293, Test MAE: 2017.4051895, Time: 10.548171997070312\n",
      "ToTal Epoch: 2378, LR: 0.000000, Loss: 209328.5539961, Val MAE: 3083.8673852, Test MAE: 2017.4051895, Time: 10.727649450302124\n",
      "ToTal Epoch: 2379, LR: 0.000000, Loss: 209365.0528066, Val MAE: 3011.9021583, Test MAE: 2017.4051895, Time: 11.916690111160278\n",
      "ToTal Epoch: 2380, LR: 0.000000, Loss: 208955.6101467, Val MAE: 3084.2065419, Test MAE: 2017.4051895, Time: 11.051926851272583\n",
      "ToTal Epoch: 2381, LR: 0.000000, Loss: 211270.2584436, Val MAE: 3051.2467897, Test MAE: 2017.4051895, Time: 11.406717777252197\n",
      "ToTal Epoch: 2382, LR: 0.000000, Loss: 210240.4244781, Val MAE: 3103.3130542, Test MAE: 2017.4051895, Time: 11.362495183944702\n",
      "ToTal Epoch: 2383, LR: 0.000000, Loss: 210188.7717575, Val MAE: 3064.0694843, Test MAE: 2017.4051895, Time: 11.978419303894043\n",
      "ToTal Epoch: 2384, LR: 0.000000, Loss: 209569.9319543, Val MAE: 3103.8840767, Test MAE: 2017.4051895, Time: 11.496007442474365\n",
      "ToTal Epoch: 2385, LR: 0.000000, Loss: 207843.8666603, Val MAE: 3094.0389150, Test MAE: 2017.4051895, Time: 11.592466831207275\n",
      "ToTal Epoch: 2386, LR: 0.000000, Loss: 210460.2867816, Val MAE: 3083.5205180, Test MAE: 2017.4051895, Time: 10.738110542297363\n",
      "ToTal Epoch: 2387, LR: 0.000000, Loss: 209896.5148235, Val MAE: 3100.8194174, Test MAE: 2017.4051895, Time: 11.07943868637085\n",
      "ToTal Epoch: 2388, LR: 0.000000, Loss: 210536.9709072, Val MAE: 3096.1266577, Test MAE: 2017.4051895, Time: 10.765592336654663\n",
      "ToTal Epoch: 2389, LR: 0.000000, Loss: 209957.4272202, Val MAE: 3060.9050676, Test MAE: 2017.4051895, Time: 11.911557674407959\n",
      "ToTal Epoch: 2390, LR: 0.000000, Loss: 210771.9916687, Val MAE: 3064.6340719, Test MAE: 2017.4051895, Time: 11.682749509811401\n",
      "ToTal Epoch: 2391, LR: 0.000000, Loss: 209663.7625185, Val MAE: 3034.9005293, Test MAE: 2017.4051895, Time: 11.286994695663452\n",
      "ToTal Epoch: 2392, LR: 0.000000, Loss: 210619.7319830, Val MAE: 3051.6079741, Test MAE: 2017.4051895, Time: 12.113057851791382\n",
      "ToTal Epoch: 2393, LR: 0.000000, Loss: 212178.2257679, Val MAE: 3019.8673708, Test MAE: 2017.4051895, Time: 11.469216585159302\n",
      "ToTal Epoch: 2394, LR: 0.000000, Loss: 210710.8669756, Val MAE: 2992.3783440, Test MAE: 2017.4051895, Time: 13.562601089477539\n",
      "ToTal Epoch: 2395, LR: 0.000000, Loss: 210859.2234653, Val MAE: 3021.8518688, Test MAE: 2017.4051895, Time: 11.660506963729858\n",
      "ToTal Epoch: 2396, LR: 0.000000, Loss: 209290.9694358, Val MAE: 3030.1232515, Test MAE: 2017.4051895, Time: 11.565374612808228\n",
      "ToTal Epoch: 2397, LR: 0.000000, Loss: 210220.1999522, Val MAE: 3067.2930043, Test MAE: 2017.4051895, Time: 12.475841283798218\n",
      "ToTal Epoch: 2398, LR: 0.000000, Loss: 210543.3113266, Val MAE: 3012.1180444, Test MAE: 2017.4051895, Time: 11.259238719940186\n",
      "ToTal Epoch: 2399, LR: 0.000000, Loss: 210721.1912292, Val MAE: 3057.9226047, Test MAE: 2017.4051895, Time: 10.958528757095337\n",
      "ToTal Epoch: 2400, LR: 0.000000, Loss: 209717.1175082, Val MAE: 3022.0624236, Test MAE: 2017.4051895, Time: 11.684050559997559\n",
      "ToTal Epoch: 2401, LR: 0.000000, Loss: 208068.5992070, Val MAE: 3085.8089123, Test MAE: 2017.4051895, Time: 11.039881229400635\n",
      "ToTal Epoch: 2402, LR: 0.000000, Loss: 209211.9099603, Val MAE: 3142.8031988, Test MAE: 2017.4051895, Time: 11.581886529922485\n",
      "ToTal Epoch: 2403, LR: 0.000000, Loss: 211054.9620599, Val MAE: 3130.8582894, Test MAE: 2017.4051895, Time: 10.892128944396973\n",
      "ToTal Epoch: 2404, LR: 0.000000, Loss: 211750.4783452, Val MAE: 3114.0366888, Test MAE: 2017.4051895, Time: 11.657411813735962\n",
      "ToTal Epoch: 2405, LR: 0.000000, Loss: 211323.3073138, Val MAE: 3032.5862235, Test MAE: 2017.4051895, Time: 11.68506407737732\n",
      "ToTal Epoch: 2406, LR: 0.000000, Loss: 208141.7355181, Val MAE: 3087.2283402, Test MAE: 2017.4051895, Time: 11.93455982208252\n",
      "ToTal Epoch: 2407, LR: 0.000000, Loss: 208817.0632781, Val MAE: 3146.9773704, Test MAE: 2017.4051895, Time: 10.713039636611938\n",
      "ToTal Epoch: 2408, LR: 0.000000, Loss: 211543.7943916, Val MAE: 3070.5574935, Test MAE: 2017.4051895, Time: 11.437717199325562\n",
      "ToTal Epoch: 2409, LR: 0.000000, Loss: 209356.1983853, Val MAE: 3071.9233834, Test MAE: 2017.4051895, Time: 11.411344528198242\n",
      "ToTal Epoch: 2410, LR: 0.000000, Loss: 210340.4906320, Val MAE: 3035.8797867, Test MAE: 2017.4051895, Time: 12.99392318725586\n",
      "ToTal Epoch: 2411, LR: 0.000000, Loss: 209672.7382793, Val MAE: 3006.9639704, Test MAE: 2017.4051895, Time: 12.182426452636719\n",
      "ToTal Epoch: 2412, LR: 0.000000, Loss: 210874.3026991, Val MAE: 3003.1303791, Test MAE: 2017.4051895, Time: 11.268187522888184\n",
      "ToTal Epoch: 2413, LR: 0.000000, Loss: 208170.0581856, Val MAE: 3034.6235286, Test MAE: 2017.4051895, Time: 12.908880949020386\n",
      "ToTal Epoch: 2414, LR: 0.000000, Loss: 210342.2982659, Val MAE: 3037.5929546, Test MAE: 2017.4051895, Time: 12.434517860412598\n",
      "ToTal Epoch: 2415, LR: 0.000000, Loss: 211605.2746955, Val MAE: 3141.8933922, Test MAE: 2017.4051895, Time: 12.123645067214966\n",
      "ToTal Epoch: 2416, LR: 0.000000, Loss: 210472.9773659, Val MAE: 3122.8104410, Test MAE: 2017.4051895, Time: 11.086663722991943\n",
      "ToTal Epoch: 2417, LR: 0.000000, Loss: 210997.8645392, Val MAE: 3120.0741420, Test MAE: 2017.4051895, Time: 12.46936821937561\n",
      "ToTal Epoch: 2418, LR: 0.000000, Loss: 209462.1362251, Val MAE: 3083.5310279, Test MAE: 2017.4051895, Time: 12.476573467254639\n",
      "ToTal Epoch: 2419, LR: 0.000000, Loss: 208659.1194382, Val MAE: 2986.5095162, Test MAE: 2017.4051895, Time: 12.89343523979187\n",
      "ToTal Epoch: 2420, LR: 0.000000, Loss: 209724.5168872, Val MAE: 3066.0474327, Test MAE: 2017.4051895, Time: 10.86945629119873\n",
      "ToTal Epoch: 2421, LR: 0.000000, Loss: 209638.6290737, Val MAE: 3053.2364376, Test MAE: 2017.4051895, Time: 10.934775590896606\n",
      "ToTal Epoch: 2422, LR: 0.000000, Loss: 209804.8753260, Val MAE: 3048.7631134, Test MAE: 2017.4051895, Time: 11.34097671508789\n",
      "ToTal Epoch: 2423, LR: 0.000000, Loss: 210820.9599006, Val MAE: 3052.4075804, Test MAE: 2017.4051895, Time: 11.98753309249878\n",
      "ToTal Epoch: 2424, LR: 0.000000, Loss: 210183.0521760, Val MAE: 3051.0009316, Test MAE: 2017.4051895, Time: 11.74600863456726\n",
      "ToTal Epoch: 2425, LR: 0.000000, Loss: 210363.1797449, Val MAE: 3036.8737914, Test MAE: 2017.4051895, Time: 11.835821628570557\n",
      "ToTal Epoch: 2426, LR: 0.000000, Loss: 208077.4942913, Val MAE: 3082.6083180, Test MAE: 2017.4051895, Time: 11.422794818878174\n",
      "ToTal Epoch: 2427, LR: 0.000000, Loss: 211267.2711604, Val MAE: 3079.8605538, Test MAE: 2017.4051895, Time: 11.037481307983398\n",
      "ToTal Epoch: 2428, LR: 0.000000, Loss: 210650.7149477, Val MAE: 3092.3927568, Test MAE: 2017.4051895, Time: 11.926515579223633\n",
      "ToTal Epoch: 2429, LR: 0.000000, Loss: 209846.4577079, Val MAE: 3045.4932307, Test MAE: 2017.4051895, Time: 13.148475170135498\n",
      "ToTal Epoch: 2430, LR: 0.000000, Loss: 210447.9907132, Val MAE: 3052.6813422, Test MAE: 2017.4051895, Time: 11.954999446868896\n",
      "ToTal Epoch: 2431, LR: 0.000000, Loss: 211802.9404672, Val MAE: 3056.7521832, Test MAE: 2017.4051895, Time: 11.893465280532837\n",
      "ToTal Epoch: 2432, LR: 0.000000, Loss: 208541.0363350, Val MAE: 2953.8762994, Test MAE: 2017.4051895, Time: 11.208842515945435\n",
      "ToTal Epoch: 2433, LR: 0.000000, Loss: 209502.3345722, Val MAE: 3022.8821371, Test MAE: 2017.4051895, Time: 11.754289388656616\n",
      "ToTal Epoch: 2434, LR: 0.000000, Loss: 209282.6146276, Val MAE: 3060.2899087, Test MAE: 2017.4051895, Time: 11.52922010421753\n",
      "ToTal Epoch: 2435, LR: 0.000000, Loss: 211751.3756461, Val MAE: 3033.2256507, Test MAE: 2017.4051895, Time: 11.522642374038696\n",
      "ToTal Epoch: 2436, LR: 0.000000, Loss: 209727.4038504, Val MAE: 3001.6660504, Test MAE: 2017.4051895, Time: 11.423248767852783\n",
      "ToTal Epoch: 2437, LR: 0.000000, Loss: 209554.5919648, Val MAE: 3050.6620806, Test MAE: 2017.4051895, Time: 11.033506155014038\n",
      "ToTal Epoch: 2438, LR: 0.000000, Loss: 210019.8738451, Val MAE: 3056.1637048, Test MAE: 2017.4051895, Time: 10.734237432479858\n",
      "ToTal Epoch: 2439, LR: 0.000000, Loss: 208949.6243443, Val MAE: 3060.2534969, Test MAE: 2017.4051895, Time: 10.367335796356201\n",
      "ToTal Epoch: 2440, LR: 0.000000, Loss: 209860.5366073, Val MAE: 3078.2091168, Test MAE: 2017.4051895, Time: 11.111158847808838\n",
      "ToTal Epoch: 2441, LR: 0.000000, Loss: 209787.4933932, Val MAE: 3098.6620089, Test MAE: 2017.4051895, Time: 10.499337196350098\n",
      "ToTal Epoch: 2442, LR: 0.000000, Loss: 210508.9533655, Val MAE: 3095.0189511, Test MAE: 2017.4051895, Time: 10.572610139846802\n",
      "ToTal Epoch: 2443, LR: 0.000000, Loss: 212656.2799408, Val MAE: 3072.8405660, Test MAE: 2017.4051895, Time: 10.282275676727295\n",
      "ToTal Epoch: 2444, LR: 0.000000, Loss: 209121.2830268, Val MAE: 3102.6233184, Test MAE: 2017.4051895, Time: 10.909119844436646\n",
      "ToTal Epoch: 2445, LR: 0.000000, Loss: 209973.7248937, Val MAE: 3085.5151007, Test MAE: 2017.4051895, Time: 12.651012182235718\n",
      "ToTal Epoch: 2446, LR: 0.000000, Loss: 211947.5971910, Val MAE: 3072.1559705, Test MAE: 2017.4051895, Time: 11.569571733474731\n",
      "ToTal Epoch: 2447, LR: 0.000000, Loss: 209657.4422395, Val MAE: 2983.2610114, Test MAE: 2017.4051895, Time: 11.15123987197876\n",
      "ToTal Epoch: 2448, LR: 0.000000, Loss: 206937.2611284, Val MAE: 3086.4329808, Test MAE: 2017.4051895, Time: 10.833200216293335\n",
      "ToTal Epoch: 2449, LR: 0.000000, Loss: 209265.4978073, Val MAE: 3116.2152507, Test MAE: 2017.4051895, Time: 10.681724786758423\n",
      "ToTal Epoch: 2450, LR: 0.000000, Loss: 209844.2713419, Val MAE: 3084.7699018, Test MAE: 2017.4051895, Time: 10.640357494354248\n",
      "ToTal Epoch: 2451, LR: 0.000000, Loss: 211313.8133282, Val MAE: 3139.5580907, Test MAE: 2017.4051895, Time: 10.859091758728027\n",
      "ToTal Epoch: 2452, LR: 0.000000, Loss: 209711.1513113, Val MAE: 3149.4174692, Test MAE: 2017.4051895, Time: 11.339778661727905\n",
      "ToTal Epoch: 2453, LR: 0.000000, Loss: 210420.2293794, Val MAE: 3160.5153396, Test MAE: 2017.4051895, Time: 10.687597513198853\n",
      "ToTal Epoch: 2454, LR: 0.000000, Loss: 209731.3552381, Val MAE: 3149.2614270, Test MAE: 2017.4051895, Time: 11.13796329498291\n",
      "ToTal Epoch: 2455, LR: 0.000000, Loss: 210183.8516027, Val MAE: 3157.7586276, Test MAE: 2017.4051895, Time: 11.077424049377441\n",
      "ToTal Epoch: 2456, LR: 0.000000, Loss: 208595.4299909, Val MAE: 3104.3000124, Test MAE: 2017.4051895, Time: 11.50490689277649\n",
      "ToTal Epoch: 2457, LR: 0.000000, Loss: 209760.4414847, Val MAE: 3061.8899096, Test MAE: 2017.4051895, Time: 11.083832740783691\n",
      "ToTal Epoch: 2458, LR: 0.000000, Loss: 208448.7710696, Val MAE: 3084.1182928, Test MAE: 2017.4051895, Time: 11.03339958190918\n",
      "ToTal Epoch: 2459, LR: 0.000000, Loss: 209162.2469785, Val MAE: 3073.7209021, Test MAE: 2017.4051895, Time: 11.644964456558228\n",
      "ToTal Epoch: 2460, LR: 0.000000, Loss: 210582.2439211, Val MAE: 3072.3952266, Test MAE: 2017.4051895, Time: 10.50555944442749\n",
      "ToTal Epoch: 2461, LR: 0.000000, Loss: 209014.5422825, Val MAE: 2967.2942320, Test MAE: 2017.4051895, Time: 11.11762547492981\n",
      "ToTal Epoch: 2462, LR: 0.000000, Loss: 210277.7182821, Val MAE: 3029.7920966, Test MAE: 2017.4051895, Time: 11.036226987838745\n",
      "ToTal Epoch: 2463, LR: 0.000000, Loss: 210082.8613959, Val MAE: 3044.3105557, Test MAE: 2017.4051895, Time: 11.179969549179077\n",
      "ToTal Epoch: 2464, LR: 0.000000, Loss: 210511.3742321, Val MAE: 3068.0620127, Test MAE: 2017.4051895, Time: 11.07062292098999\n",
      "ToTal Epoch: 2465, LR: 0.000000, Loss: 208500.0775809, Val MAE: 3064.2786297, Test MAE: 2017.4051895, Time: 11.006400108337402\n",
      "ToTal Epoch: 2466, LR: 0.000000, Loss: 208353.5593369, Val MAE: 3032.8915912, Test MAE: 2017.4051895, Time: 10.346416711807251\n",
      "ToTal Epoch: 2467, LR: 0.000000, Loss: 209193.4827115, Val MAE: 3082.4028319, Test MAE: 2017.4051895, Time: 10.825476169586182\n",
      "ToTal Epoch: 2468, LR: 0.000000, Loss: 208082.6860173, Val MAE: 3121.6685345, Test MAE: 2017.4051895, Time: 10.40159559249878\n",
      "ToTal Epoch: 2469, LR: 0.000000, Loss: 209359.7401233, Val MAE: 3119.1163867, Test MAE: 2017.4051895, Time: 10.485499143600464\n",
      "ToTal Epoch: 2470, LR: 0.000000, Loss: 210514.7317632, Val MAE: 3161.9054259, Test MAE: 2017.4051895, Time: 10.897286653518677\n",
      "ToTal Epoch: 2471, LR: 0.000000, Loss: 211189.3507858, Val MAE: 3155.5773380, Test MAE: 2017.4051895, Time: 11.193907499313354\n",
      "ToTal Epoch: 2472, LR: 0.000000, Loss: 209927.0001242, Val MAE: 3051.6657542, Test MAE: 2017.4051895, Time: 11.390387535095215\n",
      "ToTal Epoch: 2473, LR: 0.000000, Loss: 210665.2193952, Val MAE: 3085.2094082, Test MAE: 2017.4051895, Time: 10.915404796600342\n",
      "ToTal Epoch: 2474, LR: 0.000000, Loss: 210278.4575168, Val MAE: 3050.3349241, Test MAE: 2017.4051895, Time: 10.65638542175293\n",
      "ToTal Epoch: 2475, LR: 0.000000, Loss: 209797.0707304, Val MAE: 3041.5897348, Test MAE: 2017.4051895, Time: 10.720745325088501\n",
      "ToTal Epoch: 2476, LR: 0.000000, Loss: 210325.5148139, Val MAE: 2981.5651418, Test MAE: 2017.4051895, Time: 10.457717895507812\n",
      "ToTal Epoch: 2477, LR: 0.000000, Loss: 210077.8324368, Val MAE: 3060.4324696, Test MAE: 2017.4051895, Time: 11.195785760879517\n",
      "ToTal Epoch: 2478, LR: 0.000000, Loss: 210855.4750872, Val MAE: 3081.8279733, Test MAE: 2017.4051895, Time: 10.908191919326782\n",
      "ToTal Epoch: 2479, LR: 0.000000, Loss: 209052.5702002, Val MAE: 3093.4798498, Test MAE: 2017.4051895, Time: 12.238525629043579\n",
      "ToTal Epoch: 2480, LR: 0.000000, Loss: 210712.2691253, Val MAE: 3091.4537616, Test MAE: 2017.4051895, Time: 11.115584373474121\n",
      "ToTal Epoch: 2481, LR: 0.000000, Loss: 209000.2507428, Val MAE: 3050.6208295, Test MAE: 2017.4051895, Time: 10.698246717453003\n",
      "ToTal Epoch: 2482, LR: 0.000000, Loss: 210869.4846224, Val MAE: 3046.7182938, Test MAE: 2017.4051895, Time: 10.877053260803223\n",
      "ToTal Epoch: 2483, LR: 0.000000, Loss: 208854.0585678, Val MAE: 3085.4014609, Test MAE: 2017.4051895, Time: 10.702114820480347\n",
      "ToTal Epoch: 2484, LR: 0.000000, Loss: 208490.5749582, Val MAE: 3047.8912902, Test MAE: 2017.4051895, Time: 10.70594310760498\n",
      "ToTal Epoch: 2485, LR: 0.000000, Loss: 209400.7406487, Val MAE: 3106.1074725, Test MAE: 2017.4051895, Time: 10.924500703811646\n",
      "ToTal Epoch: 2486, LR: 0.000000, Loss: 208305.0529594, Val MAE: 3108.9825919, Test MAE: 2017.4051895, Time: 11.88596773147583\n",
      "ToTal Epoch: 2487, LR: 0.000000, Loss: 208273.6379879, Val MAE: 3090.9383599, Test MAE: 2017.4051895, Time: 11.205073595046997\n",
      "ToTal Epoch: 2488, LR: 0.000000, Loss: 209755.7784933, Val MAE: 3096.0763730, Test MAE: 2017.4051895, Time: 11.373655796051025\n",
      "ToTal Epoch: 2489, LR: 0.000000, Loss: 210194.9827736, Val MAE: 3088.8641844, Test MAE: 2017.4051895, Time: 10.660677433013916\n",
      "ToTal Epoch: 2490, LR: 0.000000, Loss: 209871.0681125, Val MAE: 3085.6369764, Test MAE: 2017.4051895, Time: 11.361966371536255\n",
      "ToTal Epoch: 2491, LR: 0.000000, Loss: 209597.5585726, Val MAE: 3113.4704626, Test MAE: 2017.4051895, Time: 12.879917621612549\n",
      "ToTal Epoch: 2492, LR: 0.000000, Loss: 210910.4088664, Val MAE: 3138.2356541, Test MAE: 2017.4051895, Time: 11.158209323883057\n",
      "ToTal Epoch: 2493, LR: 0.000000, Loss: 210052.3676109, Val MAE: 3134.7681581, Test MAE: 2017.4051895, Time: 11.833045959472656\n",
      "ToTal Epoch: 2494, LR: 0.000000, Loss: 209143.7240338, Val MAE: 3127.6950241, Test MAE: 2017.4051895, Time: 11.941961765289307\n",
      "ToTal Epoch: 2495, LR: 0.000000, Loss: 211309.2599054, Val MAE: 3119.3211181, Test MAE: 2017.4051895, Time: 11.690727233886719\n",
      "ToTal Epoch: 2496, LR: 0.000000, Loss: 209744.7381646, Val MAE: 3126.2589238, Test MAE: 2017.4051895, Time: 12.010853052139282\n",
      "ToTal Epoch: 2497, LR: 0.000000, Loss: 209779.6314336, Val MAE: 3140.3705859, Test MAE: 2017.4051895, Time: 11.35578179359436\n",
      "ToTal Epoch: 2498, LR: 0.000000, Loss: 211465.5353365, Val MAE: 3141.3776513, Test MAE: 2017.4051895, Time: 12.396174669265747\n",
      "ToTal Epoch: 2499, LR: 0.000000, Loss: 209958.0522238, Val MAE: 3123.6992137, Test MAE: 2017.4051895, Time: 10.227449655532837\n",
      "ToTal Epoch: 2500, LR: 0.000000, Loss: 210312.3365404, Val MAE: 3042.2996780, Test MAE: 2017.4051895, Time: 12.24841046333313\n",
      "ToTal Epoch: 2501, LR: 0.000000, Loss: 209484.8144843, Val MAE: 3131.4943486, Test MAE: 2017.4051895, Time: 11.218966960906982\n",
      "ToTal Epoch: 2502, LR: 0.000000, Loss: 208214.5687288, Val MAE: 3149.1478923, Test MAE: 2017.4051895, Time: 11.094175815582275\n",
      "ToTal Epoch: 2503, LR: 0.000000, Loss: 210757.4539340, Val MAE: 3033.8183807, Test MAE: 2017.4051895, Time: 10.780875205993652\n",
      "ToTal Epoch: 2504, LR: 0.000000, Loss: 209640.4969378, Val MAE: 3027.7830486, Test MAE: 2017.4051895, Time: 11.066651582717896\n",
      "ToTal Epoch: 2505, LR: 0.000000, Loss: 210562.7966178, Val MAE: 3034.5436970, Test MAE: 2017.4051895, Time: 12.00798511505127\n",
      "ToTal Epoch: 2506, LR: 0.000000, Loss: 208564.2703100, Val MAE: 3002.4215298, Test MAE: 2017.4051895, Time: 11.86245584487915\n",
      "ToTal Epoch: 2507, LR: 0.000000, Loss: 209520.8055797, Val MAE: 3003.9433903, Test MAE: 2017.4051895, Time: 12.139957189559937\n",
      "ToTal Epoch: 2508, LR: 0.000000, Loss: 210249.8719916, Val MAE: 2986.1755618, Test MAE: 2017.4051895, Time: 10.767509937286377\n",
      "ToTal Epoch: 2509, LR: 0.000000, Loss: 211247.4995844, Val MAE: 2950.2573712, Test MAE: 2017.4051895, Time: 11.767367601394653\n",
      "ToTal Epoch: 2510, LR: 0.000000, Loss: 209668.0658482, Val MAE: 2923.9171348, Test MAE: 2017.4051895, Time: 10.619574785232544\n",
      "ToTal Epoch: 2511, LR: 0.000000, Loss: 209917.5932356, Val MAE: 2950.9498156, Test MAE: 2017.4051895, Time: 10.410125732421875\n",
      "ToTal Epoch: 2512, LR: 0.000000, Loss: 208970.3421774, Val MAE: 2945.2874389, Test MAE: 2017.4051895, Time: 10.816625118255615\n",
      "ToTal Epoch: 2513, LR: 0.000000, Loss: 208569.2753451, Val MAE: 2926.8282361, Test MAE: 2017.4051895, Time: 10.523655891418457\n",
      "ToTal Epoch: 2514, LR: 0.000000, Loss: 207333.2644915, Val MAE: 2985.1341101, Test MAE: 2017.4051895, Time: 10.14937162399292\n",
      "ToTal Epoch: 2515, LR: 0.000000, Loss: 210917.4347490, Val MAE: 3126.7138128, Test MAE: 2017.4051895, Time: 10.275449991226196\n",
      "ToTal Epoch: 2516, LR: 0.000000, Loss: 211226.5021927, Val MAE: 3072.2122697, Test MAE: 2017.4051895, Time: 10.946391820907593\n",
      "ToTal Epoch: 2517, LR: 0.000000, Loss: 208823.9599484, Val MAE: 3162.2395857, Test MAE: 2017.4051895, Time: 10.7196946144104\n",
      "ToTal Epoch: 2518, LR: 0.000000, Loss: 211440.9398748, Val MAE: 3203.8124809, Test MAE: 2017.4051895, Time: 11.410783529281616\n",
      "ToTal Epoch: 2519, LR: 0.000000, Loss: 209319.9916687, Val MAE: 3110.4920269, Test MAE: 2017.4051895, Time: 11.31275749206543\n",
      "ToTal Epoch: 2520, LR: 0.000000, Loss: 209027.1148521, Val MAE: 3094.5353417, Test MAE: 2017.4051895, Time: 11.603960275650024\n",
      "ToTal Epoch: 2521, LR: 0.000000, Loss: 210068.3772417, Val MAE: 3073.8325642, Test MAE: 2017.4051895, Time: 10.566706418991089\n",
      "ToTal Epoch: 2522, LR: 0.000000, Loss: 208222.6290355, Val MAE: 3075.1749503, Test MAE: 2017.4051895, Time: 11.158992052078247\n",
      "ToTal Epoch: 2523, LR: 0.000000, Loss: 209668.2247934, Val MAE: 3048.5540539, Test MAE: 2017.4051895, Time: 11.120058298110962\n",
      "ToTal Epoch: 2524, LR: 0.000000, Loss: 210226.9075622, Val MAE: 3057.5003487, Test MAE: 2017.4051895, Time: 10.505777835845947\n",
      "ToTal Epoch: 2525, LR: 0.000000, Loss: 210682.0732050, Val MAE: 3065.9124293, Test MAE: 2017.4051895, Time: 10.969819068908691\n",
      "ToTal Epoch: 2526, LR: 0.000000, Loss: 209961.3917164, Val MAE: 3050.9766109, Test MAE: 2017.4051895, Time: 10.89646053314209\n",
      "ToTal Epoch: 2527, LR: 0.000000, Loss: 210799.6624660, Val MAE: 3070.2210216, Test MAE: 2017.4051895, Time: 10.468995332717896\n",
      "ToTal Epoch: 2528, LR: 0.000000, Loss: 210027.7809392, Val MAE: 3056.8637927, Test MAE: 2017.4051895, Time: 12.429494380950928\n",
      "ToTal Epoch: 2529, LR: 0.000000, Loss: 209704.8125734, Val MAE: 3007.8924702, Test MAE: 2017.4051895, Time: 11.100367546081543\n",
      "ToTal Epoch: 2530, LR: 0.000000, Loss: 209785.9064253, Val MAE: 3018.1139026, Test MAE: 2017.4051895, Time: 12.077324867248535\n",
      "ToTal Epoch: 2531, LR: 0.000000, Loss: 209719.2312616, Val MAE: 3022.6231273, Test MAE: 2017.4051895, Time: 11.57973861694336\n",
      "ToTal Epoch: 2532, LR: 0.000000, Loss: 211195.0944824, Val MAE: 3001.8936215, Test MAE: 2017.4051895, Time: 11.800445079803467\n",
      "ToTal Epoch: 2533, LR: 0.000000, Loss: 210355.2235418, Val MAE: 2988.5683568, Test MAE: 2017.4051895, Time: 11.5257248878479\n",
      "ToTal Epoch: 2534, LR: 0.000000, Loss: 209103.5438017, Val MAE: 3007.2826426, Test MAE: 2017.4051895, Time: 11.434807300567627\n",
      "ToTal Epoch: 2535, LR: 0.000000, Loss: 210603.8362394, Val MAE: 3127.0733824, Test MAE: 2017.4051895, Time: 11.44693899154663\n",
      "ToTal Epoch: 2536, LR: 0.000000, Loss: 209433.5041513, Val MAE: 3053.6431486, Test MAE: 2017.4051895, Time: 10.863208055496216\n",
      "ToTal Epoch: 2537, LR: 0.000000, Loss: 209487.9406487, Val MAE: 3053.9133226, Test MAE: 2017.4051895, Time: 12.400876998901367\n",
      "ToTal Epoch: 2538, LR: 0.000000, Loss: 211456.9530980, Val MAE: 3040.3471251, Test MAE: 2017.4051895, Time: 12.645214796066284\n",
      "ToTal Epoch: 2539, LR: 0.000000, Loss: 210217.4771318, Val MAE: 3021.8198043, Test MAE: 2017.4051895, Time: 14.087044477462769\n",
      "ToTal Epoch: 2540, LR: 0.000000, Loss: 209222.2817179, Val MAE: 3039.6526599, Test MAE: 2017.4051895, Time: 13.265078783035278\n",
      "ToTal Epoch: 2541, LR: 0.000000, Loss: 209006.6571633, Val MAE: 3030.4278740, Test MAE: 2017.4051895, Time: 13.501883506774902\n",
      "ToTal Epoch: 2542, LR: 0.000000, Loss: 210983.8707878, Val MAE: 3021.8965117, Test MAE: 2017.4051895, Time: 12.182809591293335\n",
      "ToTal Epoch: 2543, LR: 0.000000, Loss: 208443.5293556, Val MAE: 3017.9569766, Test MAE: 2017.4051895, Time: 12.25781512260437\n",
      "ToTal Epoch: 2544, LR: 0.000000, Loss: 211616.8147518, Val MAE: 3041.4470257, Test MAE: 2017.4051895, Time: 14.936996936798096\n",
      "ToTal Epoch: 2545, LR: 0.000000, Loss: 211047.8595901, Val MAE: 3027.3095477, Test MAE: 2017.4051895, Time: 10.69418978691101\n",
      "ToTal Epoch: 2546, LR: 0.000000, Loss: 210490.5129317, Val MAE: 3010.1201750, Test MAE: 2017.4051895, Time: 11.447180986404419\n",
      "ToTal Epoch: 2547, LR: 0.000000, Loss: 210262.0339177, Val MAE: 3005.9644864, Test MAE: 2017.4051895, Time: 13.18313217163086\n",
      "ToTal Epoch: 2548, LR: 0.000000, Loss: 211472.9253523, Val MAE: 2954.0860850, Test MAE: 2017.4051895, Time: 11.117530345916748\n",
      "ToTal Epoch: 2549, LR: 0.000000, Loss: 209598.5722448, Val MAE: 2935.7187046, Test MAE: 2017.4051895, Time: 11.02140474319458\n",
      "ToTal Epoch: 2550, LR: 0.000000, Loss: 210298.2595137, Val MAE: 3008.0847999, Test MAE: 2017.4051895, Time: 11.671269655227661\n",
      "ToTal Epoch: 2551, LR: 0.000000, Loss: 208678.3524578, Val MAE: 2997.4832177, Test MAE: 2017.4051895, Time: 12.858688354492188\n",
      "ToTal Epoch: 2552, LR: 0.000000, Loss: 210607.4593799, Val MAE: 2956.3416839, Test MAE: 2017.4051895, Time: 11.24128270149231\n",
      "ToTal Epoch: 2553, LR: 0.000000, Loss: 210154.5643338, Val MAE: 2975.9803180, Test MAE: 2017.4051895, Time: 10.884289979934692\n",
      "ToTal Epoch: 2554, LR: 0.000000, Loss: 210095.1709167, Val MAE: 2967.4759803, Test MAE: 2017.4051895, Time: 11.954799175262451\n",
      "ToTal Epoch: 2555, LR: 0.000000, Loss: 210314.5817226, Val MAE: 3016.0059667, Test MAE: 2017.4051895, Time: 12.277889251708984\n",
      "ToTal Epoch: 2556, LR: 0.000000, Loss: 209729.0645775, Val MAE: 3041.8100254, Test MAE: 2017.4051895, Time: 11.440437316894531\n",
      "ToTal Epoch: 2557, LR: 0.000000, Loss: 209291.6226055, Val MAE: 3083.3346614, Test MAE: 2017.4051895, Time: 11.540635347366333\n",
      "ToTal Epoch: 2558, LR: 0.000000, Loss: 210225.2902498, Val MAE: 3095.4212623, Test MAE: 2017.4051895, Time: 11.302467107772827\n",
      "ToTal Epoch: 2559, LR: 0.000000, Loss: 211122.2063918, Val MAE: 3037.9290444, Test MAE: 2017.4051895, Time: 10.934985160827637\n",
      "ToTal Epoch: 2560, LR: 0.000000, Loss: 209604.2860937, Val MAE: 3049.9313995, Test MAE: 2017.4051895, Time: 10.678752422332764\n",
      "ToTal Epoch: 2561, LR: 0.000000, Loss: 209796.1916591, Val MAE: 3034.7953308, Test MAE: 2017.4051895, Time: 10.392946004867554\n",
      "ToTal Epoch: 2562, LR: 0.000000, Loss: 208545.0829599, Val MAE: 3032.4585244, Test MAE: 2017.4051895, Time: 12.061073541641235\n",
      "ToTal Epoch: 2563, LR: 0.000000, Loss: 210226.8401854, Val MAE: 3100.2359742, Test MAE: 2017.4051895, Time: 10.820394039154053\n",
      "ToTal Epoch: 2564, LR: 0.000000, Loss: 210898.4964601, Val MAE: 3091.9896287, Test MAE: 2017.4051895, Time: 10.85809588432312\n",
      "ToTal Epoch: 2565, LR: 0.000000, Loss: 208955.6064396, Val MAE: 3083.4329999, Test MAE: 2017.4051895, Time: 11.490564107894897\n",
      "ToTal Epoch: 2566, LR: 0.000000, Loss: 210756.9031864, Val MAE: 3095.6668769, Test MAE: 2017.4051895, Time: 11.173689365386963\n",
      "ToTal Epoch: 2567, LR: 0.000000, Loss: 210472.8118473, Val MAE: 3082.1607812, Test MAE: 2017.4051895, Time: 11.50231385231018\n",
      "ToTal Epoch: 2568, LR: 0.000000, Loss: 208712.8043185, Val MAE: 3038.2445683, Test MAE: 2017.4051895, Time: 11.99428653717041\n",
      "ToTal Epoch: 2569, LR: 0.000000, Loss: 209940.3769359, Val MAE: 3068.8621876, Test MAE: 2017.4051895, Time: 11.644362926483154\n",
      "ToTal Epoch: 2570, LR: 0.000000, Loss: 210399.8036402, Val MAE: 2965.4099260, Test MAE: 2017.4051895, Time: 10.87125015258789\n",
      "ToTal Epoch: 2571, LR: 0.000000, Loss: 210223.2354273, Val MAE: 2927.4913485, Test MAE: 2017.4051895, Time: 11.110193014144897\n",
      "ToTal Epoch: 2572, LR: 0.000000, Loss: 210113.3716142, Val MAE: 2908.1732496, Test MAE: 2017.4051895, Time: 11.200220346450806\n",
      "ToTal Epoch: 2573, LR: 0.000000, Loss: 211186.1762385, Val MAE: 2985.7460063, Test MAE: 2017.4051895, Time: 10.86533808708191\n",
      "ToTal Epoch: 2574, LR: 0.000000, Loss: 209656.9142311, Val MAE: 2983.7308673, Test MAE: 2017.4051895, Time: 12.661268711090088\n",
      "ToTal Epoch: 2575, LR: 0.000000, Loss: 211167.2430325, Val MAE: 2973.4890793, Test MAE: 2017.4051895, Time: 11.423977613449097\n",
      "ToTal Epoch: 2576, LR: 0.000000, Loss: 208722.5374672, Val MAE: 3088.2889867, Test MAE: 2017.4051895, Time: 10.784794330596924\n",
      "ToTal Epoch: 2577, LR: 0.000000, Loss: 210532.7463813, Val MAE: 3021.5513787, Test MAE: 2017.4051895, Time: 10.258519172668457\n",
      "ToTal Epoch: 2578, LR: 0.000000, Loss: 210243.6807720, Val MAE: 3022.7719608, Test MAE: 2017.4051895, Time: 12.039336204528809\n",
      "ToTal Epoch: 2579, LR: 0.000000, Loss: 209471.2874409, Val MAE: 3043.9963216, Test MAE: 2017.4051895, Time: 10.450267791748047\n",
      "ToTal Epoch: 2580, LR: 0.000000, Loss: 210138.3044189, Val MAE: 2994.6125984, Test MAE: 2017.4051895, Time: 11.87930679321289\n",
      "ToTal Epoch: 2581, LR: 0.000000, Loss: 211448.3608083, Val MAE: 2990.1795412, Test MAE: 2017.4051895, Time: 10.711228847503662\n",
      "ToTal Epoch: 2582, LR: 0.000000, Loss: 210230.7325276, Val MAE: 3038.8754825, Test MAE: 2017.4051895, Time: 11.304578065872192\n",
      "ToTal Epoch: 2583, LR: 0.000000, Loss: 210811.2334400, Val MAE: 3052.5518899, Test MAE: 2017.4051895, Time: 10.92936372756958\n",
      "ToTal Epoch: 2584, LR: 0.000000, Loss: 210609.1296995, Val MAE: 3063.2783670, Test MAE: 2017.4051895, Time: 11.834859371185303\n",
      "ToTal Epoch: 2585, LR: 0.000000, Loss: 211415.6602111, Val MAE: 2986.0972159, Test MAE: 2017.4051895, Time: 11.027485370635986\n",
      "ToTal Epoch: 2586, LR: 0.000000, Loss: 210756.1353652, Val MAE: 3000.6792163, Test MAE: 2017.4051895, Time: 10.571762800216675\n",
      "ToTal Epoch: 2587, LR: 0.000000, Loss: 210346.5553146, Val MAE: 3026.1442282, Test MAE: 2017.4051895, Time: 11.425996541976929\n",
      "ToTal Epoch: 2588, LR: 0.000000, Loss: 211070.0844408, Val MAE: 3028.5674444, Test MAE: 2017.4051895, Time: 10.672643423080444\n",
      "ToTal Epoch: 2589, LR: 0.000000, Loss: 210468.2399656, Val MAE: 2988.8926661, Test MAE: 2017.4051895, Time: 11.233519792556763\n",
      "ToTal Epoch: 2590, LR: 0.000000, Loss: 209973.7976974, Val MAE: 2984.1887612, Test MAE: 2017.4051895, Time: 11.264699697494507\n",
      "ToTal Epoch: 2591, LR: 0.000000, Loss: 210953.3668753, Val MAE: 2984.9341942, Test MAE: 2017.4051895, Time: 11.31472396850586\n",
      "ToTal Epoch: 2592, LR: 0.000000, Loss: 210930.2511823, Val MAE: 3050.0342955, Test MAE: 2017.4051895, Time: 11.69048285484314\n",
      "ToTal Epoch: 2593, LR: 0.000000, Loss: 211044.0856065, Val MAE: 3018.5140641, Test MAE: 2017.4051895, Time: 11.029442548751831\n",
      "ToTal Epoch: 2594, LR: 0.000000, Loss: 211066.6614819, Val MAE: 3061.9024020, Test MAE: 2017.4051895, Time: 11.260798454284668\n",
      "ToTal Epoch: 2595, LR: 0.000000, Loss: 209486.8795108, Val MAE: 3067.7336954, Test MAE: 2017.4051895, Time: 11.526707649230957\n",
      "ToTal Epoch: 2596, LR: 0.000000, Loss: 209065.3733722, Val MAE: 3042.1367806, Test MAE: 2017.4051895, Time: 11.496969223022461\n",
      "ToTal Epoch: 2597, LR: 0.000000, Loss: 209516.7461138, Val MAE: 2988.2644367, Test MAE: 2017.4051895, Time: 12.088470697402954\n",
      "ToTal Epoch: 2598, LR: 0.000000, Loss: 210246.2734630, Val MAE: 2945.0332301, Test MAE: 2017.4051895, Time: 11.064178228378296\n",
      "ToTal Epoch: 2599, LR: 0.000000, Loss: 209034.9529642, Val MAE: 2944.5769558, Test MAE: 2017.4051895, Time: 10.68794560432434\n",
      "ToTal Epoch: 2600, LR: 0.000000, Loss: 208754.0479434, Val MAE: 2979.0525013, Test MAE: 2017.4051895, Time: 11.006606340408325\n",
      "ToTal Epoch: 2601, LR: 0.000000, Loss: 208951.5482348, Val MAE: 2970.7743828, Test MAE: 2017.4051895, Time: 12.655131578445435\n",
      "ToTal Epoch: 2602, LR: 0.000000, Loss: 209231.2673387, Val MAE: 2959.7293100, Test MAE: 2017.4051895, Time: 10.836950302124023\n",
      "ToTal Epoch: 2603, LR: 0.000000, Loss: 211001.1298524, Val MAE: 2989.9516596, Test MAE: 2017.4051895, Time: 10.909223318099976\n",
      "ToTal Epoch: 2604, LR: 0.000000, Loss: 210334.2686094, Val MAE: 2963.2332129, Test MAE: 2017.4051895, Time: 10.481770038604736\n",
      "ToTal Epoch: 2605, LR: 0.000000, Loss: 210345.4448383, Val MAE: 2952.0723840, Test MAE: 2017.4051895, Time: 10.7975013256073\n",
      "ToTal Epoch: 2606, LR: 0.000000, Loss: 210062.4021019, Val MAE: 3024.9698750, Test MAE: 2017.4051895, Time: 10.898489475250244\n",
      "ToTal Epoch: 2607, LR: 0.000000, Loss: 210382.2606984, Val MAE: 3030.7392513, Test MAE: 2017.4051895, Time: 10.693071842193604\n",
      "ToTal Epoch: 2608, LR: 0.000000, Loss: 210393.8862084, Val MAE: 3074.7251682, Test MAE: 2017.4051895, Time: 10.961174726486206\n",
      "ToTal Epoch: 2609, LR: 0.000000, Loss: 210470.5627669, Val MAE: 3059.7852652, Test MAE: 2017.4051895, Time: 11.082134008407593\n",
      "ToTal Epoch: 2610, LR: 0.000000, Loss: 210723.3911241, Val MAE: 3083.9928772, Test MAE: 2017.4051895, Time: 11.33874225616455\n",
      "ToTal Epoch: 2611, LR: 0.000000, Loss: 209116.3644389, Val MAE: 3030.0018106, Test MAE: 2017.4051895, Time: 10.982244491577148\n",
      "ToTal Epoch: 2612, LR: 0.000000, Loss: 210585.2531028, Val MAE: 3040.7388453, Test MAE: 2017.4051895, Time: 11.812208414077759\n",
      "ToTal Epoch: 2613, LR: 0.000000, Loss: 209745.8819663, Val MAE: 3096.7811282, Test MAE: 2017.4051895, Time: 10.763946294784546\n",
      "ToTal Epoch: 2614, LR: 0.000000, Loss: 210216.7352601, Val MAE: 3109.0704445, Test MAE: 2017.4051895, Time: 10.790959119796753\n",
      "ToTal Epoch: 2615, LR: 0.000000, Loss: 209876.9669326, Val MAE: 3013.4377198, Test MAE: 2017.4051895, Time: 10.999734163284302\n",
      "ToTal Epoch: 2616, LR: 0.000000, Loss: 209736.5484928, Val MAE: 3017.3547638, Test MAE: 2017.4051895, Time: 11.545345783233643\n",
      "ToTal Epoch: 2617, LR: 0.000000, Loss: 211561.1163235, Val MAE: 3033.3828012, Test MAE: 2017.4051895, Time: 11.387708187103271\n",
      "ToTal Epoch: 2618, LR: 0.000000, Loss: 210129.2513065, Val MAE: 3042.7998022, Test MAE: 2017.4051895, Time: 11.8314208984375\n",
      "ToTal Epoch: 2619, LR: 0.000000, Loss: 210218.8464530, Val MAE: 2994.0881870, Test MAE: 2017.4051895, Time: 10.732280731201172\n",
      "ToTal Epoch: 2620, LR: 0.000000, Loss: 210555.3870348, Val MAE: 2973.7807030, Test MAE: 2017.4051895, Time: 11.187182664871216\n",
      "ToTal Epoch: 2621, LR: 0.000000, Loss: 210223.5206803, Val MAE: 3004.6665520, Test MAE: 2017.4051895, Time: 10.959417343139648\n",
      "ToTal Epoch: 2622, LR: 0.000000, Loss: 210584.6368891, Val MAE: 3023.8394864, Test MAE: 2017.4051895, Time: 11.946788549423218\n",
      "ToTal Epoch: 2623, LR: 0.000000, Loss: 208442.4621029, Val MAE: 3035.3723821, Test MAE: 2017.4051895, Time: 13.125727891921997\n",
      "ToTal Epoch: 2624, LR: 0.000000, Loss: 210262.2731954, Val MAE: 3050.5583438, Test MAE: 2017.4051895, Time: 11.6339271068573\n",
      "ToTal Epoch: 2625, LR: 0.000000, Loss: 210809.8437109, Val MAE: 3054.1736557, Test MAE: 2017.4051895, Time: 11.09534502029419\n",
      "ToTal Epoch: 2626, LR: 0.000000, Loss: 210855.5059284, Val MAE: 3037.9149708, Test MAE: 2017.4051895, Time: 10.75840973854065\n",
      "ToTal Epoch: 2627, LR: 0.000000, Loss: 209768.8890078, Val MAE: 3150.3628707, Test MAE: 2017.4051895, Time: 12.984090328216553\n",
      "ToTal Epoch: 2628, LR: 0.000000, Loss: 209842.7204510, Val MAE: 3145.5330104, Test MAE: 2017.4051895, Time: 13.076027870178223\n",
      "ToTal Epoch: 2629, LR: 0.000000, Loss: 210561.0757369, Val MAE: 3081.6617271, Test MAE: 2017.4051895, Time: 12.118872165679932\n",
      "ToTal Epoch: 2630, LR: 0.000000, Loss: 210524.5000334, Val MAE: 3060.5859703, Test MAE: 2017.4051895, Time: 11.936471462249756\n",
      "ToTal Epoch: 2631, LR: 0.000000, Loss: 209145.7056323, Val MAE: 3066.6390497, Test MAE: 2017.4051895, Time: 13.399868488311768\n",
      "ToTal Epoch: 2632, LR: 0.000000, Loss: 210286.2245068, Val MAE: 3089.2777412, Test MAE: 2017.4051895, Time: 11.225794076919556\n",
      "ToTal Epoch: 2633, LR: 0.000000, Loss: 210500.5392825, Val MAE: 3101.0976745, Test MAE: 2017.4051895, Time: 11.278524160385132\n",
      "ToTal Epoch: 2634, LR: 0.000000, Loss: 210482.6465390, Val MAE: 3136.1153023, Test MAE: 2017.4051895, Time: 11.424658060073853\n",
      "ToTal Epoch: 2635, LR: 0.000000, Loss: 212548.5390914, Val MAE: 3124.6626204, Test MAE: 2017.4051895, Time: 11.670108556747437\n",
      "ToTal Epoch: 2636, LR: 0.000000, Loss: 209953.5703053, Val MAE: 3193.4803180, Test MAE: 2017.4051895, Time: 11.047683238983154\n",
      "ToTal Epoch: 2637, LR: 0.000000, Loss: 210511.6096498, Val MAE: 3075.8528004, Test MAE: 2017.4051895, Time: 11.738187789916992\n",
      "ToTal Epoch: 2638, LR: 0.000000, Loss: 210446.1711938, Val MAE: 3120.4271813, Test MAE: 2017.4051895, Time: 12.34992790222168\n",
      "ToTal Epoch: 2639, LR: 0.000000, Loss: 210629.0478383, Val MAE: 3043.0318686, Test MAE: 2017.4051895, Time: 11.542434692382812\n",
      "ToTal Epoch: 2640, LR: 0.000000, Loss: 210851.8871829, Val MAE: 2999.9580993, Test MAE: 2017.4051895, Time: 12.031231880187988\n",
      "ToTal Epoch: 2641, LR: 0.000000, Loss: 210596.8048918, Val MAE: 3030.1473668, Test MAE: 2017.4051895, Time: 11.8160879611969\n",
      "ToTal Epoch: 2642, LR: 0.000000, Loss: 210468.1041036, Val MAE: 2982.1353522, Test MAE: 2017.4051895, Time: 11.527519941329956\n",
      "ToTal Epoch: 2643, LR: 0.000000, Loss: 210632.1146133, Val MAE: 2963.1821687, Test MAE: 2017.4051895, Time: 11.66850996017456\n",
      "ToTal Epoch: 2644, LR: 0.000000, Loss: 209551.9153108, Val MAE: 3055.0683664, Test MAE: 2017.4051895, Time: 10.709815263748169\n",
      "ToTal Epoch: 2645, LR: 0.000000, Loss: 208806.0561601, Val MAE: 3039.2858146, Test MAE: 2017.4051895, Time: 11.53943657875061\n",
      "ToTal Epoch: 2646, LR: 0.000000, Loss: 209795.4332394, Val MAE: 3030.3152087, Test MAE: 2017.4051895, Time: 10.795905113220215\n",
      "ToTal Epoch: 2647, LR: 0.000000, Loss: 210397.6038217, Val MAE: 3000.0531415, Test MAE: 2017.4051895, Time: 11.254323244094849\n",
      "ToTal Epoch: 2648, LR: 0.000000, Loss: 211000.8282425, Val MAE: 2951.5434295, Test MAE: 2017.4051895, Time: 10.970794200897217\n",
      "ToTal Epoch: 2649, LR: 0.000000, Loss: 209841.0115702, Val MAE: 2967.0748347, Test MAE: 2017.4051895, Time: 10.811192989349365\n",
      "ToTal Epoch: 2650, LR: 0.000000, Loss: 210978.6166149, Val MAE: 2988.4974394, Test MAE: 2017.4051895, Time: 11.868045091629028\n",
      "ToTal Epoch: 2651, LR: 0.000000, Loss: 210161.6797592, Val MAE: 2992.4428313, Test MAE: 2017.4051895, Time: 11.617254972457886\n",
      "ToTal Epoch: 2652, LR: 0.000000, Loss: 209729.2971289, Val MAE: 3033.6030775, Test MAE: 2017.4051895, Time: 11.180180549621582\n",
      "ToTal Epoch: 2653, LR: 0.000000, Loss: 210993.4716667, Val MAE: 3063.1155221, Test MAE: 2017.4051895, Time: 10.766684532165527\n",
      "ToTal Epoch: 2654, LR: 0.000000, Loss: 208212.9056323, Val MAE: 3114.3279399, Test MAE: 2017.4051895, Time: 11.583053588867188\n",
      "ToTal Epoch: 2655, LR: 0.000000, Loss: 210754.3545598, Val MAE: 3113.4821381, Test MAE: 2017.4051895, Time: 11.52330231666565\n",
      "ToTal Epoch: 2656, LR: 0.000000, Loss: 209755.9878469, Val MAE: 3112.6374016, Test MAE: 2017.4051895, Time: 13.160695314407349\n",
      "ToTal Epoch: 2657, LR: 0.000000, Loss: 208676.1999140, Val MAE: 3075.9105041, Test MAE: 2017.4051895, Time: 10.955582857131958\n",
      "ToTal Epoch: 2658, LR: 0.000000, Loss: 209888.5107342, Val MAE: 3073.1092353, Test MAE: 2017.4051895, Time: 12.59611964225769\n",
      "ToTal Epoch: 2659, LR: 0.000000, Loss: 210358.8367458, Val MAE: 3118.0971394, Test MAE: 2017.4051895, Time: 11.651399612426758\n",
      "ToTal Epoch: 2660, LR: 0.000000, Loss: 209479.0994888, Val MAE: 3116.5071037, Test MAE: 2017.4051895, Time: 11.7809317111969\n",
      "ToTal Epoch: 2661, LR: 0.000000, Loss: 208364.1827163, Val MAE: 3090.6487283, Test MAE: 2017.4051895, Time: 12.371188640594482\n",
      "ToTal Epoch: 2662, LR: 0.000000, Loss: 208934.2211054, Val MAE: 3121.6707129, Test MAE: 2017.4051895, Time: 11.686225652694702\n",
      "ToTal Epoch: 2663, LR: 0.000000, Loss: 210362.5629198, Val MAE: 3092.9011647, Test MAE: 2017.4051895, Time: 11.699482917785645\n",
      "ToTal Epoch: 2664, LR: 0.000000, Loss: 212046.3768022, Val MAE: 3066.1431103, Test MAE: 2017.4051895, Time: 12.521068572998047\n",
      "ToTal Epoch: 2665, LR: 0.000000, Loss: 211593.4208761, Val MAE: 3093.6448015, Test MAE: 2017.4051895, Time: 10.989575862884521\n",
      "ToTal Epoch: 2666, LR: 0.000000, Loss: 208473.7232886, Val MAE: 3119.3235592, Test MAE: 2017.4051895, Time: 11.34201693534851\n",
      "ToTal Epoch: 2667, LR: 0.000000, Loss: 210067.6646061, Val MAE: 3086.6960225, Test MAE: 2017.4051895, Time: 11.78307056427002\n",
      "ToTal Epoch: 2668, LR: 0.000000, Loss: 209780.2932786, Val MAE: 3086.1060059, Test MAE: 2017.4051895, Time: 11.159052848815918\n",
      "ToTal Epoch: 2669, LR: 0.000000, Loss: 210776.9996083, Val MAE: 3062.5449964, Test MAE: 2017.4051895, Time: 11.56347370147705\n",
      "ToTal Epoch: 2670, LR: 0.000000, Loss: 209931.7251421, Val MAE: 3087.3516348, Test MAE: 2017.4051895, Time: 11.89255404472351\n",
      "ToTal Epoch: 2671, LR: 0.000000, Loss: 210016.7724072, Val MAE: 3104.2998548, Test MAE: 2017.4051895, Time: 11.638599634170532\n",
      "ToTal Epoch: 2672, LR: 0.000000, Loss: 211751.6601347, Val MAE: 3100.3628898, Test MAE: 2017.4051895, Time: 11.053857803344727\n",
      "ToTal Epoch: 2673, LR: 0.000000, Loss: 209881.4663546, Val MAE: 3092.3030555, Test MAE: 2017.4051895, Time: 11.683504104614258\n",
      "ToTal Epoch: 2674, LR: 0.000000, Loss: 211917.6194143, Val MAE: 3142.5745194, Test MAE: 2017.4051895, Time: 11.135377883911133\n",
      "ToTal Epoch: 2675, LR: 0.000000, Loss: 210134.6261310, Val MAE: 3129.3250162, Test MAE: 2017.4051895, Time: 11.73261833190918\n",
      "ToTal Epoch: 2676, LR: 0.000000, Loss: 210261.3769646, Val MAE: 3127.9486643, Test MAE: 2017.4051895, Time: 11.592390775680542\n",
      "ToTal Epoch: 2677, LR: 0.000000, Loss: 210675.1484450, Val MAE: 3075.2289612, Test MAE: 2017.4051895, Time: 12.059913158416748\n",
      "ToTal Epoch: 2678, LR: 0.000000, Loss: 209690.9643147, Val MAE: 3116.8429928, Test MAE: 2017.4051895, Time: 13.179609060287476\n",
      "ToTal Epoch: 2679, LR: 0.000000, Loss: 209615.3911623, Val MAE: 3123.5065113, Test MAE: 2017.4051895, Time: 10.864774227142334\n",
      "ToTal Epoch: 2680, LR: 0.000000, Loss: 208948.8332489, Val MAE: 3114.5199161, Test MAE: 2017.4051895, Time: 10.694254398345947\n",
      "ToTal Epoch: 2681, LR: 0.000000, Loss: 209879.1919362, Val MAE: 3088.7907017, Test MAE: 2017.4051895, Time: 11.031829357147217\n",
      "ToTal Epoch: 2682, LR: 0.000000, Loss: 209071.8311183, Val MAE: 3092.8187342, Test MAE: 2017.4051895, Time: 11.730430364608765\n",
      "ToTal Epoch: 2683, LR: 0.000000, Loss: 209822.9368366, Val MAE: 3066.8064998, Test MAE: 2017.4051895, Time: 11.244824171066284\n",
      "ToTal Epoch: 2684, LR: 0.000000, Loss: 209332.4677017, Val MAE: 3098.1871752, Test MAE: 2017.4051895, Time: 12.065402269363403\n",
      "ToTal Epoch: 2685, LR: 0.000000, Loss: 210514.4992882, Val MAE: 3108.6775587, Test MAE: 2017.4051895, Time: 12.194891691207886\n",
      "ToTal Epoch: 2686, LR: 0.000000, Loss: 208878.9454737, Val MAE: 3174.9274727, Test MAE: 2017.4051895, Time: 11.780375480651855\n",
      "ToTal Epoch: 2687, LR: 0.000000, Loss: 210693.2131658, Val MAE: 3174.7166074, Test MAE: 2017.4051895, Time: 11.045416116714478\n",
      "ToTal Epoch: 2688, LR: 0.000000, Loss: 210063.4407299, Val MAE: 3177.2253449, Test MAE: 2017.4051895, Time: 11.008697271347046\n",
      "ToTal Epoch: 2689, LR: 0.000000, Loss: 210380.4562366, Val MAE: 3115.9385319, Test MAE: 2017.4051895, Time: 11.415266752243042\n",
      "ToTal Epoch: 2690, LR: 0.000000, Loss: 209760.8434529, Val MAE: 3153.9950604, Test MAE: 2017.4051895, Time: 10.537279605865479\n",
      "ToTal Epoch: 2691, LR: 0.000000, Loss: 209697.3975254, Val MAE: 3114.2857668, Test MAE: 2017.4051895, Time: 11.042946338653564\n",
      "ToTal Epoch: 2692, LR: 0.000000, Loss: 210278.4198729, Val MAE: 3042.1031252, Test MAE: 2017.4051895, Time: 11.655851125717163\n",
      "ToTal Epoch: 2693, LR: 0.000000, Loss: 208592.3008073, Val MAE: 3037.8785112, Test MAE: 2017.4051895, Time: 12.236539125442505\n",
      "ToTal Epoch: 2694, LR: 0.000000, Loss: 207496.7085845, Val MAE: 3066.6077018, Test MAE: 2017.4051895, Time: 12.00846004486084\n",
      "ToTal Epoch: 2695, LR: 0.000000, Loss: 210100.2291119, Val MAE: 3115.7517246, Test MAE: 2017.4051895, Time: 10.96516728401184\n",
      "ToTal Epoch: 2696, LR: 0.000000, Loss: 209399.2131849, Val MAE: 3180.4350875, Test MAE: 2017.4051895, Time: 10.678224325180054\n",
      "ToTal Epoch: 2697, LR: 0.000000, Loss: 209774.6412268, Val MAE: 3152.9076330, Test MAE: 2017.4051895, Time: 11.159425258636475\n",
      "ToTal Epoch: 2698, LR: 0.000000, Loss: 210082.8650265, Val MAE: 3122.8279351, Test MAE: 2017.4051895, Time: 11.006311416625977\n",
      "ToTal Epoch: 2699, LR: 0.000000, Loss: 208823.2124206, Val MAE: 3102.9819757, Test MAE: 2017.4051895, Time: 10.860418796539307\n",
      "ToTal Epoch: 2700, LR: 0.000000, Loss: 210683.7376774, Val MAE: 3082.5794686, Test MAE: 2017.4051895, Time: 11.989598989486694\n",
      "ToTal Epoch: 2701, LR: 0.000000, Loss: 209156.5718053, Val MAE: 3054.2381573, Test MAE: 2017.4051895, Time: 11.3997061252594\n",
      "ToTal Epoch: 2702, LR: 0.000000, Loss: 210974.3686619, Val MAE: 3079.5027660, Test MAE: 2017.4051895, Time: 11.994087934494019\n",
      "ToTal Epoch: 2703, LR: 0.000000, Loss: 210859.1209669, Val MAE: 3053.4515545, Test MAE: 2017.4051895, Time: 11.338363647460938\n",
      "ToTal Epoch: 2704, LR: 0.000000, Loss: 208583.7327856, Val MAE: 3060.6968729, Test MAE: 2017.4051895, Time: 11.731777906417847\n",
      "ToTal Epoch: 2705, LR: 0.000000, Loss: 209084.1052883, Val MAE: 2988.3617528, Test MAE: 2017.4051895, Time: 11.185810565948486\n",
      "ToTal Epoch: 2706, LR: 0.000000, Loss: 211077.4418191, Val MAE: 2974.7568362, Test MAE: 2017.4051895, Time: 11.763859510421753\n",
      "ToTal Epoch: 2707, LR: 0.000000, Loss: 210673.4962404, Val MAE: 3019.6878678, Test MAE: 2017.4051895, Time: 13.294113159179688\n",
      "ToTal Epoch: 2708, LR: 0.000000, Loss: 209220.4445039, Val MAE: 2986.9215442, Test MAE: 2017.4051895, Time: 13.772976875305176\n",
      "ToTal Epoch: 2709, LR: 0.000000, Loss: 211069.5594898, Val MAE: 3032.6276370, Test MAE: 2017.4051895, Time: 11.223559617996216\n",
      "ToTal Epoch: 2710, LR: 0.000000, Loss: 210763.2206755, Val MAE: 2994.7432928, Test MAE: 2017.4051895, Time: 13.116618871688843\n",
      "ToTal Epoch: 2711, LR: 0.000000, Loss: 209046.4268285, Val MAE: 2930.6533383, Test MAE: 2017.4051895, Time: 10.949160814285278\n",
      "ToTal Epoch: 2712, LR: 0.000000, Loss: 210057.4316152, Val MAE: 2985.2654781, Test MAE: 2017.4051895, Time: 11.62290620803833\n",
      "ToTal Epoch: 2713, LR: 0.000000, Loss: 210886.9666842, Val MAE: 2944.6294906, Test MAE: 2017.4051895, Time: 11.024218797683716\n",
      "ToTal Epoch: 2714, LR: 0.000000, Loss: 210064.4080065, Val MAE: 3003.5435584, Test MAE: 2017.4051895, Time: 10.964078903198242\n",
      "ToTal Epoch: 2715, LR: 0.000000, Loss: 211008.2643864, Val MAE: 3034.4995748, Test MAE: 2017.4051895, Time: 10.966439723968506\n",
      "ToTal Epoch: 2716, LR: 0.000000, Loss: 211764.5494482, Val MAE: 3102.6529705, Test MAE: 2017.4051895, Time: 12.761862993240356\n",
      "ToTal Epoch: 2717, LR: 0.000000, Loss: 209037.6340133, Val MAE: 3020.3663342, Test MAE: 2017.4051895, Time: 11.20606517791748\n",
      "ToTal Epoch: 2718, LR: 0.000000, Loss: 210529.8841829, Val MAE: 3038.6754806, Test MAE: 2017.4051895, Time: 11.529725790023804\n",
      "ToTal Epoch: 2719, LR: 0.000000, Loss: 209335.2172742, Val MAE: 3101.5851104, Test MAE: 2017.4051895, Time: 14.118671417236328\n",
      "ToTal Epoch: 2720, LR: 0.000000, Loss: 210710.5831749, Val MAE: 3153.8987856, Test MAE: 2017.4051895, Time: 13.005902767181396\n",
      "ToTal Epoch: 2721, LR: 0.000000, Loss: 210839.7991688, Val MAE: 3086.0400663, Test MAE: 2017.4051895, Time: 12.049574136734009\n",
      "ToTal Epoch: 2722, LR: 0.000000, Loss: 210598.4752878, Val MAE: 3083.5748634, Test MAE: 2017.4051895, Time: 10.607446432113647\n",
      "ToTal Epoch: 2723, LR: 0.000000, Loss: 210073.3771557, Val MAE: 3086.0827935, Test MAE: 2017.4051895, Time: 11.545128345489502\n",
      "ToTal Epoch: 2724, LR: 0.000000, Loss: 209924.0635169, Val MAE: 3087.4067635, Test MAE: 2017.4051895, Time: 10.7708420753479\n",
      "ToTal Epoch: 2725, LR: 0.000000, Loss: 210460.8085224, Val MAE: 2982.8187772, Test MAE: 2017.4051895, Time: 12.660564422607422\n",
      "ToTal Epoch: 2726, LR: 0.000000, Loss: 208621.1179286, Val MAE: 3113.7436989, Test MAE: 2017.4051895, Time: 11.075661897659302\n",
      "ToTal Epoch: 2727, LR: 0.000000, Loss: 208570.5887546, Val MAE: 3139.9526962, Test MAE: 2017.4051895, Time: 12.118920087814331\n",
      "ToTal Epoch: 2728, LR: 0.000000, Loss: 208393.6730712, Val MAE: 3140.7310728, Test MAE: 2017.4051895, Time: 11.111106157302856\n",
      "ToTal Epoch: 2729, LR: 0.000000, Loss: 209376.1076578, Val MAE: 3151.3745987, Test MAE: 2017.4051895, Time: 11.11331558227539\n",
      "ToTal Epoch: 2730, LR: 0.000000, Loss: 209981.9893183, Val MAE: 3081.0452591, Test MAE: 2017.4051895, Time: 11.23061490058899\n",
      "ToTal Epoch: 2731, LR: 0.000000, Loss: 209752.7618975, Val MAE: 3022.4300141, Test MAE: 2017.4051895, Time: 11.28777265548706\n",
      "ToTal Epoch: 2732, LR: 0.000000, Loss: 208426.0285673, Val MAE: 3109.7463263, Test MAE: 2017.4051895, Time: 11.124770641326904\n",
      "ToTal Epoch: 2733, LR: 0.000000, Loss: 211284.4752687, Val MAE: 3100.9714563, Test MAE: 2017.4051895, Time: 13.958383798599243\n",
      "ToTal Epoch: 2734, LR: 0.000000, Loss: 209899.9898725, Val MAE: 3079.0792966, Test MAE: 2017.4051895, Time: 11.103485345840454\n",
      "ToTal Epoch: 2735, LR: 0.000000, Loss: 209759.0583672, Val MAE: 3114.8395723, Test MAE: 2017.4051895, Time: 11.660229921340942\n",
      "ToTal Epoch: 2736, LR: 0.000000, Loss: 211479.3397984, Val MAE: 3055.1504338, Test MAE: 2017.4051895, Time: 11.493496656417847\n",
      "ToTal Epoch: 2737, LR: 0.000000, Loss: 209839.4466536, Val MAE: 3034.9819088, Test MAE: 2017.4051895, Time: 11.802167654037476\n",
      "ToTal Epoch: 2738, LR: 0.000000, Loss: 208801.3343142, Val MAE: 3058.2670689, Test MAE: 2017.4051895, Time: 11.054779529571533\n",
      "ToTal Epoch: 2739, LR: 0.000000, Loss: 211262.9592701, Val MAE: 3025.6235955, Test MAE: 2017.4051895, Time: 11.523243188858032\n",
      "ToTal Epoch: 2740, LR: 0.000000, Loss: 210030.8350260, Val MAE: 3004.2749513, Test MAE: 2017.4051895, Time: 11.266277074813843\n",
      "ToTal Epoch: 2741, LR: 0.000000, Loss: 210409.0576219, Val MAE: 3086.9033431, Test MAE: 2017.4051895, Time: 11.36732029914856\n",
      "ToTal Epoch: 2742, LR: 0.000000, Loss: 209277.0483352, Val MAE: 2969.9759755, Test MAE: 2017.4051895, Time: 10.80807375907898\n",
      "ToTal Epoch: 2743, LR: 0.000000, Loss: 208586.1927101, Val MAE: 3108.7513711, Test MAE: 2017.4051895, Time: 11.766992568969727\n",
      "ToTal Epoch: 2744, LR: 0.000000, Loss: 210250.6693928, Val MAE: 3017.8114633, Test MAE: 2017.4051895, Time: 11.09780240058899\n",
      "ToTal Epoch: 2745, LR: 0.000000, Loss: 209590.9760092, Val MAE: 3060.7814817, Test MAE: 2017.4051895, Time: 10.569857120513916\n",
      "ToTal Epoch: 2746, LR: 0.000000, Loss: 210548.8038982, Val MAE: 3143.6673068, Test MAE: 2017.4051895, Time: 12.361562967300415\n",
      "ToTal Epoch: 2747, LR: 0.000000, Loss: 210499.4745904, Val MAE: 3106.4401657, Test MAE: 2017.4051895, Time: 11.60728931427002\n",
      "ToTal Epoch: 2748, LR: 0.000000, Loss: 211153.4027612, Val MAE: 3123.6216799, Test MAE: 2017.4051895, Time: 11.51767611503601\n",
      "ToTal Epoch: 2749, LR: 0.000000, Loss: 207840.2275832, Val MAE: 3177.6573511, Test MAE: 2017.4051895, Time: 10.817737340927124\n",
      "ToTal Epoch: 2750, LR: 0.000000, Loss: 209841.7078871, Val MAE: 3117.9065725, Test MAE: 2017.4051895, Time: 11.977379560470581\n",
      "ToTal Epoch: 2751, LR: 0.000000, Loss: 210585.0855969, Val MAE: 3106.6877102, Test MAE: 2017.4051895, Time: 13.004399299621582\n",
      "ToTal Epoch: 2752, LR: 0.000000, Loss: 209732.6643290, Val MAE: 3088.0485506, Test MAE: 2017.4051895, Time: 11.749987602233887\n",
      "ToTal Epoch: 2753, LR: 0.000000, Loss: 211493.4957053, Val MAE: 3050.5069413, Test MAE: 2017.4051895, Time: 11.944838523864746\n",
      "ToTal Epoch: 2754, LR: 0.000000, Loss: 210279.8441122, Val MAE: 3036.0029953, Test MAE: 2017.4051895, Time: 12.261645793914795\n",
      "ToTal Epoch: 2755, LR: 0.000000, Loss: 210309.5769550, Val MAE: 3011.3519739, Test MAE: 2017.4051895, Time: 12.786557674407959\n",
      "ToTal Epoch: 2756, LR: 0.000000, Loss: 208349.4858835, Val MAE: 3066.6603751, Test MAE: 2017.4051895, Time: 11.225562810897827\n",
      "ToTal Epoch: 2757, LR: 0.000000, Loss: 211186.1292314, Val MAE: 3057.3107468, Test MAE: 2017.4051895, Time: 10.33079218864441\n",
      "ToTal Epoch: 2758, LR: 0.000000, Loss: 210076.2271246, Val MAE: 3141.0026848, Test MAE: 2017.4051895, Time: 11.148571014404297\n",
      "ToTal Epoch: 2759, LR: 0.000000, Loss: 208485.6183060, Val MAE: 3115.2306428, Test MAE: 2017.4051895, Time: 11.864210367202759\n",
      "ToTal Epoch: 2760, LR: 0.000000, Loss: 211278.1932833, Val MAE: 3099.6727003, Test MAE: 2017.4051895, Time: 11.68823766708374\n",
      "ToTal Epoch: 2761, LR: 0.000000, Loss: 210387.0437300, Val MAE: 3070.2880408, Test MAE: 2017.4051895, Time: 11.247820615768433\n",
      "ToTal Epoch: 2762, LR: 0.000000, Loss: 210425.5485979, Val MAE: 3091.1410514, Test MAE: 2017.4051895, Time: 12.064234495162964\n",
      "ToTal Epoch: 2763, LR: 0.000000, Loss: 209991.4199016, Val MAE: 3035.0922810, Test MAE: 2017.4051895, Time: 11.410660743713379\n",
      "ToTal Epoch: 2764, LR: 0.000000, Loss: 209856.2243730, Val MAE: 3062.8142055, Test MAE: 2017.4051895, Time: 11.695788621902466\n",
      "ToTal Epoch: 2765, LR: 0.000000, Loss: 211727.6354464, Val MAE: 2974.3290816, Test MAE: 2017.4051895, Time: 11.264948844909668\n",
      "ToTal Epoch: 2766, LR: 0.000000, Loss: 211352.4603640, Val MAE: 3043.8634488, Test MAE: 2017.4051895, Time: 11.043884515762329\n",
      "ToTal Epoch: 2767, LR: 0.000000, Loss: 210174.0587589, Val MAE: 3080.4881478, Test MAE: 2017.4051895, Time: 11.292217254638672\n",
      "ToTal Epoch: 2768, LR: 0.000000, Loss: 210757.5171834, Val MAE: 3037.1132911, Test MAE: 2017.4051895, Time: 11.655541896820068\n",
      "ToTal Epoch: 2769, LR: 0.000000, Loss: 210633.7809965, Val MAE: 3059.8297648, Test MAE: 2017.4051895, Time: 11.726478099822998\n",
      "ToTal Epoch: 2770, LR: 0.000000, Loss: 210233.4649405, Val MAE: 3065.1490102, Test MAE: 2017.4051895, Time: 11.843374729156494\n",
      "ToTal Epoch: 2771, LR: 0.000000, Loss: 208579.7711938, Val MAE: 3072.9778243, Test MAE: 2017.4051895, Time: 11.42272663116455\n",
      "ToTal Epoch: 2772, LR: 0.000000, Loss: 208697.6880524, Val MAE: 3089.3599423, Test MAE: 2017.4051895, Time: 10.769683361053467\n",
      "ToTal Epoch: 2773, LR: 0.000000, Loss: 211156.4270004, Val MAE: 3116.5795068, Test MAE: 2017.4051895, Time: 13.364769220352173\n",
      "ToTal Epoch: 2774, LR: 0.000000, Loss: 211406.5806908, Val MAE: 3087.3120079, Test MAE: 2017.4051895, Time: 11.960631608963013\n",
      "ToTal Epoch: 2775, LR: 0.000000, Loss: 208208.9359385, Val MAE: 3113.3027545, Test MAE: 2017.4051895, Time: 11.71549129486084\n",
      "ToTal Epoch: 2776, LR: 0.000000, Loss: 209022.6872402, Val MAE: 3119.6777450, Test MAE: 2017.4051895, Time: 11.44961953163147\n",
      "ToTal Epoch: 2777, LR: 0.000000, Loss: 208684.2045001, Val MAE: 3130.3760653, Test MAE: 2017.4051895, Time: 12.688779592514038\n",
      "ToTal Epoch: 2778, LR: 0.000000, Loss: 210178.8455740, Val MAE: 3147.1781749, Test MAE: 2017.4051895, Time: 11.48055362701416\n",
      "ToTal Epoch: 2779, LR: 0.000000, Loss: 209902.0626188, Val MAE: 3183.4905412, Test MAE: 2017.4051895, Time: 11.931073427200317\n",
      "ToTal Epoch: 2780, LR: 0.000000, Loss: 210636.4754598, Val MAE: 3164.2293578, Test MAE: 2017.4051895, Time: 16.004892587661743\n",
      "ToTal Epoch: 2781, LR: 0.000000, Loss: 210569.4009650, Val MAE: 3104.6945846, Test MAE: 2017.4051895, Time: 12.32530665397644\n",
      "ToTal Epoch: 2782, LR: 0.000000, Loss: 210378.2345961, Val MAE: 3069.8271660, Test MAE: 2017.4051895, Time: 11.346623659133911\n",
      "ToTal Epoch: 2783, LR: 0.000000, Loss: 211266.6729852, Val MAE: 2975.4299950, Test MAE: 2017.4051895, Time: 11.374338865280151\n",
      "ToTal Epoch: 2784, LR: 0.000000, Loss: 208978.7298524, Val MAE: 2968.7820215, Test MAE: 2017.4051895, Time: 11.49006175994873\n",
      "ToTal Epoch: 2785, LR: 0.000000, Loss: 210046.3375914, Val MAE: 2962.8293539, Test MAE: 2017.4051895, Time: 11.461508750915527\n",
      "ToTal Epoch: 2786, LR: 0.000000, Loss: 209308.3526680, Val MAE: 2972.9885873, Test MAE: 2017.4051895, Time: 14.25136947631836\n",
      "ToTal Epoch: 2787, LR: 0.000000, Loss: 210465.8541060, Val MAE: 3035.4497487, Test MAE: 2017.4051895, Time: 11.667622327804565\n",
      "ToTal Epoch: 2788, LR: 0.000000, Loss: 209264.2596474, Val MAE: 3044.7942368, Test MAE: 2017.4051895, Time: 12.166930675506592\n",
      "ToTal Epoch: 2789, LR: 0.000000, Loss: 209507.4083218, Val MAE: 3057.2833782, Test MAE: 2017.4051895, Time: 11.19441556930542\n",
      "ToTal Epoch: 2790, LR: 0.000000, Loss: 210958.9312186, Val MAE: 3112.3246723, Test MAE: 2017.4051895, Time: 10.361264705657959\n",
      "ToTal Epoch: 2791, LR: 0.000000, Loss: 211068.6650552, Val MAE: 3125.9689817, Test MAE: 2017.4051895, Time: 11.023109912872314\n",
      "ToTal Epoch: 2792, LR: 0.000000, Loss: 209867.1029666, Val MAE: 3184.0507720, Test MAE: 2017.4051895, Time: 12.860087633132935\n",
      "ToTal Epoch: 2793, LR: 0.000000, Loss: 209992.5115750, Val MAE: 3126.6315448, Test MAE: 2017.4051895, Time: 11.80496621131897\n",
      "ToTal Epoch: 2794, LR: 0.000000, Loss: 208449.5747385, Val MAE: 3112.2226936, Test MAE: 2017.4051895, Time: 11.378629207611084\n",
      "ToTal Epoch: 2795, LR: 0.000000, Loss: 211086.3011704, Val MAE: 3106.8980691, Test MAE: 2017.4051895, Time: 11.57222056388855\n",
      "ToTal Epoch: 2796, LR: 0.000000, Loss: 210081.6273635, Val MAE: 3021.3246866, Test MAE: 2017.4051895, Time: 11.225216627120972\n",
      "ToTal Epoch: 2797, LR: 0.000000, Loss: 210042.6644628, Val MAE: 3000.0118522, Test MAE: 2017.4051895, Time: 10.93601679801941\n",
      "ToTal Epoch: 2798, LR: 0.000000, Loss: 209990.0968614, Val MAE: 3065.9592028, Test MAE: 2017.4051895, Time: 11.394028186798096\n",
      "ToTal Epoch: 2799, LR: 0.000000, Loss: 209602.3114126, Val MAE: 3065.2604668, Test MAE: 2017.4051895, Time: 11.395254850387573\n",
      "ToTal Epoch: 2800, LR: 0.000000, Loss: 210606.2855396, Val MAE: 3016.7235630, Test MAE: 2017.4051895, Time: 11.19243836402893\n",
      "ToTal Epoch: 2801, LR: 0.000000, Loss: 210272.3303874, Val MAE: 3058.6578289, Test MAE: 2017.4051895, Time: 11.033900737762451\n",
      "ToTal Epoch: 2802, LR: 0.000000, Loss: 208241.1770888, Val MAE: 3081.9292259, Test MAE: 2017.4051895, Time: 11.867562532424927\n",
      "ToTal Epoch: 2803, LR: 0.000000, Loss: 209051.5481202, Val MAE: 3098.1298297, Test MAE: 2017.4051895, Time: 12.278238773345947\n",
      "ToTal Epoch: 2804, LR: 0.000000, Loss: 209399.2312999, Val MAE: 3099.6456805, Test MAE: 2017.4051895, Time: 11.680680274963379\n",
      "ToTal Epoch: 2805, LR: 0.000000, Loss: 211222.9247982, Val MAE: 3107.5558693, Test MAE: 2017.4051895, Time: 11.321182012557983\n",
      "ToTal Epoch: 2806, LR: 0.000000, Loss: 209135.0992978, Val MAE: 3109.8636637, Test MAE: 2017.4051895, Time: 11.562998294830322\n",
      "ToTal Epoch: 2807, LR: 0.000000, Loss: 209778.2537811, Val MAE: 3179.7600799, Test MAE: 2017.4051895, Time: 11.939061641693115\n",
      "ToTal Epoch: 2808, LR: 0.000000, Loss: 208518.4331343, Val MAE: 3126.0660208, Test MAE: 2017.4051895, Time: 12.231000661849976\n",
      "ToTal Epoch: 2809, LR: 0.000000, Loss: 210251.4721445, Val MAE: 3086.3029026, Test MAE: 2017.4051895, Time: 11.277871131896973\n",
      "ToTal Epoch: 2810, LR: 0.000000, Loss: 210431.2599245, Val MAE: 3098.0714142, Test MAE: 2017.4051895, Time: 11.32251501083374\n",
      "ToTal Epoch: 2811, LR: 0.000000, Loss: 208966.7344002, Val MAE: 3078.1098850, Test MAE: 2017.4051895, Time: 11.071492671966553\n",
      "ToTal Epoch: 2812, LR: 0.000000, Loss: 209944.7192471, Val MAE: 3067.8489977, Test MAE: 2017.4051895, Time: 12.482704401016235\n",
      "ToTal Epoch: 2813, LR: 0.000000, Loss: 211358.2565710, Val MAE: 3139.2687505, Test MAE: 2017.4051895, Time: 11.126289367675781\n",
      "ToTal Epoch: 2814, LR: 0.000000, Loss: 210157.0945015, Val MAE: 3065.2933626, Test MAE: 2017.4051895, Time: 11.409133434295654\n",
      "ToTal Epoch: 2815, LR: 0.000000, Loss: 211090.8229112, Val MAE: 3039.4691632, Test MAE: 2017.4051895, Time: 11.628774642944336\n",
      "ToTal Epoch: 2816, LR: 0.000000, Loss: 209838.4250322, Val MAE: 3023.1387535, Test MAE: 2017.4051895, Time: 12.1415696144104\n",
      "ToTal Epoch: 2817, LR: 0.000000, Loss: 210044.4147327, Val MAE: 3064.7893879, Test MAE: 2017.4051895, Time: 12.33572006225586\n",
      "ToTal Epoch: 2818, LR: 0.000000, Loss: 210225.7098361, Val MAE: 3092.7701215, Test MAE: 2017.4051895, Time: 11.633671760559082\n",
      "ToTal Epoch: 2819, LR: 0.000000, Loss: 211268.2430994, Val MAE: 3033.3506315, Test MAE: 2017.4051895, Time: 11.948407649993896\n",
      "ToTal Epoch: 2820, LR: 0.000000, Loss: 209719.3407538, Val MAE: 3082.3658803, Test MAE: 2017.4051895, Time: 11.522533416748047\n",
      "ToTal Epoch: 2821, LR: 0.000000, Loss: 208967.1220752, Val MAE: 3066.3191403, Test MAE: 2017.4051895, Time: 12.63718867301941\n",
      "ToTal Epoch: 2822, LR: 0.000000, Loss: 210035.9172598, Val MAE: 3090.9570483, Test MAE: 2017.4051895, Time: 10.790348529815674\n",
      "ToTal Epoch: 2823, LR: 0.000000, Loss: 208614.8722113, Val MAE: 3016.9734484, Test MAE: 2017.4051895, Time: 10.634578943252563\n",
      "ToTal Epoch: 2824, LR: 0.000000, Loss: 210924.9687670, Val MAE: 2982.2815056, Test MAE: 2017.4051895, Time: 11.352910280227661\n",
      "ToTal Epoch: 2825, LR: 0.000000, Loss: 209387.4642717, Val MAE: 2984.3084251, Test MAE: 2017.4051895, Time: 11.7369704246521\n",
      "ToTal Epoch: 2826, LR: 0.000000, Loss: 211257.5636555, Val MAE: 3019.8551555, Test MAE: 2017.4051895, Time: 12.21427297592163\n",
      "ToTal Epoch: 2827, LR: 0.000000, Loss: 210291.0927626, Val MAE: 3035.2952830, Test MAE: 2017.4051895, Time: 11.1723792552948\n",
      "ToTal Epoch: 2828, LR: 0.000000, Loss: 210478.9450150, Val MAE: 2996.8007194, Test MAE: 2017.4051895, Time: 10.95128059387207\n",
      "ToTal Epoch: 2829, LR: 0.000000, Loss: 210337.2370515, Val MAE: 2999.5353990, Test MAE: 2017.4051895, Time: 11.07877492904663\n",
      "ToTal Epoch: 2830, LR: 0.000000, Loss: 211474.8451154, Val MAE: 3083.3745748, Test MAE: 2017.4051895, Time: 13.171633005142212\n",
      "ToTal Epoch: 2831, LR: 0.000000, Loss: 211394.6242201, Val MAE: 3066.2561291, Test MAE: 2017.4051895, Time: 12.549889326095581\n",
      "ToTal Epoch: 2832, LR: 0.000000, Loss: 211376.1528687, Val MAE: 3070.2673460, Test MAE: 2017.4051895, Time: 11.640544652938843\n",
      "ToTal Epoch: 2833, LR: 0.000000, Loss: 212403.9588401, Val MAE: 3042.0261074, Test MAE: 2017.4051895, Time: 11.5589120388031\n",
      "ToTal Epoch: 2834, LR: 0.000000, Loss: 209599.7457412, Val MAE: 3028.3644472, Test MAE: 2017.4051895, Time: 12.362943887710571\n",
      "ToTal Epoch: 2835, LR: 0.000000, Loss: 209997.3864042, Val MAE: 3021.9178562, Test MAE: 2017.4051895, Time: 12.17270827293396\n",
      "ToTal Epoch: 2836, LR: 0.000000, Loss: 210860.6161372, Val MAE: 3046.7759067, Test MAE: 2017.4051895, Time: 12.116976261138916\n",
      "ToTal Epoch: 2837, LR: 0.000000, Loss: 209348.5338939, Val MAE: 3087.8519930, Test MAE: 2017.4051895, Time: 12.287773370742798\n",
      "ToTal Epoch: 2838, LR: 0.000000, Loss: 210209.3491043, Val MAE: 3071.6361977, Test MAE: 2017.4051895, Time: 13.76392126083374\n",
      "ToTal Epoch: 2839, LR: 0.000000, Loss: 209665.3459323, Val MAE: 3078.0619506, Test MAE: 2017.4051895, Time: 11.840598821640015\n",
      "ToTal Epoch: 2840, LR: 0.000000, Loss: 211420.3635217, Val MAE: 2982.6915367, Test MAE: 2017.4051895, Time: 12.070763349533081\n",
      "ToTal Epoch: 2841, LR: 0.000000, Loss: 209044.7955668, Val MAE: 2975.2512086, Test MAE: 2017.4051895, Time: 11.23703122138977\n",
      "ToTal Epoch: 2842, LR: 0.000000, Loss: 210312.9779009, Val MAE: 3054.3309161, Test MAE: 2017.4051895, Time: 11.699843645095825\n",
      "ToTal Epoch: 2843, LR: 0.000000, Loss: 210777.9694836, Val MAE: 2984.6674453, Test MAE: 2017.4051895, Time: 13.185556173324585\n",
      "ToTal Epoch: 2844, LR: 0.000000, Loss: 210600.3283237, Val MAE: 2956.8847741, Test MAE: 2017.4051895, Time: 11.60722827911377\n",
      "ToTal Epoch: 2845, LR: 0.000000, Loss: 210410.2263030, Val MAE: 2961.4851095, Test MAE: 2017.4051895, Time: 11.313550472259521\n",
      "ToTal Epoch: 2846, LR: 0.000000, Loss: 210356.2593417, Val MAE: 3007.7339391, Test MAE: 2017.4051895, Time: 12.035993814468384\n",
      "ToTal Epoch: 2847, LR: 0.000000, Loss: 210409.7678116, Val MAE: 3019.2462690, Test MAE: 2017.4051895, Time: 12.112151145935059\n",
      "ToTal Epoch: 2848, LR: 0.000000, Loss: 210789.3053838, Val MAE: 3058.2544523, Test MAE: 2017.4051895, Time: 12.026821374893188\n",
      "ToTal Epoch: 2849, LR: 0.000000, Loss: 208711.1188267, Val MAE: 3086.5600875, Test MAE: 2017.4051895, Time: 11.303667306900024\n",
      "ToTal Epoch: 2850, LR: 0.000000, Loss: 210185.7066641, Val MAE: 3081.4660628, Test MAE: 2017.4051895, Time: 11.446768283843994\n",
      "ToTal Epoch: 2851, LR: 0.000000, Loss: 210690.3548655, Val MAE: 3133.4590595, Test MAE: 2017.4051895, Time: 11.949941158294678\n",
      "ToTal Epoch: 2852, LR: 0.000000, Loss: 210685.6990971, Val MAE: 3123.6584117, Test MAE: 2017.4051895, Time: 10.815179586410522\n",
      "ToTal Epoch: 2853, LR: 0.000000, Loss: 210294.8335738, Val MAE: 3073.8577305, Test MAE: 2017.4051895, Time: 11.485773086547852\n",
      "ToTal Epoch: 2854, LR: 0.000000, Loss: 209346.9773086, Val MAE: 3110.6522969, Test MAE: 2017.4051895, Time: 10.965531349182129\n",
      "ToTal Epoch: 2855, LR: 0.000000, Loss: 211607.7598433, Val MAE: 3001.5107057, Test MAE: 2017.4051895, Time: 11.842682361602783\n",
      "ToTal Epoch: 2856, LR: 0.000000, Loss: 209835.8573353, Val MAE: 3000.9794103, Test MAE: 2017.4051895, Time: 11.84659481048584\n",
      "ToTal Epoch: 2857, LR: 0.000000, Loss: 211414.8593322, Val MAE: 3022.7241076, Test MAE: 2017.4051895, Time: 14.385156869888306\n",
      "ToTal Epoch: 2858, LR: 0.000000, Loss: 211053.8175703, Val MAE: 2971.2210502, Test MAE: 2017.4051895, Time: 11.694110631942749\n",
      "ToTal Epoch: 2859, LR: 0.000000, Loss: 209429.1439927, Val MAE: 2980.1212642, Test MAE: 2017.4051895, Time: 11.59968090057373\n",
      "ToTal Epoch: 2860, LR: 0.000000, Loss: 209465.6003440, Val MAE: 2971.6186177, Test MAE: 2017.4051895, Time: 12.86717677116394\n",
      "ToTal Epoch: 2861, LR: 0.000000, Loss: 210468.5019061, Val MAE: 3063.1931944, Test MAE: 2017.4051895, Time: 11.832504987716675\n",
      "ToTal Epoch: 2862, LR: 0.000000, Loss: 209441.6801796, Val MAE: 3138.9249360, Test MAE: 2017.4051895, Time: 11.239970684051514\n",
      "ToTal Epoch: 2863, LR: 0.000000, Loss: 208852.0943200, Val MAE: 3134.0127647, Test MAE: 2017.4051895, Time: 12.729716777801514\n",
      "ToTal Epoch: 2864, LR: 0.000000, Loss: 208915.6265036, Val MAE: 3137.4224232, Test MAE: 2017.4051895, Time: 12.080596446990967\n",
      "ToTal Epoch: 2865, LR: 0.000000, Loss: 208544.1966273, Val MAE: 3049.4725359, Test MAE: 2017.4051895, Time: 11.614123106002808\n",
      "ToTal Epoch: 2866, LR: 0.000000, Loss: 210233.6980270, Val MAE: 3027.2007710, Test MAE: 2017.4051895, Time: 11.002512693405151\n",
      "ToTal Epoch: 2867, LR: 0.000000, Loss: 210097.3264797, Val MAE: 2985.1316642, Test MAE: 2017.4051895, Time: 11.212684631347656\n",
      "ToTal Epoch: 2868, LR: 0.000000, Loss: 210778.8744279, Val MAE: 3009.9826540, Test MAE: 2017.4051895, Time: 11.42182731628418\n",
      "ToTal Epoch: 2869, LR: 0.000000, Loss: 209473.6268285, Val MAE: 3000.7949820, Test MAE: 2017.4051895, Time: 10.95548939704895\n",
      "ToTal Epoch: 2870, LR: 0.000000, Loss: 209406.4404720, Val MAE: 3015.2363229, Test MAE: 2017.4051895, Time: 11.182599306106567\n",
      "ToTal Epoch: 2871, LR: 0.000000, Loss: 209771.7877801, Val MAE: 3078.2166648, Test MAE: 2017.4051895, Time: 11.469377994537354\n",
      "ToTal Epoch: 2872, LR: 0.000000, Loss: 209505.8161181, Val MAE: 3051.7672122, Test MAE: 2017.4051895, Time: 11.65649700164795\n",
      "ToTal Epoch: 2873, LR: 0.000000, Loss: 209702.1527731, Val MAE: 3064.5348831, Test MAE: 2017.4051895, Time: 11.333594560623169\n",
      "ToTal Epoch: 2874, LR: 0.000000, Loss: 211461.6825873, Val MAE: 3076.0938814, Test MAE: 2017.4051895, Time: 11.012949705123901\n",
      "ToTal Epoch: 2875, LR: 0.000000, Loss: 211347.3835188, Val MAE: 3043.9634784, Test MAE: 2017.4051895, Time: 11.882251739501953\n",
      "ToTal Epoch: 2876, LR: 0.000000, Loss: 211581.3769646, Val MAE: 2996.5697374, Test MAE: 2017.4051895, Time: 12.162138938903809\n",
      "ToTal Epoch: 2877, LR: 0.000000, Loss: 210353.2268093, Val MAE: 3011.6657829, Test MAE: 2017.4051895, Time: 11.201344728469849\n",
      "ToTal Epoch: 2878, LR: 0.000000, Loss: 209902.6385516, Val MAE: 3010.1824266, Test MAE: 2017.4051895, Time: 11.678009033203125\n",
      "ToTal Epoch: 2879, LR: 0.000000, Loss: 211008.0429943, Val MAE: 3015.5907714, Test MAE: 2017.4051895, Time: 11.126322507858276\n",
      "ToTal Epoch: 2880, LR: 0.000000, Loss: 210512.0567525, Val MAE: 3026.2894023, Test MAE: 2017.4051895, Time: 11.43962574005127\n",
      "ToTal Epoch: 2881, LR: 0.000000, Loss: 212529.9021067, Val MAE: 2997.7354487, Test MAE: 2017.4051895, Time: 13.19897198677063\n",
      "ToTal Epoch: 2882, LR: 0.000000, Loss: 209944.4479817, Val MAE: 3060.7750946, Test MAE: 2017.4051895, Time: 11.349406480789185\n",
      "ToTal Epoch: 2883, LR: 0.000000, Loss: 209897.2861988, Val MAE: 3021.4906367, Test MAE: 2017.4051895, Time: 10.515689373016357\n",
      "ToTal Epoch: 2884, LR: 0.000000, Loss: 211573.6661921, Val MAE: 3067.1752369, Test MAE: 2017.4051895, Time: 11.328851461410522\n",
      "ToTal Epoch: 2885, LR: 0.000000, Loss: 208835.1737830, Val MAE: 3063.9055406, Test MAE: 2017.4051895, Time: 11.18161129951477\n",
      "ToTal Epoch: 2886, LR: 0.000000, Loss: 209132.8778102, Val MAE: 3033.4510577, Test MAE: 2017.4051895, Time: 11.063270807266235\n",
      "ToTal Epoch: 2887, LR: 0.000000, Loss: 211089.4190799, Val MAE: 3078.0584489, Test MAE: 2017.4051895, Time: 11.98800778388977\n",
      "ToTal Epoch: 2888, LR: 0.000000, Loss: 208366.5974681, Val MAE: 3114.9421912, Test MAE: 2017.4051895, Time: 12.097666263580322\n",
      "ToTal Epoch: 2889, LR: 0.000000, Loss: 211101.6915301, Val MAE: 3100.6541074, Test MAE: 2017.4051895, Time: 12.130029678344727\n",
      "ToTal Epoch: 2890, LR: 0.000000, Loss: 211383.1364831, Val MAE: 3072.1480929, Test MAE: 2017.4051895, Time: 10.862156391143799\n",
      "ToTal Epoch: 2891, LR: 0.000000, Loss: 212588.9992643, Val MAE: 3049.0791628, Test MAE: 2017.4051895, Time: 11.452465534210205\n",
      "ToTal Epoch: 2892, LR: 0.000000, Loss: 210125.5868533, Val MAE: 3061.9153052, Test MAE: 2017.4051895, Time: 10.91266393661499\n",
      "ToTal Epoch: 2893, LR: 0.000000, Loss: 211425.1616873, Val MAE: 3021.8239700, Test MAE: 2017.4051895, Time: 11.038145303726196\n",
      "ToTal Epoch: 2894, LR: 0.000000, Loss: 209494.3175273, Val MAE: 3084.9069546, Test MAE: 2017.4051895, Time: 12.308770656585693\n",
      "ToTal Epoch: 2895, LR: 0.000000, Loss: 209808.8560646, Val MAE: 3102.9803036, Test MAE: 2017.4051895, Time: 12.67404556274414\n",
      "ToTal Epoch: 2896, LR: 0.000000, Loss: 209584.3614962, Val MAE: 3118.0055033, Test MAE: 2017.4051895, Time: 11.75083303451538\n",
      "ToTal Epoch: 2897, LR: 0.000000, Loss: 211298.6872785, Val MAE: 3139.6127847, Test MAE: 2017.4051895, Time: 11.123850584030151\n",
      "ToTal Epoch: 2898, LR: 0.000000, Loss: 209674.1777289, Val MAE: 3200.0647692, Test MAE: 2017.4051895, Time: 11.170080661773682\n",
      "ToTal Epoch: 2899, LR: 0.000000, Loss: 208175.8939856, Val MAE: 3205.8430597, Test MAE: 2017.4051895, Time: 11.675516843795776\n",
      "ToTal Epoch: 2900, LR: 0.000000, Loss: 211859.8070797, Val MAE: 3182.0922476, Test MAE: 2017.4051895, Time: 10.933840274810791\n",
      "ToTal Epoch: 2901, LR: 0.000000, Loss: 209926.6447045, Val MAE: 3127.8475455, Test MAE: 2017.4051895, Time: 11.993332624435425\n",
      "ToTal Epoch: 2902, LR: 0.000000, Loss: 208609.3773086, Val MAE: 3100.6150443, Test MAE: 2017.4051895, Time: 11.431040525436401\n",
      "ToTal Epoch: 2903, LR: 0.000000, Loss: 209703.4700807, Val MAE: 3049.0933845, Test MAE: 2017.4051895, Time: 11.502102136611938\n",
      "ToTal Epoch: 2904, LR: 0.000000, Loss: 209692.4653705, Val MAE: 3022.7152746, Test MAE: 2017.4051895, Time: 11.005566120147705\n",
      "ToTal Epoch: 2905, LR: 0.000000, Loss: 210858.1114986, Val MAE: 2977.6088005, Test MAE: 2017.4051895, Time: 11.804910659790039\n",
      "ToTal Epoch: 2906, LR: 0.000000, Loss: 210153.8129078, Val MAE: 3041.3675428, Test MAE: 2017.4051895, Time: 11.645103931427002\n",
      "ToTal Epoch: 2907, LR: 0.000000, Loss: 210354.4062294, Val MAE: 3057.5883637, Test MAE: 2017.4051895, Time: 11.902325630187988\n",
      "ToTal Epoch: 2908, LR: 0.000000, Loss: 208677.8059141, Val MAE: 3002.2752188, Test MAE: 2017.4051895, Time: 11.062757968902588\n",
      "ToTal Epoch: 2909, LR: 0.000000, Loss: 210538.0546697, Val MAE: 3014.5489423, Test MAE: 2017.4051895, Time: 11.368682146072388\n",
      "ToTal Epoch: 2910, LR: 0.000000, Loss: 209692.6465963, Val MAE: 3052.5200451, Test MAE: 2017.4051895, Time: 11.145756483078003\n",
      "ToTal Epoch: 2911, LR: 0.000000, Loss: 210114.1017150, Val MAE: 3061.9262545, Test MAE: 2017.4051895, Time: 11.610021591186523\n",
      "ToTal Epoch: 2912, LR: 0.000000, Loss: 211378.1406583, Val MAE: 3092.2705706, Test MAE: 2017.4051895, Time: 11.482810974121094\n",
      "ToTal Epoch: 2913, LR: 0.000000, Loss: 210274.9056896, Val MAE: 3086.7125277, Test MAE: 2017.4051895, Time: 11.264724016189575\n",
      "ToTal Epoch: 2914, LR: 0.000000, Loss: 208974.8740840, Val MAE: 3057.6411517, Test MAE: 2017.4051895, Time: 11.317898035049438\n",
      "ToTal Epoch: 2915, LR: 0.000000, Loss: 208934.2892466, Val MAE: 3039.8782294, Test MAE: 2017.4051895, Time: 11.245540142059326\n",
      "ToTal Epoch: 2916, LR: 0.000000, Loss: 210706.7121961, Val MAE: 3050.4722684, Test MAE: 2017.4051895, Time: 11.767287015914917\n",
      "ToTal Epoch: 2917, LR: 0.000000, Loss: 209699.7411551, Val MAE: 3087.7224260, Test MAE: 2017.4051895, Time: 10.811323165893555\n",
      "ToTal Epoch: 2918, LR: 0.000000, Loss: 209301.9112406, Val MAE: 3136.9359140, Test MAE: 2017.4051895, Time: 11.478444576263428\n",
      "ToTal Epoch: 2919, LR: 0.000000, Loss: 209043.9102279, Val MAE: 3201.9284568, Test MAE: 2017.4051895, Time: 10.791017532348633\n",
      "ToTal Epoch: 2920, LR: 0.000000, Loss: 210646.7811016, Val MAE: 3187.6371436, Test MAE: 2017.4051895, Time: 13.22391676902771\n",
      "ToTal Epoch: 2921, LR: 0.000000, Loss: 209782.3668657, Val MAE: 3169.5111356, Test MAE: 2017.4051895, Time: 11.34678840637207\n",
      "ToTal Epoch: 2922, LR: 0.000000, Loss: 208170.4566378, Val MAE: 3159.0903606, Test MAE: 2017.4051895, Time: 11.519802570343018\n",
      "ToTal Epoch: 2923, LR: 0.000000, Loss: 211112.2374051, Val MAE: 3130.3247392, Test MAE: 2017.4051895, Time: 10.94108271598816\n",
      "ToTal Epoch: 2924, LR: 0.000000, Loss: 209929.1073043, Val MAE: 3136.4647252, Test MAE: 2017.4051895, Time: 12.01076078414917\n",
      "ToTal Epoch: 2925, LR: 0.000000, Loss: 209190.5461807, Val MAE: 3086.0391921, Test MAE: 2017.4051895, Time: 10.934558153152466\n",
      "ToTal Epoch: 2926, LR: 0.000000, Loss: 210315.7026704, Val MAE: 3070.8177836, Test MAE: 2017.4051895, Time: 11.642921686172485\n",
      "ToTal Epoch: 2927, LR: 0.000000, Loss: 208062.5023838, Val MAE: 3101.5069174, Test MAE: 2017.4051895, Time: 11.049269676208496\n",
      "ToTal Epoch: 2928, LR: 0.000000, Loss: 208464.2657240, Val MAE: 3089.5948225, Test MAE: 2017.4051895, Time: 10.763481855392456\n",
      "ToTal Epoch: 2929, LR: 0.000000, Loss: 211259.3845889, Val MAE: 3085.6452171, Test MAE: 2017.4051895, Time: 11.61274003982544\n",
      "ToTal Epoch: 2930, LR: 0.000000, Loss: 211194.1678689, Val MAE: 3092.9615388, Test MAE: 2017.4051895, Time: 11.446382761001587\n",
      "ToTal Epoch: 2931, LR: 0.000000, Loss: 210865.0897244, Val MAE: 3042.2930282, Test MAE: 2017.4051895, Time: 10.689539670944214\n",
      "ToTal Epoch: 2932, LR: 0.000000, Loss: 210681.2598672, Val MAE: 3031.1163724, Test MAE: 2017.4051895, Time: 10.348045825958252\n",
      "ToTal Epoch: 2933, LR: 0.000000, Loss: 209567.7098553, Val MAE: 2993.5938240, Test MAE: 2017.4051895, Time: 11.053068161010742\n",
      "ToTal Epoch: 2934, LR: 0.000000, Loss: 210060.7609803, Val MAE: 3057.7525415, Test MAE: 2017.4051895, Time: 11.469776391983032\n",
      "ToTal Epoch: 2935, LR: 0.000000, Loss: 210987.4078250, Val MAE: 3076.8595888, Test MAE: 2017.4051895, Time: 11.449007034301758\n",
      "ToTal Epoch: 2936, LR: 0.000000, Loss: 210420.8950461, Val MAE: 3131.4045851, Test MAE: 2017.4051895, Time: 12.099385023117065\n",
      "ToTal Epoch: 2937, LR: 0.000000, Loss: 211015.8176659, Val MAE: 3111.4107143, Test MAE: 2017.4051895, Time: 11.193920135498047\n",
      "ToTal Epoch: 2938, LR: 0.000000, Loss: 210034.7644389, Val MAE: 3063.8125525, Test MAE: 2017.4051895, Time: 11.448230266571045\n",
      "ToTal Epoch: 2939, LR: 0.000000, Loss: 212107.7439832, Val MAE: 3056.2111328, Test MAE: 2017.4051895, Time: 12.75019907951355\n",
      "ToTal Epoch: 2940, LR: 0.000000, Loss: 209691.3449959, Val MAE: 3034.5163905, Test MAE: 2017.4051895, Time: 11.408313989639282\n",
      "ToTal Epoch: 2941, LR: 0.000000, Loss: 210235.4347299, Val MAE: 3067.2530717, Test MAE: 2017.4051895, Time: 11.656363248825073\n",
      "ToTal Epoch: 2942, LR: 0.000000, Loss: 210736.0511728, Val MAE: 3115.8498338, Test MAE: 2017.4051895, Time: 11.392857551574707\n",
      "ToTal Epoch: 2943, LR: 0.000000, Loss: 211126.5812258, Val MAE: 3117.1305272, Test MAE: 2017.4051895, Time: 12.279443979263306\n",
      "ToTal Epoch: 2944, LR: 0.000000, Loss: 210869.0887307, Val MAE: 3084.0621083, Test MAE: 2017.4051895, Time: 10.677106857299805\n",
      "ToTal Epoch: 2945, LR: 0.000000, Loss: 210063.9742798, Val MAE: 3085.8422906, Test MAE: 2017.4051895, Time: 12.585061073303223\n",
      "ToTal Epoch: 2946, LR: 0.000000, Loss: 210607.5040940, Val MAE: 3119.2246044, Test MAE: 2017.4051895, Time: 12.092560768127441\n",
      "ToTal Epoch: 2947, LR: 0.000000, Loss: 209400.5723021, Val MAE: 3112.7565400, Test MAE: 2017.4051895, Time: 11.660337924957275\n",
      "ToTal Epoch: 2948, LR: 0.000000, Loss: 209195.8995653, Val MAE: 3100.3252312, Test MAE: 2017.4051895, Time: 11.463297128677368\n",
      "ToTal Epoch: 2949, LR: 0.000000, Loss: 210862.9640854, Val MAE: 3040.0087040, Test MAE: 2017.4051895, Time: 11.238415718078613\n",
      "ToTal Epoch: 2950, LR: 0.000000, Loss: 209323.2051975, Val MAE: 3105.7714162, Test MAE: 2017.4051895, Time: 11.187085628509521\n",
      "ToTal Epoch: 2951, LR: 0.000000, Loss: 210979.3858501, Val MAE: 3099.6097751, Test MAE: 2017.4051895, Time: 11.361509323120117\n",
      "ToTal Epoch: 2952, LR: 0.000000, Loss: 211123.2384847, Val MAE: 3058.1873710, Test MAE: 2017.4051895, Time: 10.898236751556396\n",
      "ToTal Epoch: 2953, LR: 0.000000, Loss: 208212.6439211, Val MAE: 3091.0904275, Test MAE: 2017.4051895, Time: 11.81731629371643\n",
      "ToTal Epoch: 2954, LR: 0.000000, Loss: 209330.5059380, Val MAE: 3169.6687161, Test MAE: 2017.4051895, Time: 11.515478372573853\n",
      "ToTal Epoch: 2955, LR: 0.000000, Loss: 209142.6947690, Val MAE: 3090.6678275, Test MAE: 2017.4051895, Time: 12.031461477279663\n",
      "ToTal Epoch: 2956, LR: 0.000000, Loss: 210358.7456361, Val MAE: 3012.1641300, Test MAE: 2017.4051895, Time: 11.338674306869507\n",
      "ToTal Epoch: 2957, LR: 0.000000, Loss: 209496.4635360, Val MAE: 3026.0854831, Test MAE: 2017.4051895, Time: 11.535440921783447\n",
      "ToTal Epoch: 2958, LR: 0.000000, Loss: 207634.9624039, Val MAE: 3001.9834040, Test MAE: 2017.4051895, Time: 11.645056962966919\n",
      "ToTal Epoch: 2959, LR: 0.000000, Loss: 211242.8427077, Val MAE: 3051.8961677, Test MAE: 2017.4051895, Time: 12.0626220703125\n",
      "ToTal Epoch: 2960, LR: 0.000000, Loss: 210395.6628099, Val MAE: 3034.6607907, Test MAE: 2017.4051895, Time: 12.409859657287598\n",
      "ToTal Epoch: 2961, LR: 0.000000, Loss: 210377.2957531, Val MAE: 3030.0454741, Test MAE: 2017.4051895, Time: 12.136297941207886\n",
      "ToTal Epoch: 2962, LR: 0.000000, Loss: 208390.4702049, Val MAE: 3017.3627513, Test MAE: 2017.4051895, Time: 12.263206481933594\n",
      "ToTal Epoch: 2963, LR: 0.000000, Loss: 209771.7147088, Val MAE: 3034.6600073, Test MAE: 2017.4051895, Time: 11.729066133499146\n",
      "ToTal Epoch: 2964, LR: 0.000000, Loss: 211226.2259972, Val MAE: 3015.3375707, Test MAE: 2017.4051895, Time: 10.946073532104492\n",
      "ToTal Epoch: 2965, LR: 0.000000, Loss: 208142.0757273, Val MAE: 3079.3008866, Test MAE: 2017.4051895, Time: 11.628127336502075\n",
      "ToTal Epoch: 2966, LR: 0.000000, Loss: 210039.1158840, Val MAE: 3046.7162539, Test MAE: 2017.4051895, Time: 11.482149124145508\n",
      "ToTal Epoch: 2967, LR: 0.000000, Loss: 210238.6935079, Val MAE: 3092.0517418, Test MAE: 2017.4051895, Time: 12.082263946533203\n",
      "ToTal Epoch: 2968, LR: 0.000000, Loss: 209983.3081928, Val MAE: 3169.8682498, Test MAE: 2017.4051895, Time: 11.30594253540039\n",
      "ToTal Epoch: 2969, LR: 0.000000, Loss: 211437.1310371, Val MAE: 3133.8265784, Test MAE: 2017.4051895, Time: 11.134011030197144\n",
      "ToTal Epoch: 2970, LR: 0.000000, Loss: 209516.8209812, Val MAE: 3117.5417144, Test MAE: 2017.4051895, Time: 10.464373111724854\n",
      "ToTal Epoch: 2971, LR: 0.000000, Loss: 210505.5017054, Val MAE: 3133.9123003, Test MAE: 2017.4051895, Time: 10.631085872650146\n",
      "ToTal Epoch: 2972, LR: 0.000000, Loss: 210173.4333349, Val MAE: 3111.5077725, Test MAE: 2017.4051895, Time: 10.726413249969482\n",
      "ToTal Epoch: 2973, LR: 0.000000, Loss: 210583.4018631, Val MAE: 3090.4638462, Test MAE: 2017.4051895, Time: 10.65036678314209\n",
      "ToTal Epoch: 2974, LR: 0.000000, Loss: 208350.9694358, Val MAE: 3112.6121923, Test MAE: 2017.4051895, Time: 11.92415475845337\n",
      "ToTal Epoch: 2975, LR: 0.000000, Loss: 209162.6891129, Val MAE: 3129.5021115, Test MAE: 2017.4051895, Time: 11.45024299621582\n",
      "ToTal Epoch: 2976, LR: 0.000000, Loss: 208929.4949028, Val MAE: 3151.9954043, Test MAE: 2017.4051895, Time: 11.937796592712402\n",
      "ToTal Epoch: 2977, LR: 0.000000, Loss: 209525.0871638, Val MAE: 3147.4173355, Test MAE: 2017.4051895, Time: 11.097583293914795\n",
      "ToTal Epoch: 2978, LR: 0.000000, Loss: 210379.5242345, Val MAE: 3178.6479544, Test MAE: 2017.4051895, Time: 10.977467775344849\n",
      "ToTal Epoch: 2979, LR: 0.000000, Loss: 211893.6221277, Val MAE: 3185.7632997, Test MAE: 2017.4051895, Time: 10.91470193862915\n",
      "ToTal Epoch: 2980, LR: 0.000000, Loss: 210663.2662304, Val MAE: 3180.7503010, Test MAE: 2017.4051895, Time: 11.488013982772827\n",
      "ToTal Epoch: 2981, LR: 0.000000, Loss: 209978.6414943, Val MAE: 3127.6033259, Test MAE: 2017.4051895, Time: 11.40894603729248\n",
      "ToTal Epoch: 2982, LR: 0.000000, Loss: 209979.7029762, Val MAE: 3133.7430396, Test MAE: 2017.4051895, Time: 11.583735466003418\n",
      "ToTal Epoch: 2983, LR: 0.000000, Loss: 210047.9411838, Val MAE: 3132.8486538, Test MAE: 2017.4051895, Time: 13.151103019714355\n",
      "ToTal Epoch: 2984, LR: 0.000000, Loss: 210268.5194095, Val MAE: 3125.7589381, Test MAE: 2017.4051895, Time: 10.943294286727905\n",
      "ToTal Epoch: 2985, LR: 0.000000, Loss: 210378.1791430, Val MAE: 3097.5128220, Test MAE: 2017.4051895, Time: 11.757637739181519\n",
      "ToTal Epoch: 2986, LR: 0.000000, Loss: 208007.2408542, Val MAE: 3166.2443486, Test MAE: 2017.4051895, Time: 12.459425210952759\n",
      "ToTal Epoch: 2987, LR: 0.000000, Loss: 209556.5599580, Val MAE: 3169.6027431, Test MAE: 2017.4051895, Time: 12.382253170013428\n",
      "ToTal Epoch: 2988, LR: 0.000000, Loss: 209554.8795873, Val MAE: 3136.9410160, Test MAE: 2017.4051895, Time: 12.354817390441895\n",
      "ToTal Epoch: 2989, LR: 0.000000, Loss: 209562.5755697, Val MAE: 3134.8027689, Test MAE: 2017.4051895, Time: 11.45713186264038\n",
      "ToTal Epoch: 2990, LR: 0.000000, Loss: 210045.4181245, Val MAE: 3148.6053466, Test MAE: 2017.4051895, Time: 10.875074863433838\n",
      "ToTal Epoch: 2991, LR: 0.000000, Loss: 208874.5961687, Val MAE: 3131.9734102, Test MAE: 2017.4051895, Time: 11.593057870864868\n",
      "ToTal Epoch: 2992, LR: 0.000000, Loss: 210962.3740505, Val MAE: 3085.6623290, Test MAE: 2017.4051895, Time: 11.262315034866333\n",
      "ToTal Epoch: 2993, LR: 0.000000, Loss: 208622.7771652, Val MAE: 3118.2335091, Test MAE: 2017.4051895, Time: 11.34166431427002\n",
      "ToTal Epoch: 2994, LR: 0.000000, Loss: 209449.9228204, Val MAE: 3143.1069661, Test MAE: 2017.4051895, Time: 11.409359455108643\n",
      "ToTal Epoch: 2995, LR: 0.000000, Loss: 209903.9387761, Val MAE: 3124.6062782, Test MAE: 2017.4051895, Time: 11.034615993499756\n",
      "ToTal Epoch: 2996, LR: 0.000000, Loss: 210649.9086801, Val MAE: 3127.3415883, Test MAE: 2017.4051895, Time: 11.353061437606812\n",
      "ToTal Epoch: 2997, LR: 0.000000, Loss: 210131.4743993, Val MAE: 3115.1437935, Test MAE: 2017.4051895, Time: 11.266403198242188\n",
      "ToTal Epoch: 2998, LR: 0.000000, Loss: 210832.8420007, Val MAE: 3090.1819298, Test MAE: 2017.4051895, Time: 11.269136667251587\n",
      "ToTal Epoch: 2999, LR: 0.000000, Loss: 209484.5304161, Val MAE: 3089.4938995, Test MAE: 2017.4051895, Time: 12.277703046798706\n",
      "ToTal Epoch: 3000, LR: 0.000000, Loss: 210597.9147566, Val MAE: 3080.9646153, Test MAE: 2017.4051895, Time: 10.628654956817627\n",
      "ToTal Epoch: 3001, LR: 0.000000, Loss: 211680.8608417, Val MAE: 3049.3691575, Test MAE: 2017.4051895, Time: 11.09325122833252\n",
      "ToTal Epoch: 3002, LR: 0.000000, Loss: 210710.4393637, Val MAE: 3068.5993226, Test MAE: 2017.4051895, Time: 13.22355604171753\n",
      "ToTal Epoch: 3003, LR: 0.000000, Loss: 210291.1704581, Val MAE: 3056.0562897, Test MAE: 2017.4051895, Time: 11.507629156112671\n",
      "ToTal Epoch: 3004, LR: 0.000000, Loss: 210483.2536569, Val MAE: 3016.1247994, Test MAE: 2017.4051895, Time: 12.458216905593872\n",
      "ToTal Epoch: 3005, LR: 0.000000, Loss: 209336.4872689, Val MAE: 3055.4741458, Test MAE: 2017.4051895, Time: 11.343374490737915\n",
      "ToTal Epoch: 3006, LR: 0.000000, Loss: 209717.7116706, Val MAE: 3142.6789106, Test MAE: 2017.4051895, Time: 11.194647550582886\n",
      "ToTal Epoch: 3007, LR: 0.000000, Loss: 209757.7916973, Val MAE: 3066.2361461, Test MAE: 2017.4051895, Time: 12.065651655197144\n",
      "ToTal Epoch: 3008, LR: 0.000000, Loss: 210153.2763770, Val MAE: 3153.4253134, Test MAE: 2017.4051895, Time: 12.622636556625366\n",
      "ToTal Epoch: 3009, LR: 0.000000, Loss: 210726.3570057, Val MAE: 3030.5108442, Test MAE: 2017.4051895, Time: 11.629691362380981\n",
      "ToTal Epoch: 3010, LR: 0.000000, Loss: 209582.9260975, Val MAE: 3043.1623194, Test MAE: 2017.4051895, Time: 13.71969985961914\n",
      "ToTal Epoch: 3011, LR: 0.000000, Loss: 210152.1008933, Val MAE: 3060.4466483, Test MAE: 2017.4051895, Time: 11.357130527496338\n",
      "ToTal Epoch: 3012, LR: 0.000000, Loss: 211159.0684947, Val MAE: 3129.2436798, Test MAE: 2017.4051895, Time: 12.232889175415039\n",
      "ToTal Epoch: 3013, LR: 0.000000, Loss: 209019.9612096, Val MAE: 3050.2175438, Test MAE: 2017.4051895, Time: 11.841706991195679\n",
      "ToTal Epoch: 3014, LR: 0.000000, Loss: 208803.1302537, Val MAE: 3143.7114146, Test MAE: 2017.4051895, Time: 11.448597431182861\n",
      "ToTal Epoch: 3015, LR: 0.000000, Loss: 210690.3927005, Val MAE: 3124.9264599, Test MAE: 2017.4051895, Time: 12.627325057983398\n",
      "ToTal Epoch: 3016, LR: 0.000000, Loss: 211838.9613338, Val MAE: 3035.3061989, Test MAE: 2017.4051895, Time: 13.15738582611084\n",
      "ToTal Epoch: 3017, LR: 0.000000, Loss: 211492.3744136, Val MAE: 3042.9594034, Test MAE: 2017.4051895, Time: 10.884180068969727\n",
      "ToTal Epoch: 3018, LR: 0.000000, Loss: 209262.7399417, Val MAE: 3077.8452668, Test MAE: 2017.4051895, Time: 11.397024869918823\n",
      "ToTal Epoch: 3019, LR: 0.000000, Loss: 210979.3896718, Val MAE: 3105.9941814, Test MAE: 2017.4051895, Time: 11.17788815498352\n",
      "ToTal Epoch: 3020, LR: 0.000000, Loss: 209964.4972054, Val MAE: 3117.8102499, Test MAE: 2017.4051895, Time: 11.286602020263672\n",
      "ToTal Epoch: 3021, LR: 0.000000, Loss: 209661.1761334, Val MAE: 3167.1285781, Test MAE: 2017.4051895, Time: 12.34375524520874\n",
      "ToTal Epoch: 3022, LR: 0.000000, Loss: 208907.3105623, Val MAE: 3116.4102079, Test MAE: 2017.4051895, Time: 11.662030220031738\n",
      "ToTal Epoch: 3023, LR: 0.000000, Loss: 210434.4568289, Val MAE: 3124.1318744, Test MAE: 2017.4051895, Time: 12.726438045501709\n",
      "ToTal Epoch: 3024, LR: 0.000000, Loss: 209824.2079396, Val MAE: 3084.1231894, Test MAE: 2017.4051895, Time: 11.880702495574951\n",
      "ToTal Epoch: 3025, LR: 0.000000, Loss: 208667.8834759, Val MAE: 3126.9488697, Test MAE: 2017.4051895, Time: 11.749889850616455\n",
      "ToTal Epoch: 3026, LR: 0.000000, Loss: 210518.4988296, Val MAE: 3044.2604286, Test MAE: 2017.4051895, Time: 12.15883493423462\n",
      "ToTal Epoch: 3027, LR: 0.000000, Loss: 210102.3517699, Val MAE: 3020.5466493, Test MAE: 2017.4051895, Time: 11.395544052124023\n",
      "ToTal Epoch: 3028, LR: 0.000000, Loss: 211230.9685568, Val MAE: 3035.6210588, Test MAE: 2017.4051895, Time: 11.323701858520508\n",
      "ToTal Epoch: 3029, LR: 0.000000, Loss: 209957.2388095, Val MAE: 3147.4276829, Test MAE: 2017.4051895, Time: 11.050503015518188\n",
      "ToTal Epoch: 3030, LR: 0.000000, Loss: 209394.7063488, Val MAE: 3165.9400319, Test MAE: 2017.4051895, Time: 11.376244306564331\n",
      "ToTal Epoch: 3031, LR: 0.000000, Loss: 211746.0549754, Val MAE: 3116.1815954, Test MAE: 2017.4051895, Time: 11.554459810256958\n",
      "ToTal Epoch: 3032, LR: 0.000000, Loss: 210851.5359289, Val MAE: 3143.8519262, Test MAE: 2017.4051895, Time: 11.966051578521729\n",
      "ToTal Epoch: 3033, LR: 0.000000, Loss: 211103.2229685, Val MAE: 3166.9913198, Test MAE: 2017.4051895, Time: 12.089613199234009\n",
      "ToTal Epoch: 3034, LR: 0.000000, Loss: 210335.9202026, Val MAE: 3152.0928065, Test MAE: 2017.4051895, Time: 11.389139413833618\n",
      "ToTal Epoch: 3035, LR: 0.000000, Loss: 210645.2941480, Val MAE: 3138.1419877, Test MAE: 2017.4051895, Time: 10.884572744369507\n",
      "ToTal Epoch: 3036, LR: 0.000000, Loss: 211041.6529308, Val MAE: 3072.6382901, Test MAE: 2017.4051895, Time: 12.239362478256226\n",
      "ToTal Epoch: 3037, LR: 0.000000, Loss: 209471.7708116, Val MAE: 3051.3467334, Test MAE: 2017.4051895, Time: 12.34639859199524\n",
      "ToTal Epoch: 3038, LR: 0.000000, Loss: 209903.9631969, Val MAE: 3054.9031329, Test MAE: 2017.4051895, Time: 11.515276670455933\n",
      "ToTal Epoch: 3039, LR: 0.000000, Loss: 210494.3097310, Val MAE: 3037.8573435, Test MAE: 2017.4051895, Time: 11.726560354232788\n",
      "ToTal Epoch: 3040, LR: 0.000000, Loss: 210176.5547222, Val MAE: 3052.0515029, Test MAE: 2017.4051895, Time: 11.79676079750061\n",
      "ToTal Epoch: 3041, LR: 0.000000, Loss: 211752.8767783, Val MAE: 3121.7679718, Test MAE: 2017.4051895, Time: 11.276571035385132\n",
      "ToTal Epoch: 3042, LR: 0.000000, Loss: 211653.3055367, Val MAE: 3019.2195311, Test MAE: 2017.4051895, Time: 10.72791051864624\n",
      "ToTal Epoch: 3043, LR: 0.000000, Loss: 209718.2086849, Val MAE: 3011.9795250, Test MAE: 2017.4051895, Time: 11.007490634918213\n",
      "ToTal Epoch: 3044, LR: 0.000000, Loss: 208948.6504562, Val MAE: 3033.5552482, Test MAE: 2017.4051895, Time: 11.24747371673584\n",
      "ToTal Epoch: 3045, LR: 0.000000, Loss: 212140.7921655, Val MAE: 3028.1891625, Test MAE: 2017.4051895, Time: 11.903997421264648\n",
      "ToTal Epoch: 3046, LR: 0.000000, Loss: 208198.9293842, Val MAE: 3117.4921750, Test MAE: 2017.4051895, Time: 10.967998027801514\n",
      "ToTal Epoch: 3047, LR: 0.000000, Loss: 210960.6798070, Val MAE: 3116.0865054, Test MAE: 2017.4051895, Time: 10.85725998878479\n",
      "ToTal Epoch: 3048, LR: 0.000000, Loss: 211688.0459753, Val MAE: 3080.3644950, Test MAE: 2017.4051895, Time: 10.308649063110352\n",
      "ToTal Epoch: 3049, LR: 0.000000, Loss: 208218.1092820, Val MAE: 3064.0115560, Test MAE: 2017.4051895, Time: 11.716034650802612\n",
      "ToTal Epoch: 3050, LR: 0.000000, Loss: 211831.9317441, Val MAE: 3171.6239729, Test MAE: 2017.4051895, Time: 10.741076231002808\n",
      "ToTal Epoch: 3051, LR: 0.000000, Loss: 208743.3944107, Val MAE: 3127.1381086, Test MAE: 2017.4051895, Time: 10.869272470474243\n",
      "ToTal Epoch: 3052, LR: 0.000000, Loss: 210593.7695696, Val MAE: 3180.6879299, Test MAE: 2017.4051895, Time: 11.781552791595459\n",
      "ToTal Epoch: 3053, LR: 0.000000, Loss: 210059.0158697, Val MAE: 3163.5262459, Test MAE: 2017.4051895, Time: 10.811660051345825\n",
      "ToTal Epoch: 3054, LR: 0.000000, Loss: 210346.9062246, Val MAE: 3081.1057431, Test MAE: 2017.4051895, Time: 14.087760210037231\n",
      "ToTal Epoch: 3055, LR: 0.000000, Loss: 209625.2035733, Val MAE: 3027.8558721, Test MAE: 2017.4051895, Time: 12.004748106002808\n",
      "ToTal Epoch: 3056, LR: 0.000000, Loss: 209528.1668179, Val MAE: 3112.3903873, Test MAE: 2017.4051895, Time: 11.854732513427734\n",
      "ToTal Epoch: 3057, LR: 0.000000, Loss: 210366.8396885, Val MAE: 3112.4061568, Test MAE: 2017.4051895, Time: 10.998429775238037\n",
      "ToTal Epoch: 3058, LR: 0.000000, Loss: 207970.3944203, Val MAE: 3128.3316327, Test MAE: 2017.4051895, Time: 10.899011373519897\n",
      "ToTal Epoch: 3059, LR: 0.000000, Loss: 209724.2249845, Val MAE: 3080.5951043, Test MAE: 2017.4051895, Time: 10.97797966003418\n",
      "ToTal Epoch: 3060, LR: 0.000000, Loss: 210013.8418000, Val MAE: 3094.8454244, Test MAE: 2017.4051895, Time: 12.018686294555664\n",
      "ToTal Epoch: 3061, LR: 0.000000, Loss: 211878.5268046, Val MAE: 3088.7808463, Test MAE: 2017.4051895, Time: 11.30773639678955\n",
      "ToTal Epoch: 3062, LR: 0.000000, Loss: 209565.5730187, Val MAE: 3040.8190018, Test MAE: 2017.4051895, Time: 10.672375202178955\n",
      "ToTal Epoch: 3063, LR: 0.000000, Loss: 209563.9583433, Val MAE: 3028.2729066, Test MAE: 2017.4051895, Time: 10.648419618606567\n",
      "ToTal Epoch: 3064, LR: 0.000000, Loss: 208042.5719390, Val MAE: 3082.6318696, Test MAE: 2017.4051895, Time: 11.679164409637451\n",
      "ToTal Epoch: 3065, LR: 0.000000, Loss: 210433.5808150, Val MAE: 3060.0299578, Test MAE: 2017.4051895, Time: 11.307255029678345\n",
      "ToTal Epoch: 3066, LR: 0.000000, Loss: 209564.6174748, Val MAE: 3048.4638510, Test MAE: 2017.4051895, Time: 10.683526277542114\n",
      "ToTal Epoch: 3067, LR: 0.000000, Loss: 209953.3838437, Val MAE: 3065.5551861, Test MAE: 2017.4051895, Time: 10.821556091308594\n",
      "ToTal Epoch: 3068, LR: 0.000000, Loss: 210217.5644962, Val MAE: 3081.2682154, Test MAE: 2017.4051895, Time: 11.444521188735962\n",
      "ToTal Epoch: 3069, LR: 0.000000, Loss: 212160.2860937, Val MAE: 3086.5484694, Test MAE: 2017.4051895, Time: 10.836859703063965\n",
      "ToTal Epoch: 3070, LR: 0.000000, Loss: 209262.9253714, Val MAE: 3067.8064186, Test MAE: 2017.4051895, Time: 12.314623355865479\n",
      "ToTal Epoch: 3071, LR: 0.000000, Loss: 209515.8722018, Val MAE: 3036.2537883, Test MAE: 2017.4051895, Time: 11.456843376159668\n",
      "ToTal Epoch: 3072, LR: 0.000000, Loss: 209473.0786796, Val MAE: 3119.3713837, Test MAE: 2017.4051895, Time: 12.331558227539062\n",
      "ToTal Epoch: 3073, LR: 0.000000, Loss: 209051.4327043, Val MAE: 3070.7801059, Test MAE: 2017.4051895, Time: 11.144552230834961\n",
      "ToTal Epoch: 3074, LR: 0.000000, Loss: 210228.7009411, Val MAE: 3113.4854057, Test MAE: 2017.4051895, Time: 11.44740629196167\n",
      "ToTal Epoch: 3075, LR: 0.000000, Loss: 210373.6878231, Val MAE: 3084.8890306, Test MAE: 2017.4051895, Time: 11.527296304702759\n",
      "ToTal Epoch: 3076, LR: 0.000000, Loss: 209039.0861128, Val MAE: 3063.7334423, Test MAE: 2017.4051895, Time: 11.26617169380188\n",
      "ToTal Epoch: 3077, LR: 0.000000, Loss: 208503.9754264, Val MAE: 3036.2481178, Test MAE: 2017.4051895, Time: 12.281930685043335\n",
      "ToTal Epoch: 3078, LR: 0.000000, Loss: 209443.3601299, Val MAE: 3010.3171482, Test MAE: 2017.4051895, Time: 10.965391635894775\n",
      "ToTal Epoch: 3079, LR: 0.000000, Loss: 210556.1248937, Val MAE: 3004.9460034, Test MAE: 2017.4051895, Time: 12.77445125579834\n",
      "ToTal Epoch: 3080, LR: 0.000000, Loss: 210527.8032198, Val MAE: 3025.4785265, Test MAE: 2017.4051895, Time: 11.306911706924438\n",
      "ToTal Epoch: 3081, LR: 0.000000, Loss: 208868.6047867, Val MAE: 3067.1238726, Test MAE: 2017.4051895, Time: 11.63684868812561\n",
      "ToTal Epoch: 3082, LR: 0.000000, Loss: 210674.2744566, Val MAE: 3060.2773446, Test MAE: 2017.4051895, Time: 11.852755069732666\n",
      "ToTal Epoch: 3083, LR: 0.000000, Loss: 211081.7920413, Val MAE: 3060.8831642, Test MAE: 2017.4051895, Time: 11.943902015686035\n",
      "ToTal Epoch: 3084, LR: 0.000000, Loss: 209491.4783356, Val MAE: 3067.5306505, Test MAE: 2017.4051895, Time: 11.334628820419312\n",
      "ToTal Epoch: 3085, LR: 0.000000, Loss: 209205.4532461, Val MAE: 3048.3581126, Test MAE: 2017.4051895, Time: 11.991820096969604\n",
      "ToTal Epoch: 3086, LR: 0.000000, Loss: 211625.1819806, Val MAE: 3022.1378650, Test MAE: 2017.4051895, Time: 11.618968963623047\n",
      "ToTal Epoch: 3087, LR: 0.000000, Loss: 210037.0844122, Val MAE: 3003.9877752, Test MAE: 2017.4051895, Time: 12.916929960250854\n",
      "ToTal Epoch: 3088, LR: 0.000000, Loss: 209843.4919410, Val MAE: 2988.9431132, Test MAE: 2017.4051895, Time: 11.376065015792847\n",
      "ToTal Epoch: 3089, LR: 0.000000, Loss: 209481.4997946, Val MAE: 3004.9366162, Test MAE: 2017.4051895, Time: 10.822265148162842\n",
      "ToTal Epoch: 3090, LR: 0.000000, Loss: 211252.2922467, Val MAE: 2982.9110152, Test MAE: 2017.4051895, Time: 11.544524192810059\n",
      "ToTal Epoch: 3091, LR: 0.000000, Loss: 210420.9136961, Val MAE: 3049.0564330, Test MAE: 2017.4051895, Time: 10.911524057388306\n",
      "ToTal Epoch: 3092, LR: 0.000000, Loss: 209344.4200067, Val MAE: 3036.2171138, Test MAE: 2017.4051895, Time: 11.694161176681519\n",
      "ToTal Epoch: 3093, LR: 0.000000, Loss: 211659.7862514, Val MAE: 3077.9530115, Test MAE: 2017.4051895, Time: 11.007203578948975\n",
      "ToTal Epoch: 3094, LR: 0.000000, Loss: 212000.0196818, Val MAE: 3168.1578671, Test MAE: 2017.4051895, Time: 11.227982759475708\n",
      "ToTal Epoch: 3095, LR: 0.000000, Loss: 209454.2775522, Val MAE: 3142.2769290, Test MAE: 2017.4051895, Time: 10.991693496704102\n",
      "ToTal Epoch: 3096, LR: 0.000000, Loss: 210202.0696508, Val MAE: 3077.0310661, Test MAE: 2017.4051895, Time: 11.400501728057861\n",
      "ToTal Epoch: 3097, LR: 0.000000, Loss: 210760.2563990, Val MAE: 3092.2846299, Test MAE: 2017.4051895, Time: 11.45715069770813\n",
      "ToTal Epoch: 3098, LR: 0.000000, Loss: 209361.0123728, Val MAE: 3084.4091235, Test MAE: 2017.4051895, Time: 11.751827955245972\n",
      "ToTal Epoch: 3099, LR: 0.000000, Loss: 211112.2743611, Val MAE: 3077.0775816, Test MAE: 2017.4051895, Time: 11.407829284667969\n",
      "ToTal Epoch: 3100, LR: 0.000000, Loss: 210305.8542206, Val MAE: 3127.3752436, Test MAE: 2017.4051895, Time: 11.405207633972168\n",
      "ToTal Epoch: 3101, LR: 0.000000, Loss: 211405.9662733, Val MAE: 3191.6600407, Test MAE: 2017.4051895, Time: 10.958311557769775\n",
      "ToTal Epoch: 3102, LR: 0.000000, Loss: 211449.7157216, Val MAE: 3155.9124675, Test MAE: 2017.4051895, Time: 12.557490348815918\n",
      "ToTal Epoch: 3103, LR: 0.000000, Loss: 212522.2778197, Val MAE: 3139.4872736, Test MAE: 2017.4051895, Time: 10.91423487663269\n",
      "ToTal Epoch: 3104, LR: 0.000000, Loss: 211056.0510581, Val MAE: 3060.2668253, Test MAE: 2017.4051895, Time: 10.842379570007324\n",
      "ToTal Epoch: 3105, LR: 0.000000, Loss: 210873.5378971, Val MAE: 3029.2889819, Test MAE: 2017.4051895, Time: 11.435050249099731\n",
      "ToTal Epoch: 3106, LR: 0.000000, Loss: 210454.8051784, Val MAE: 3071.7313307, Test MAE: 2017.4051895, Time: 11.322301864624023\n",
      "ToTal Epoch: 3107, LR: 0.000000, Loss: 210238.2963550, Val MAE: 3061.5248414, Test MAE: 2017.4051895, Time: 10.84661054611206\n",
      "ToTal Epoch: 3108, LR: 0.000000, Loss: 210526.5636077, Val MAE: 3088.5845611, Test MAE: 2017.4051895, Time: 10.85623049736023\n",
      "ToTal Epoch: 3109, LR: 0.000000, Loss: 210043.2127263, Val MAE: 3049.2545001, Test MAE: 2017.4051895, Time: 10.944128513336182\n",
      "ToTal Epoch: 3110, LR: 0.000000, Loss: 209745.0256724, Val MAE: 3033.2289612, Test MAE: 2017.4051895, Time: 11.205925226211548\n",
      "ToTal Epoch: 3111, LR: 0.000000, Loss: 207922.7044380, Val MAE: 3078.8192788, Test MAE: 2017.4051895, Time: 10.713072061538696\n",
      "ToTal Epoch: 3112, LR: 0.000000, Loss: 208987.9282282, Val MAE: 3080.1284300, Test MAE: 2017.4051895, Time: 11.391512155532837\n",
      "ToTal Epoch: 3113, LR: 0.000000, Loss: 208179.3977356, Val MAE: 3101.0809113, Test MAE: 2017.4051895, Time: 11.328185796737671\n",
      "ToTal Epoch: 3114, LR: 0.000000, Loss: 210774.2496154, Val MAE: 3062.5889274, Test MAE: 2017.4051895, Time: 11.629392385482788\n",
      "ToTal Epoch: 3115, LR: 0.000000, Loss: 208649.9937133, Val MAE: 3064.9538953, Test MAE: 2017.4051895, Time: 11.040533542633057\n",
      "ToTal Epoch: 3116, LR: 0.000000, Loss: 208841.8018631, Val MAE: 3043.1890765, Test MAE: 2017.4051895, Time: 11.059800863265991\n",
      "ToTal Epoch: 3117, LR: 0.000000, Loss: 209710.5469068, Val MAE: 3050.2234102, Test MAE: 2017.4051895, Time: 12.593456268310547\n",
      "ToTal Epoch: 3118, LR: 0.000000, Loss: 211021.1269861, Val MAE: 3031.4478378, Test MAE: 2017.4051895, Time: 11.098612546920776\n",
      "ToTal Epoch: 3119, LR: 0.000000, Loss: 208719.4441313, Val MAE: 3041.5179384, Test MAE: 2017.4051895, Time: 11.453148365020752\n",
      "ToTal Epoch: 3120, LR: 0.000000, Loss: 209737.9797640, Val MAE: 3056.8322346, Test MAE: 2017.4051895, Time: 11.40216064453125\n",
      "ToTal Epoch: 3121, LR: 0.000000, Loss: 208397.1869106, Val MAE: 2998.9897625, Test MAE: 2017.4051895, Time: 10.589984655380249\n",
      "ToTal Epoch: 3122, LR: 0.000000, Loss: 208984.0257966, Val MAE: 2954.2855853, Test MAE: 2017.4051895, Time: 10.85359263420105\n",
      "ToTal Epoch: 3123, LR: 0.000000, Loss: 210807.8328763, Val MAE: 2987.3494420, Test MAE: 2017.4051895, Time: 11.562068223953247\n",
      "ToTal Epoch: 3124, LR: 0.000000, Loss: 210086.3325085, Val MAE: 3007.8284176, Test MAE: 2017.4051895, Time: 11.499933004379272\n",
      "ToTal Epoch: 3125, LR: 0.000000, Loss: 208573.2746572, Val MAE: 3030.0093155, Test MAE: 2017.4051895, Time: 12.320572853088379\n",
      "ToTal Epoch: 3126, LR: 0.000000, Loss: 209758.5000143, Val MAE: 3054.4944632, Test MAE: 2017.4051895, Time: 11.24908447265625\n",
      "ToTal Epoch: 3127, LR: 0.000000, Loss: 210653.1883247, Val MAE: 3047.6901896, Test MAE: 2017.4051895, Time: 11.264954566955566\n",
      "ToTal Epoch: 3128, LR: 0.000000, Loss: 208843.7549515, Val MAE: 3136.4578078, Test MAE: 2017.4051895, Time: 10.867176532745361\n",
      "ToTal Epoch: 3129, LR: 0.000000, Loss: 208557.6267902, Val MAE: 3055.0101898, Test MAE: 2017.4051895, Time: 11.135408401489258\n",
      "ToTal Epoch: 3130, LR: 0.000000, Loss: 209943.4238380, Val MAE: 3161.3912568, Test MAE: 2017.4051895, Time: 12.555291652679443\n",
      "ToTal Epoch: 3131, LR: 0.000000, Loss: 211414.1968757, Val MAE: 3068.4944823, Test MAE: 2017.4051895, Time: 12.01099157333374\n",
      "ToTal Epoch: 3132, LR: 0.000000, Loss: 210049.2788229, Val MAE: 3033.5293320, Test MAE: 2017.4051895, Time: 11.370193243026733\n",
      "ToTal Epoch: 3133, LR: 0.000000, Loss: 208141.9285912, Val MAE: 3141.2101248, Test MAE: 2017.4051895, Time: 12.446053504943848\n",
      "ToTal Epoch: 3134, LR: 0.000000, Loss: 208810.7529738, Val MAE: 3148.6012239, Test MAE: 2017.4051895, Time: 10.727110624313354\n",
      "ToTal Epoch: 3135, LR: 0.000000, Loss: 211806.2814121, Val MAE: 3166.5668472, Test MAE: 2017.4051895, Time: 11.898869514465332\n",
      "ToTal Epoch: 3136, LR: 0.000000, Loss: 211716.8111976, Val MAE: 3147.8501920, Test MAE: 2017.4051895, Time: 12.509086608886719\n",
      "ToTal Epoch: 3137, LR: 0.000000, Loss: 209843.9033488, Val MAE: 3052.0346060, Test MAE: 2017.4051895, Time: 10.970021724700928\n",
      "ToTal Epoch: 3138, LR: 0.000000, Loss: 210488.2998519, Val MAE: 3108.6375449, Test MAE: 2017.4051895, Time: 12.290233373641968\n",
      "ToTal Epoch: 3139, LR: 0.000000, Loss: 211094.3661014, Val MAE: 3077.0223047, Test MAE: 2017.4051895, Time: 12.227229595184326\n",
      "ToTal Epoch: 3140, LR: 0.000000, Loss: 210885.9468208, Val MAE: 3121.8020188, Test MAE: 2017.4051895, Time: 11.951785802841187\n",
      "ToTal Epoch: 3141, LR: 0.000000, Loss: 211025.8966799, Val MAE: 3113.9459747, Test MAE: 2017.4051895, Time: 11.1255202293396\n",
      "ToTal Epoch: 3142, LR: 0.000000, Loss: 209553.7767544, Val MAE: 3041.7100961, Test MAE: 2017.4051895, Time: 10.911540985107422\n",
      "ToTal Epoch: 3143, LR: 0.000000, Loss: 209018.2386853, Val MAE: 3096.8674186, Test MAE: 2017.4051895, Time: 11.241449356079102\n",
      "ToTal Epoch: 3144, LR: 0.000000, Loss: 210146.6881957, Val MAE: 3073.1215222, Test MAE: 2017.4051895, Time: 10.85846209526062\n",
      "ToTal Epoch: 3145, LR: 0.000000, Loss: 211708.2510868, Val MAE: 3062.9617299, Test MAE: 2017.4051895, Time: 11.339377403259277\n",
      "ToTal Epoch: 3146, LR: 0.000000, Loss: 209110.0537524, Val MAE: 3065.0965614, Test MAE: 2017.4051895, Time: 11.751328706741333\n",
      "ToTal Epoch: 3147, LR: 0.000000, Loss: 209674.9780347, Val MAE: 3067.7807412, Test MAE: 2017.4051895, Time: 12.246639966964722\n",
      "ToTal Epoch: 3148, LR: 0.000000, Loss: 211554.1540725, Val MAE: 3069.9341846, Test MAE: 2017.4051895, Time: 14.051284313201904\n",
      "ToTal Epoch: 3149, LR: 0.000000, Loss: 211641.6487269, Val MAE: 3112.9100168, Test MAE: 2017.4051895, Time: 11.796755313873291\n",
      "ToTal Epoch: 3150, LR: 0.000000, Loss: 209423.1697320, Val MAE: 3150.9822623, Test MAE: 2017.4051895, Time: 10.850325584411621\n",
      "ToTal Epoch: 3151, LR: 0.000000, Loss: 209598.2754502, Val MAE: 3128.8862216, Test MAE: 2017.4051895, Time: 11.697216749191284\n",
      "ToTal Epoch: 3152, LR: 0.000000, Loss: 208169.2213061, Val MAE: 3159.0172122, Test MAE: 2017.4051895, Time: 16.234809637069702\n",
      "ToTal Epoch: 3153, LR: 0.000000, Loss: 209150.1790283, Val MAE: 3124.5543406, Test MAE: 2017.4051895, Time: 11.65728759765625\n",
      "ToTal Epoch: 3154, LR: 0.000000, Loss: 210500.5619070, Val MAE: 3171.4251271, Test MAE: 2017.4051895, Time: 11.152961492538452\n",
      "ToTal Epoch: 3155, LR: 0.000000, Loss: 209798.8922371, Val MAE: 3197.5775243, Test MAE: 2017.4051895, Time: 11.352444410324097\n",
      "ToTal Epoch: 3156, LR: 0.000000, Loss: 210688.6226723, Val MAE: 3206.4546597, Test MAE: 2017.4051895, Time: 10.983008861541748\n",
      "ToTal Epoch: 3157, LR: 0.000000, Loss: 210223.1546744, Val MAE: 3143.8605203, Test MAE: 2017.4051895, Time: 11.16677188873291\n",
      "ToTal Epoch: 3158, LR: 0.000000, Loss: 208709.6106244, Val MAE: 3077.0552148, Test MAE: 2017.4051895, Time: 13.14178991317749\n",
      "ToTal Epoch: 3159, LR: 0.000000, Loss: 209380.2874313, Val MAE: 3147.2609398, Test MAE: 2017.4051895, Time: 12.456872940063477\n",
      "ToTal Epoch: 3160, LR: 0.000000, Loss: 210804.9885253, Val MAE: 3162.1374494, Test MAE: 2017.4051895, Time: 11.967199563980103\n",
      "ToTal Epoch: 3161, LR: 0.000000, Loss: 209710.0782879, Val MAE: 3079.7952734, Test MAE: 2017.4051895, Time: 11.971245050430298\n",
      "ToTal Epoch: 3162, LR: 0.000000, Loss: 211394.3956050, Val MAE: 3067.9295555, Test MAE: 2017.4051895, Time: 10.618703365325928\n",
      "ToTal Epoch: 3163, LR: 0.000000, Loss: 211402.3475660, Val MAE: 3061.4393918, Test MAE: 2017.4051895, Time: 12.869682312011719\n",
      "ToTal Epoch: 3164, LR: 0.000000, Loss: 209752.4007070, Val MAE: 3068.3740971, Test MAE: 2017.4051895, Time: 10.733437299728394\n",
      "ToTal Epoch: 3165, LR: 0.000000, Loss: 209495.5811780, Val MAE: 3073.3894558, Test MAE: 2017.4051895, Time: 11.401585817337036\n",
      "ToTal Epoch: 3166, LR: 0.000000, Loss: 210411.9521139, Val MAE: 3081.2262526, Test MAE: 2017.4051895, Time: 11.971001386642456\n",
      "ToTal Epoch: 3167, LR: 0.000000, Loss: 211374.5133139, Val MAE: 3086.6143946, Test MAE: 2017.4051895, Time: 11.317343711853027\n",
      "ToTal Epoch: 3168, LR: 0.000000, Loss: 210788.3895094, Val MAE: 3045.5186167, Test MAE: 2017.4051895, Time: 10.54780912399292\n",
      "ToTal Epoch: 3169, LR: 0.000000, Loss: 211319.2599245, Val MAE: 3075.7924024, Test MAE: 2017.4051895, Time: 10.858392238616943\n",
      "ToTal Epoch: 3170, LR: 0.000000, Loss: 208688.2352267, Val MAE: 3064.9434285, Test MAE: 2017.4051895, Time: 10.737409353256226\n",
      "ToTal Epoch: 3171, LR: 0.000000, Loss: 209594.9871686, Val MAE: 3045.8095238, Test MAE: 2017.4051895, Time: 11.168001413345337\n",
      "ToTal Epoch: 3172, LR: 0.000000, Loss: 210000.3748340, Val MAE: 3037.5745720, Test MAE: 2017.4051895, Time: 11.30702018737793\n",
      "ToTal Epoch: 3173, LR: 0.000000, Loss: 210961.5601777, Val MAE: 3010.2413294, Test MAE: 2017.4051895, Time: 10.611414432525635\n",
      "ToTal Epoch: 3174, LR: 0.000000, Loss: 210599.3936082, Val MAE: 3073.8117213, Test MAE: 2017.4051895, Time: 10.735066413879395\n",
      "ToTal Epoch: 3175, LR: 0.000000, Loss: 210140.0296947, Val MAE: 3117.8459881, Test MAE: 2017.4051895, Time: 11.049959182739258\n",
      "ToTal Epoch: 3176, LR: 0.000000, Loss: 210412.8131849, Val MAE: 3071.3981598, Test MAE: 2017.4051895, Time: 10.752081394195557\n",
      "ToTal Epoch: 3177, LR: 0.000000, Loss: 212347.8496919, Val MAE: 3126.6099136, Test MAE: 2017.4051895, Time: 10.91852617263794\n",
      "ToTal Epoch: 3178, LR: 0.000000, Loss: 210200.7828405, Val MAE: 3136.3582989, Test MAE: 2017.4051895, Time: 11.356360673904419\n",
      "ToTal Epoch: 3179, LR: 0.000000, Loss: 209936.9215688, Val MAE: 3082.7731312, Test MAE: 2017.4051895, Time: 11.201300621032715\n",
      "ToTal Epoch: 3180, LR: 0.000000, Loss: 211786.3839488, Val MAE: 3085.0810021, Test MAE: 2017.4051895, Time: 11.60149359703064\n",
      "ToTal Epoch: 3181, LR: 0.000000, Loss: 209335.3791621, Val MAE: 3078.5910007, Test MAE: 2017.4051895, Time: 11.027985095977783\n",
      "ToTal Epoch: 3182, LR: 0.000000, Loss: 211990.1315626, Val MAE: 3016.2734130, Test MAE: 2017.4051895, Time: 12.488966226577759\n",
      "ToTal Epoch: 3183, LR: 0.000000, Loss: 209402.7305403, Val MAE: 3039.0610334, Test MAE: 2017.4051895, Time: 11.349812984466553\n",
      "ToTal Epoch: 3184, LR: 0.000000, Loss: 210438.7463622, Val MAE: 3015.1923966, Test MAE: 2017.4051895, Time: 13.170421361923218\n",
      "ToTal Epoch: 3185, LR: 0.000000, Loss: 210041.6724215, Val MAE: 3115.2275806, Test MAE: 2017.4051895, Time: 11.336409091949463\n",
      "ToTal Epoch: 3186, LR: 0.000000, Loss: 209757.4857307, Val MAE: 3089.2667393, Test MAE: 2017.4051895, Time: 11.65589952468872\n",
      "ToTal Epoch: 3187, LR: 0.000000, Loss: 212642.7998662, Val MAE: 3074.4732525, Test MAE: 2017.4051895, Time: 12.34834361076355\n",
      "ToTal Epoch: 3188, LR: 0.000000, Loss: 210654.4260641, Val MAE: 3090.5896010, Test MAE: 2017.4051895, Time: 11.030025959014893\n",
      "ToTal Epoch: 3189, LR: 0.000000, Loss: 210410.9374480, Val MAE: 3106.2506258, Test MAE: 2017.4051895, Time: 11.317285776138306\n",
      "ToTal Epoch: 3190, LR: 0.000000, Loss: 211208.1651364, Val MAE: 3084.1475913, Test MAE: 2017.4051895, Time: 10.850005626678467\n",
      "ToTal Epoch: 3191, LR: 0.000000, Loss: 208445.5847131, Val MAE: 2990.7111758, Test MAE: 2017.4051895, Time: 10.785888195037842\n",
      "ToTal Epoch: 3192, LR: 0.000000, Loss: 211068.5228873, Val MAE: 2984.6507586, Test MAE: 2017.4051895, Time: 11.601450204849243\n",
      "ToTal Epoch: 3193, LR: 0.000000, Loss: 210079.9857068, Val MAE: 3009.1651666, Test MAE: 2017.4051895, Time: 10.804748296737671\n",
      "ToTal Epoch: 3194, LR: 0.000000, Loss: 211044.1511489, Val MAE: 3032.7867318, Test MAE: 2017.4051895, Time: 11.560352087020874\n",
      "ToTal Epoch: 3195, LR: 0.000000, Loss: 209435.3393398, Val MAE: 3082.0299196, Test MAE: 2017.4051895, Time: 11.594670057296753\n",
      "ToTal Epoch: 3196, LR: 0.000000, Loss: 211288.1287919, Val MAE: 3011.7717888, Test MAE: 2017.4051895, Time: 10.823036432266235\n",
      "ToTal Epoch: 3197, LR: 0.000000, Loss: 210003.4791000, Val MAE: 3006.7925696, Test MAE: 2017.4051895, Time: 11.746647834777832\n",
      "ToTal Epoch: 3198, LR: 0.000000, Loss: 209517.6809440, Val MAE: 3068.1231847, Test MAE: 2017.4051895, Time: 12.781492710113525\n",
      "ToTal Epoch: 3199, LR: 0.000000, Loss: 209767.5297377, Val MAE: 3062.7456480, Test MAE: 2017.4051895, Time: 12.08769178390503\n",
      "ToTal Epoch: 3200, LR: 0.000000, Loss: 212295.4953423, Val MAE: 3003.9181333, Test MAE: 2017.4051895, Time: 12.576990604400635\n",
      "ToTal Epoch: 3201, LR: 0.000000, Loss: 210638.0688100, Val MAE: 3091.9808817, Test MAE: 2017.4051895, Time: 13.445805311203003\n",
      "ToTal Epoch: 3202, LR: 0.000000, Loss: 210698.5561936, Val MAE: 3105.2842429, Test MAE: 2017.4051895, Time: 11.141075134277344\n",
      "ToTal Epoch: 3203, LR: 0.000000, Loss: 210619.5372665, Val MAE: 3075.5637707, Test MAE: 2017.4051895, Time: 12.72210168838501\n",
      "ToTal Epoch: 3204, LR: 0.000000, Loss: 209904.7617828, Val MAE: 3103.2485286, Test MAE: 2017.4051895, Time: 11.010907649993896\n",
      "ToTal Epoch: 3205, LR: 0.000000, Loss: 210880.7141260, Val MAE: 3144.3876548, Test MAE: 2017.4051895, Time: 11.209385633468628\n",
      "ToTal Epoch: 3206, LR: 0.000000, Loss: 210709.9757512, Val MAE: 3140.5362302, Test MAE: 2017.4051895, Time: 10.920108079910278\n",
      "ToTal Epoch: 3207, LR: 0.000000, Loss: 209407.9816940, Val MAE: 3126.4331098, Test MAE: 2017.4051895, Time: 11.098745584487915\n",
      "ToTal Epoch: 3208, LR: 0.000000, Loss: 208829.4921129, Val MAE: 3140.9716426, Test MAE: 2017.4051895, Time: 10.812050342559814\n",
      "ToTal Epoch: 3209, LR: 0.000000, Loss: 208755.4048822, Val MAE: 3142.5597340, Test MAE: 2017.4051895, Time: 11.776218175888062\n",
      "ToTal Epoch: 3210, LR: 0.000000, Loss: 209113.6697846, Val MAE: 3059.2350665, Test MAE: 2017.4051895, Time: 10.638364553451538\n",
      "ToTal Epoch: 3211, LR: 0.000000, Loss: 210636.7207758, Val MAE: 3084.1266720, Test MAE: 2017.4051895, Time: 10.90094542503357\n",
      "ToTal Epoch: 3212, LR: 0.000000, Loss: 209913.1258778, Val MAE: 3145.9546453, Test MAE: 2017.4051895, Time: 10.498834609985352\n",
      "ToTal Epoch: 3213, LR: 0.000000, Loss: 211365.6165098, Val MAE: 3113.3119984, Test MAE: 2017.4051895, Time: 10.92914080619812\n",
      "ToTal Epoch: 3214, LR: 0.000000, Loss: 210554.5481680, Val MAE: 3058.6197021, Test MAE: 2017.4051895, Time: 11.080110549926758\n",
      "ToTal Epoch: 3215, LR: 0.000000, Loss: 211247.0024172, Val MAE: 3041.8511140, Test MAE: 2017.4051895, Time: 10.933868169784546\n",
      "ToTal Epoch: 3216, LR: 0.000000, Loss: 210100.7520757, Val MAE: 3077.5860468, Test MAE: 2017.4051895, Time: 10.979421854019165\n",
      "ToTal Epoch: 3217, LR: 0.000000, Loss: 209361.7997611, Val MAE: 3082.5478054, Test MAE: 2017.4051895, Time: 12.088746547698975\n",
      "ToTal Epoch: 3218, LR: 0.000000, Loss: 209864.9856208, Val MAE: 3112.7043778, Test MAE: 2017.4051895, Time: 11.78000020980835\n",
      "ToTal Epoch: 3219, LR: 0.000000, Loss: 210104.8489180, Val MAE: 3088.2133398, Test MAE: 2017.4051895, Time: 12.323199033737183\n",
      "ToTal Epoch: 3220, LR: 0.000000, Loss: 209745.2229112, Val MAE: 3075.1693610, Test MAE: 2017.4051895, Time: 11.24478816986084\n",
      "ToTal Epoch: 3221, LR: 0.000000, Loss: 210764.9704104, Val MAE: 3075.7427243, Test MAE: 2017.4051895, Time: 12.12917709350586\n",
      "ToTal Epoch: 3222, LR: 0.000000, Loss: 208923.0778197, Val MAE: 3036.0151246, Test MAE: 2017.4051895, Time: 10.665310859680176\n",
      "ToTal Epoch: 3223, LR: 0.000000, Loss: 209931.5671523, Val MAE: 3029.3495376, Test MAE: 2017.4051895, Time: 12.028446197509766\n",
      "ToTal Epoch: 3224, LR: 0.000000, Loss: 210193.3467730, Val MAE: 3055.3749427, Test MAE: 2017.4051895, Time: 10.84449028968811\n",
      "ToTal Epoch: 3225, LR: 0.000000, Loss: 210083.0907371, Val MAE: 3065.2152412, Test MAE: 2017.4051895, Time: 10.341867446899414\n",
      "ToTal Epoch: 3226, LR: 0.000000, Loss: 210957.7916209, Val MAE: 3071.8732372, Test MAE: 2017.4051895, Time: 11.146234512329102\n",
      "ToTal Epoch: 3227, LR: 0.000000, Loss: 209160.8349305, Val MAE: 3028.8846499, Test MAE: 2017.4051895, Time: 11.549797773361206\n",
      "ToTal Epoch: 3228, LR: 0.000000, Loss: 210564.5015621, Val MAE: 3038.9145743, Test MAE: 2017.4051895, Time: 13.391060590744019\n",
      "ToTal Epoch: 3229, LR: 0.000000, Loss: 211245.9806430, Val MAE: 3054.3563833, Test MAE: 2017.4051895, Time: 12.092078447341919\n",
      "ToTal Epoch: 3230, LR: 0.000000, Loss: 208244.2664501, Val MAE: 3042.2915377, Test MAE: 2017.4051895, Time: 11.112498044967651\n",
      "ToTal Epoch: 3231, LR: 0.000000, Loss: 209468.6001242, Val MAE: 3020.9383742, Test MAE: 2017.4051895, Time: 11.436455488204956\n",
      "ToTal Epoch: 3232, LR: 0.000000, Loss: 210802.1421870, Val MAE: 3035.9237369, Test MAE: 2017.4051895, Time: 10.950289726257324\n",
      "ToTal Epoch: 3233, LR: 0.000000, Loss: 208673.7356327, Val MAE: 3074.1088053, Test MAE: 2017.4051895, Time: 13.263892889022827\n",
      "ToTal Epoch: 3234, LR: 0.000000, Loss: 210341.5069412, Val MAE: 3054.3958190, Test MAE: 2017.4051895, Time: 13.56132960319519\n",
      "ToTal Epoch: 3235, LR: 0.000000, Loss: 208666.2600105, Val MAE: 3061.6234283, Test MAE: 2017.4051895, Time: 12.122678518295288\n",
      "ToTal Epoch: 3236, LR: 0.000000, Loss: 211716.6097931, Val MAE: 3057.9634831, Test MAE: 2017.4051895, Time: 11.591334581375122\n",
      "ToTal Epoch: 3237, LR: 0.000000, Loss: 210249.7994936, Val MAE: 3037.5379357, Test MAE: 2017.4051895, Time: 10.594139337539673\n",
      "ToTal Epoch: 3238, LR: 0.000000, Loss: 209878.0029618, Val MAE: 3063.4062572, Test MAE: 2017.4051895, Time: 12.531399488449097\n",
      "ToTal Epoch: 3239, LR: 0.000000, Loss: 209929.1424258, Val MAE: 3056.9594799, Test MAE: 2017.4051895, Time: 10.152982711791992\n",
      "ToTal Epoch: 3240, LR: 0.000000, Loss: 210473.3379449, Val MAE: 3020.3247057, Test MAE: 2017.4051895, Time: 11.565701723098755\n",
      "ToTal Epoch: 3241, LR: 0.000000, Loss: 209917.2030383, Val MAE: 3068.8436712, Test MAE: 2017.4051895, Time: 10.54649567604065\n",
      "ToTal Epoch: 3242, LR: 0.000000, Loss: 209596.5658052, Val MAE: 3075.4495337, Test MAE: 2017.4051895, Time: 11.470010757446289\n",
      "ToTal Epoch: 3243, LR: 0.000000, Loss: 210001.3917546, Val MAE: 3109.3985133, Test MAE: 2017.4051895, Time: 12.419543743133545\n",
      "ToTal Epoch: 3244, LR: 0.000000, Loss: 210032.6364305, Val MAE: 3128.0517657, Test MAE: 2017.4051895, Time: 11.47344708442688\n",
      "ToTal Epoch: 3245, LR: 0.000000, Loss: 210654.3748913, Val MAE: 3018.3353254, Test MAE: 2017.4051895, Time: 11.676589965820312\n",
      "ToTal Epoch: 3246, LR: 0.000000, Loss: 209209.7861940, Val MAE: 3081.1496312, Test MAE: 2017.4051895, Time: 11.723746061325073\n",
      "ToTal Epoch: 3247, LR: 0.000000, Loss: 208563.1678594, Val MAE: 3032.3715031, Test MAE: 2017.4051895, Time: 12.322983980178833\n",
      "ToTal Epoch: 3248, LR: 0.000000, Loss: 211350.3318588, Val MAE: 3062.9016663, Test MAE: 2017.4051895, Time: 11.011565685272217\n",
      "ToTal Epoch: 3249, LR: 0.000000, Loss: 211129.2529499, Val MAE: 3063.4375621, Test MAE: 2017.4051895, Time: 10.798477172851562\n",
      "ToTal Epoch: 3250, LR: 0.000000, Loss: 210599.0759471, Val MAE: 3007.0471366, Test MAE: 2017.4051895, Time: 11.085987329483032\n",
      "ToTal Epoch: 3251, LR: 0.000000, Loss: 209823.2224717, Val MAE: 3044.0721165, Test MAE: 2017.4051895, Time: 11.645285844802856\n",
      "ToTal Epoch: 3252, LR: 0.000000, Loss: 209242.6014809, Val MAE: 3093.4208897, Test MAE: 2017.4051895, Time: 10.783177614212036\n",
      "ToTal Epoch: 3253, LR: 0.000000, Loss: 209426.9233459, Val MAE: 3030.3390612, Test MAE: 2017.4051895, Time: 11.492236375808716\n",
      "ToTal Epoch: 3254, LR: 0.000000, Loss: 208555.5535470, Val MAE: 3065.5468977, Test MAE: 2017.4051895, Time: 11.37970519065857\n",
      "ToTal Epoch: 3255, LR: 0.000000, Loss: 209275.6529117, Val MAE: 3050.9062858, Test MAE: 2017.4051895, Time: 12.42551565170288\n",
      "ToTal Epoch: 3256, LR: 0.000000, Loss: 211163.4230736, Val MAE: 3058.5873318, Test MAE: 2017.4051895, Time: 11.922291040420532\n",
      "ToTal Epoch: 3257, LR: 0.000000, Loss: 210892.5680982, Val MAE: 3108.3143774, Test MAE: 2017.4051895, Time: 13.162817478179932\n",
      "ToTal Epoch: 3258, LR: 0.000000, Loss: 208928.6609277, Val MAE: 3054.9854535, Test MAE: 2017.4051895, Time: 11.421426057815552\n",
      "ToTal Epoch: 3259, LR: 0.000000, Loss: 210489.3679071, Val MAE: 2995.5522768, Test MAE: 2017.4051895, Time: 11.824081897735596\n",
      "ToTal Epoch: 3260, LR: 0.000000, Loss: 208809.9705537, Val MAE: 3007.1000583, Test MAE: 2017.4051895, Time: 11.424913883209229\n",
      "ToTal Epoch: 3261, LR: 0.000000, Loss: 210265.3379449, Val MAE: 3019.9732429, Test MAE: 2017.4051895, Time: 11.702334642410278\n",
      "ToTal Epoch: 3262, LR: 0.000000, Loss: 209610.2949028, Val MAE: 3053.3375229, Test MAE: 2017.4051895, Time: 11.206591367721558\n",
      "ToTal Epoch: 3263, LR: 0.000000, Loss: 209991.8857307, Val MAE: 3040.3267074, Test MAE: 2017.4051895, Time: 12.191735744476318\n",
      "ToTal Epoch: 3264, LR: 0.000000, Loss: 208571.7919457, Val MAE: 3070.9880570, Test MAE: 2017.4051895, Time: 12.22481918334961\n",
      "ToTal Epoch: 3265, LR: 0.000000, Loss: 208794.5513018, Val MAE: 3044.4712270, Test MAE: 2017.4051895, Time: 11.916459560394287\n",
      "ToTal Epoch: 3266, LR: 0.000000, Loss: 209589.2431281, Val MAE: 3083.7674176, Test MAE: 2017.4051895, Time: 10.984391450881958\n",
      "ToTal Epoch: 3267, LR: 0.000000, Loss: 210311.0170162, Val MAE: 3129.0083744, Test MAE: 2017.4051895, Time: 11.106507301330566\n",
      "ToTal Epoch: 3268, LR: 0.000000, Loss: 209757.7581809, Val MAE: 3099.0710560, Test MAE: 2017.4051895, Time: 11.23959732055664\n",
      "ToTal Epoch: 3269, LR: 0.000000, Loss: 210168.8708928, Val MAE: 3114.1747927, Test MAE: 2017.4051895, Time: 10.335376501083374\n",
      "ToTal Epoch: 3270, LR: 0.000000, Loss: 210530.1437157, Val MAE: 3049.9641233, Test MAE: 2017.4051895, Time: 11.130127906799316\n",
      "ToTal Epoch: 3271, LR: 0.000000, Loss: 210647.1336932, Val MAE: 3063.0636704, Test MAE: 2017.4051895, Time: 11.030177593231201\n",
      "ToTal Epoch: 3272, LR: 0.000000, Loss: 211660.6566474, Val MAE: 3071.5157170, Test MAE: 2017.4051895, Time: 10.818628787994385\n",
      "ToTal Epoch: 3273, LR: 0.000000, Loss: 209404.3161706, Val MAE: 3127.8298890, Test MAE: 2017.4051895, Time: 11.180473804473877\n",
      "ToTal Epoch: 3274, LR: 0.000000, Loss: 208952.3715855, Val MAE: 3145.7970553, Test MAE: 2017.4051895, Time: 11.936863660812378\n",
      "ToTal Epoch: 3275, LR: 0.000000, Loss: 210715.3258491, Val MAE: 3079.0249943, Test MAE: 2017.4051895, Time: 11.140578269958496\n",
      "ToTal Epoch: 3276, LR: 0.000000, Loss: 210545.7892896, Val MAE: 3053.7595687, Test MAE: 2017.4051895, Time: 11.566725015640259\n",
      "ToTal Epoch: 3277, LR: 0.000000, Loss: 209277.8153537, Val MAE: 3010.9376385, Test MAE: 2017.4051895, Time: 10.839920282363892\n",
      "ToTal Epoch: 3278, LR: 0.000000, Loss: 211696.6485454, Val MAE: 3092.9200011, Test MAE: 2017.4051895, Time: 11.154142379760742\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# load modules and set parameters\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('run/GAT_Net/run_10')\n",
    "    ## logdir=./python/run/GAT_Net/run_02\n",
    "#input parameters\n",
    "total_num_epoch = 1500 #the total number of epoch that have run\n",
    "running_num_epoch = 2000 #the number of epoch that run in this time\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-6, weight_decay=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=50, min_lr=1e-8)\n",
    "\n",
    "#################################################################################\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "\n",
    "    # to save the metrics\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        best_validation_error = validation_error\n",
    "    test_error = test(test_loader)\n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    epoch_time_end = time.time() # to measure time    \n",
    "    writer.add_scalar('loss in train', loss, total_num_epoch) #tensorboard\n",
    "    writer.add_scalar('validation MAE', validation_error, total_num_epoch) #tensorboard    \n",
    "    writer.add_scalar('test MAE', test_error, total_num_epoch) #tensorboard\n",
    "    writer.add_scalar('learning rate', lr, total_num_epoch) #tensorboard\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "\n",
    "    total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total running Time: {total_time_finish - total_time_start}')\n",
    "writer.close() #tensorboard : if close() is not declared, the writer does not save any valeus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b7d22-8955-4515-b967-4fcb90aa440d",
   "metadata": {},
   "source": [
    "tensorboard command :   \n",
    "tensorboard --logdir=./python/run/GAT_Net/run_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af50e58-0169-4d0b-aba6-fa0d6417c60f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2590d-aebf-44a8-b714-2551ab82b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': total_num_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model/GAT_epoch2000_run05_20211216')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5e17d-5469-4798-8d99-3b76a946607b",
   "metadata": {},
   "source": [
    "To load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00501a99-0536-4920-a1ef-db341882c638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(11, 80)\n",
       "  (conv2): GCNConv(80, 80)\n",
       "  (conv3): GCNConv(80, 80)\n",
       "  (conv4): GCNConv(80, 80)\n",
       "  (conv5): GCNConv(80, 80)\n",
       "  (lin1): Linear(in_features=80, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = 'model/GCNConv01-2_epoch300_20211207'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca79d2-3817-44f3-ae2c-3cb12438ec25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GAT_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d1ab39-51b6-4fc3-89ad-6f16deecca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "batch_size= 256\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84fe23d-b1e2-47d5-b973-0fb9b37c9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class GAT_gate(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAT_gate, self).__init__()\n",
    "        self.W = nn.Linear(in_channels, out_channels)\n",
    "        #self.A = nn.Parameter(torch.Tensor(n_out_feature, n_out_feature))\n",
    "        self.A = nn.Parameter(torch.zeros(size=(out_channels, out_channels)))\n",
    "        self.gate = nn.Linear(out_channels*2, 1)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = self.W(x)\n",
    "        batch_size = h.size()[0]\n",
    "        N = h.size()[1]\n",
    "        #print(h.size(), self.A.size())\n",
    "        #print(torch.matmul(h,self.A).size())\n",
    "        #[original] e = torch.einsum('ijl,ikl->ijk', (torch.matmul(h,self.A), h))\n",
    "        e = torch.einsum('jl,kl->jk', (torch.matmul(h,self.A), h))\n",
    "        #[original] e = e + e.permute((0,2,1))\n",
    "        e = e + e.permute((1,0))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        #attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        #h_prime = torch.matmul(attention, h)\n",
    "        attention = attention*adj\n",
    "        #[original] h_prime = F.relu(torch.einsum('aij,ajk->aik',(attention, h)))\n",
    "        #h_prime = F.relu(torch.einsum('ij,jk->ik',(attention, h)))\n",
    "        #print(attention.size(), h.size())\n",
    "        h_prime = F.relu(torch.matmul(attention.permute(0, 1, 2)[-1, :, :], h))\n",
    "        #[original] coeff = torch.sigmoid(self.gate(torch.cat([x,h_prime], -1))).repeat(1,1,x.size(-1))\n",
    "        #print(x.size(), h_prime.size())\n",
    "        coeff = torch.sigmoid(self.gate(torch.cat([x,h_prime], dim=1)))\n",
    "        retval = coeff*x+(1-coeff)*h_prime\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e289ecc-d0f5-4d67-9575-44cf340abc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, SAGPooling, global_add_pool\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "\n",
    "\n",
    "class GAT_gate_Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, FC_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.in_channels = in_channels # dim(node features)\n",
    "        self.hidden_channels = hidden_channels # dim(node embedding)\n",
    "        self.FC_channels = FC_channels # fully connected layers\n",
    "        self.out_channels = out_channels\n",
    "        #self.layers1 = [d_graph_layer for i in range(n_graph_layer+1)]\n",
    "        #self.GATConvList = nn.ModuleList([GAT_gate(self.layers1[i], self.layers1[i+1]) for i in range(len(self.layers1)-1)])\n",
    "        #self.depth = depth # the number of message passing\n",
    "        \n",
    "        # model structurec_hs = F.dropout(c_hs, p=self.dropout_rate, training=self.training)\n",
    "        self.embede = nn.Linear(in_channels, hidden_channels, bias = False)\n",
    "        self.conv1 = GAT_gate(in_channels= self.hidden_channels, out_channels= self.hidden_channels)\n",
    "        self.conv2 = GAT_gate(in_channels= self.hidden_channels, out_channels= self.hidden_channels)\n",
    "        self.conv3 = GAT_gate(in_channels= self.hidden_channels, out_channels= self.hidden_channels)\n",
    "        self.conv4 = GAT_gate(in_channels= self.hidden_channels, out_channels= self.hidden_channels)\n",
    "        self.conv5 = GAT_gate(in_channels= self.hidden_channels, out_channels= self.hidden_channels)\n",
    "        self.FC1 = Linear(self.hidden_channels, self.FC_channels) # readout function\n",
    "        self.FC2 = Linear(self.FC_channels, self.FC_channels)\n",
    "        self.predict = Linear(self.FC_channels, self.out_channels) # predictor\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "        adj = to_dense_adj(data.edge_index).to(device)\n",
    "        \n",
    "        x = self.embede(x)\n",
    "        \n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, adj)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv4(x, adj)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv5(x, adj)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.FC1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        x = self.FC2(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "2e1700c0-26c5-4817-a88b-682e6b27eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "-------------------------------\n",
      "GAT_gate_Net(\n",
      "  (embede): Linear(in_features=11, out_features=140, bias=False)\n",
      "  (conv1): GAT_gate(\n",
      "    (W): Linear(in_features=140, out_features=140, bias=True)\n",
      "    (gate): Linear(in_features=280, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): GAT_gate(\n",
      "    (W): Linear(in_features=140, out_features=140, bias=True)\n",
      "    (gate): Linear(in_features=280, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): GAT_gate(\n",
      "    (W): Linear(in_features=140, out_features=140, bias=True)\n",
      "    (gate): Linear(in_features=280, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): GAT_gate(\n",
      "    (W): Linear(in_features=140, out_features=140, bias=True)\n",
      "    (gate): Linear(in_features=280, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv5): GAT_gate(\n",
      "    (W): Linear(in_features=140, out_features=140, bias=True)\n",
      "    (gate): Linear(in_features=280, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (FC1): Linear(in_features=140, out_features=128, bias=True)\n",
      "  (FC2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (predict): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT_gate_Net(in_channels= 11, hidden_channels= 140, FC_channels= 128, out_channels= 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=10, min_lr=0.0025)\n",
    "print(device)\n",
    "print('-------------------------------')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "32e16ca6-7eb8-4b50-a3f9-aa4e0e25dbc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|      Modules      | Parameters |\n",
      "+-------------------+------------+\n",
      "|   embede.weight   |    1540    |\n",
      "|      conv1.A      |   19600    |\n",
      "|   conv1.W.weight  |   19600    |\n",
      "|    conv1.W.bias   |    140     |\n",
      "| conv1.gate.weight |    280     |\n",
      "|  conv1.gate.bias  |     1      |\n",
      "|      conv2.A      |   19600    |\n",
      "|   conv2.W.weight  |   19600    |\n",
      "|    conv2.W.bias   |    140     |\n",
      "| conv2.gate.weight |    280     |\n",
      "|  conv2.gate.bias  |     1      |\n",
      "|      conv3.A      |   19600    |\n",
      "|   conv3.W.weight  |   19600    |\n",
      "|    conv3.W.bias   |    140     |\n",
      "| conv3.gate.weight |    280     |\n",
      "|  conv3.gate.bias  |     1      |\n",
      "|      conv4.A      |   19600    |\n",
      "|   conv4.W.weight  |   19600    |\n",
      "|    conv4.W.bias   |    140     |\n",
      "| conv4.gate.weight |    280     |\n",
      "|  conv4.gate.bias  |     1      |\n",
      "|      conv5.A      |   19600    |\n",
      "|   conv5.W.weight  |   19600    |\n",
      "|    conv5.W.bias   |    140     |\n",
      "| conv5.gate.weight |    280     |\n",
      "|  conv5.gate.bias  |     1      |\n",
      "|     FC1.weight    |   17920    |\n",
      "|      FC1.bias     |    128     |\n",
      "|     FC2.weight    |   16384    |\n",
      "|      FC2.bias     |    128     |\n",
      "|   predict.weight  |    128     |\n",
      "|    predict.bias   |     1      |\n",
      "+-------------------+------------+\n",
      "Total Trainable Params: 234334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234334"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dc929346-2f96-4131-9c2f-350ebc294eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(model, input_size=())\n",
    "#summary(model, 'cpu')\n",
    "#summary(model, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "824eaaf1-5822-409e-b11c-a6ea8e3afb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        #batch = batch.to(device)\n",
    "        #y = torch.index_select(batch.y.to(device), 1, torch.tensor(10).to(device)).to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10))\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y.to(device)) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7754f6eb-af66-4cab-be11-04087fdc4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        #batch = data.to(device) #it trigger error!\n",
    "        out = model(batch)\n",
    "        out = out\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10)).to(device)\n",
    "        tmp = (out - y)**2\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fc2c061-30da-4fe5-80db-28dabd9ab7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847037.6311243599"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533a7699-7f9e-4007-9a6e-3a14adcd2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ed8a61e-2c28-4a3f-ba04-adedba45d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTal Epoch: 417, LR: 0.002500, Loss: 700126.3143171, Val MAE: 1199936.8878698, Test MAE: 1226311.5774669, Time: 52.01580333709717\n",
      "ToTal Epoch: 418, LR: 0.002500, Loss: 710971.8143028, Val MAE: 981658.8146450, Test MAE: 1004195.4304059, Time: 52.186503171920776\n",
      "ToTal Epoch: 419, LR: 0.002500, Loss: 703730.3679358, Val MAE: 1109378.7064129, Test MAE: 1004195.4304059, Time: 45.445844888687134\n",
      "ToTal Epoch: 420, LR: 0.002500, Loss: 784161.2004777, Val MAE: 589979.6303600, Test MAE: 607340.1378889, Time: 52.88370752334595\n",
      "ToTal Epoch: 421, LR: 0.002500, Loss: 730078.9413844, Val MAE: 732566.4902545, Test MAE: 607340.1378889, Time: 47.67762565612793\n",
      "ToTal Epoch: 422, LR: 0.002500, Loss: 706971.4072135, Val MAE: 1129909.4568524, Test MAE: 607340.1378889, Time: 49.83571004867554\n",
      "ToTal Epoch: 423, LR: 0.002500, Loss: 713487.9789041, Val MAE: 1009972.1910877, Test MAE: 607340.1378889, Time: 49.136478662490845\n",
      "ToTal Epoch: 424, LR: 0.002500, Loss: 733790.7174318, Val MAE: 819762.5706642, Test MAE: 607340.1378889, Time: 48.73959803581238\n",
      "ToTal Epoch: 425, LR: 0.002500, Loss: 715405.2968232, Val MAE: 932778.0882061, Test MAE: 607340.1378889, Time: 49.6124222278595\n",
      "ToTal Epoch: 426, LR: 0.002500, Loss: 701514.1745569, Val MAE: 1422368.3962394, Test MAE: 607340.1378889, Time: 49.60164737701416\n",
      "ToTal Epoch: 427, LR: 0.002500, Loss: 705058.1948120, Val MAE: 1340146.1365130, Test MAE: 607340.1378889, Time: 50.01947021484375\n",
      "ToTal Epoch: 428, LR: 0.002500, Loss: 694809.3423398, Val MAE: 706774.0744478, Test MAE: 607340.1378889, Time: 48.812841176986694\n",
      "ToTal Epoch: 429, LR: 0.002500, Loss: 720843.0204557, Val MAE: 926722.0276695, Test MAE: 607340.1378889, Time: 48.06435418128967\n",
      "ToTal Epoch: 430, LR: 0.002500, Loss: 722256.4012803, Val MAE: 673597.8304670, Test MAE: 607340.1378889, Time: 46.929797410964966\n",
      "ToTal Epoch: 431, LR: 0.002500, Loss: 705131.6424784, Val MAE: 682258.8568371, Test MAE: 607340.1378889, Time: 47.34221124649048\n",
      "ToTal Epoch: 432, LR: 0.001750, Loss: 691652.2284240, Val MAE: 1151546.4306352, Test MAE: 607340.1378889, Time: 50.9049506187439\n",
      "ToTal Epoch: 433, LR: 0.001750, Loss: 688063.8135002, Val MAE: 1204209.8894749, Test MAE: 607340.1378889, Time: 48.3165237903595\n",
      "ToTal Epoch: 434, LR: 0.001750, Loss: 696836.8327903, Val MAE: 898078.5630207, Test MAE: 607340.1378889, Time: 47.633496046066284\n",
      "ToTal Epoch: 435, LR: 0.001750, Loss: 704662.1598433, Val MAE: 574922.3419705, Test MAE: 597329.0945502, Time: 52.631380796432495\n",
      "ToTal Epoch: 436, LR: 0.001750, Loss: 712616.3920317, Val MAE: 1007920.5992509, Test MAE: 597329.0945502, Time: 48.83472728729248\n",
      "ToTal Epoch: 437, LR: 0.001750, Loss: 703698.7724263, Val MAE: 965244.7077887, Test MAE: 597329.0945502, Time: 49.155304193496704\n",
      "ToTal Epoch: 438, LR: 0.001750, Loss: 709129.2904409, Val MAE: 738436.9664450, Test MAE: 597329.0945502, Time: 47.726009130477905\n",
      "ToTal Epoch: 439, LR: 0.001750, Loss: 699618.2823293, Val MAE: 866173.3229382, Test MAE: 597329.0945502, Time: 47.84829640388489\n",
      "ToTal Epoch: 440, LR: 0.001750, Loss: 702268.3418526, Val MAE: 857279.2332034, Test MAE: 597329.0945502, Time: 48.229166746139526\n",
      "ToTal Epoch: 441, LR: 0.001750, Loss: 700731.8451822, Val MAE: 908236.3922648, Test MAE: 597329.0945502, Time: 48.17149806022644\n",
      "ToTal Epoch: 442, LR: 0.001750, Loss: 708290.7930636, Val MAE: 876826.2899946, Test MAE: 597329.0945502, Time: 48.803133487701416\n",
      "ToTal Epoch: 443, LR: 0.001750, Loss: 698317.5945349, Val MAE: 681025.7794084, Test MAE: 597329.0945502, Time: 49.126840114593506\n",
      "ToTal Epoch: 444, LR: 0.001750, Loss: 699770.0600583, Val MAE: 1101274.8195368, Test MAE: 597329.0945502, Time: 47.831658363342285\n",
      "ToTal Epoch: 445, LR: 0.001750, Loss: 696023.0289782, Val MAE: 640275.7532676, Test MAE: 597329.0945502, Time: 47.773338079452515\n",
      "ToTal Epoch: 446, LR: 0.001750, Loss: 727318.9975637, Val MAE: 1009436.8117404, Test MAE: 597329.0945502, Time: 47.6505172252655\n",
      "ToTal Epoch: 447, LR: 0.001225, Loss: 708155.0099842, Val MAE: 738741.5473515, Test MAE: 597329.0945502, Time: 47.91410231590271\n",
      "ToTal Epoch: 448, LR: 0.001225, Loss: 704381.9802990, Val MAE: 868115.9208133, Test MAE: 597329.0945502, Time: 47.15026307106018\n",
      "ToTal Epoch: 449, LR: 0.001225, Loss: 703470.9958057, Val MAE: 809261.8267981, Test MAE: 597329.0945502, Time: 47.16686129570007\n",
      "ToTal Epoch: 450, LR: 0.001225, Loss: 699165.0121053, Val MAE: 807987.2689750, Test MAE: 597329.0945502, Time: 49.485503911972046\n",
      "ToTal Epoch: 451, LR: 0.001225, Loss: 694293.2581474, Val MAE: 802648.1742720, Test MAE: 597329.0945502, Time: 50.04898166656494\n",
      "ToTal Epoch: 452, LR: 0.001225, Loss: 691599.7414226, Val MAE: 676291.2799817, Test MAE: 597329.0945502, Time: 49.37025856971741\n",
      "ToTal Epoch: 453, LR: 0.001225, Loss: 687829.6935174, Val MAE: 684686.6082703, Test MAE: 597329.0945502, Time: 49.46614098548889\n",
      "ToTal Epoch: 454, LR: 0.001225, Loss: 685392.9448049, Val MAE: 694420.4589162, Test MAE: 597329.0945502, Time: 49.61495757102966\n",
      "ToTal Epoch: 455, LR: 0.001225, Loss: 683809.2474084, Val MAE: 599107.2384010, Test MAE: 597329.0945502, Time: 49.13328456878662\n",
      "ToTal Epoch: 456, LR: 0.001225, Loss: 676388.0284718, Val MAE: 802194.8238172, Test MAE: 597329.0945502, Time: 48.79361939430237\n",
      "ToTal Epoch: 457, LR: 0.001225, Loss: 680773.3947356, Val MAE: 703386.3609264, Test MAE: 597329.0945502, Time: 47.640777587890625\n",
      "ToTal Epoch: 458, LR: 0.000857, Loss: 683416.7728276, Val MAE: 954347.2744783, Test MAE: 597329.0945502, Time: 47.967854261398315\n",
      "ToTal Epoch: 459, LR: 0.000857, Loss: 671853.1605408, Val MAE: 741846.7764274, Test MAE: 597329.0945502, Time: 50.36007595062256\n",
      "ToTal Epoch: 460, LR: 0.000857, Loss: 669547.8728515, Val MAE: 841339.4236796, Test MAE: 597329.0945502, Time: 49.36257219314575\n",
      "ToTal Epoch: 461, LR: 0.000857, Loss: 670036.1297473, Val MAE: 671905.4223038, Test MAE: 597329.0945502, Time: 47.4533269405365\n",
      "ToTal Epoch: 462, LR: 0.000857, Loss: 675183.6986958, Val MAE: 541836.3965451, Test MAE: 558689.8527861, Time: 51.691078901290894\n",
      "ToTal Epoch: 463, LR: 0.000857, Loss: 668619.6220704, Val MAE: 568042.8531682, Test MAE: 558689.8527861, Time: 47.71591877937317\n",
      "ToTal Epoch: 464, LR: 0.000857, Loss: 663126.0338795, Val MAE: 684702.7232286, Test MAE: 558689.8527861, Time: 47.26398706436157\n",
      "ToTal Epoch: 465, LR: 0.000857, Loss: 668360.8377203, Val MAE: 568139.5606512, Test MAE: 558689.8527861, Time: 47.89816498756409\n",
      "ToTal Epoch: 466, LR: 0.000857, Loss: 666035.1169159, Val MAE: 618604.2956508, Test MAE: 558689.8527861, Time: 47.424099922180176\n",
      "ToTal Epoch: 467, LR: 0.000857, Loss: 664689.7939904, Val MAE: 693699.4903310, Test MAE: 558689.8527861, Time: 47.4966835975647\n",
      "ToTal Epoch: 468, LR: 0.000857, Loss: 666018.1674485, Val MAE: 645964.6013911, Test MAE: 558689.8527861, Time: 47.72889280319214\n",
      "ToTal Epoch: 469, LR: 0.000857, Loss: 664167.2717336, Val MAE: 528415.8709776, Test MAE: 545922.8274861, Time: 51.29541301727295\n",
      "ToTal Epoch: 470, LR: 0.000857, Loss: 662070.3595662, Val MAE: 576091.8994115, Test MAE: 545922.8274861, Time: 47.21727681159973\n",
      "ToTal Epoch: 471, LR: 0.000857, Loss: 659485.4609277, Val MAE: 503355.0176565, Test MAE: 518778.4813881, Time: 51.319154024124146\n",
      "ToTal Epoch: 472, LR: 0.000857, Loss: 659194.4251087, Val MAE: 703425.6155316, Test MAE: 518778.4813881, Time: 47.9638249874115\n",
      "ToTal Epoch: 473, LR: 0.000857, Loss: 658697.3288874, Val MAE: 720982.3899717, Test MAE: 518778.4813881, Time: 47.2737934589386\n",
      "ToTal Epoch: 474, LR: 0.000857, Loss: 650729.6311088, Val MAE: 534668.3127723, Test MAE: 518778.4813881, Time: 48.54564118385315\n",
      "ToTal Epoch: 475, LR: 0.000857, Loss: 655025.3656523, Val MAE: 609140.4723687, Test MAE: 518778.4813881, Time: 48.29283046722412\n",
      "ToTal Epoch: 476, LR: 0.000857, Loss: 649717.3450533, Val MAE: 654833.3354735, Test MAE: 518778.4813881, Time: 47.12171816825867\n",
      "ToTal Epoch: 477, LR: 0.000857, Loss: 656090.1197535, Val MAE: 611406.8809906, Test MAE: 518778.4813881, Time: 49.12397384643555\n",
      "ToTal Epoch: 478, LR: 0.000857, Loss: 653571.0848899, Val MAE: 594366.2964152, Test MAE: 518778.4813881, Time: 47.38643503189087\n",
      "ToTal Epoch: 479, LR: 0.000857, Loss: 651817.5353365, Val MAE: 663686.7715356, Test MAE: 518778.4813881, Time: 47.490073442459106\n",
      "ToTal Epoch: 480, LR: 0.000857, Loss: 656636.7131706, Val MAE: 575226.1536345, Test MAE: 518778.4813881, Time: 48.17713260650635\n",
      "ToTal Epoch: 481, LR: 0.000857, Loss: 656631.9805092, Val MAE: 657345.3415883, Test MAE: 518778.4813881, Time: 47.97320294380188\n",
      "ToTal Epoch: 482, LR: 0.000857, Loss: 653793.2064396, Val MAE: 456424.8181610, Test MAE: 473501.2189865, Time: 51.96826887130737\n",
      "ToTal Epoch: 483, LR: 0.000857, Loss: 650497.7647160, Val MAE: 840682.4049530, Test MAE: 473501.2189865, Time: 47.983200788497925\n",
      "ToTal Epoch: 484, LR: 0.000857, Loss: 652104.2140926, Val MAE: 493970.5957349, Test MAE: 473501.2189865, Time: 48.31901931762695\n",
      "ToTal Epoch: 485, LR: 0.000857, Loss: 656926.5683084, Val MAE: 618428.6710999, Test MAE: 473501.2189865, Time: 48.01115083694458\n",
      "ToTal Epoch: 486, LR: 0.000857, Loss: 645983.5363493, Val MAE: 587924.4546358, Test MAE: 473501.2189865, Time: 46.687700033187866\n",
      "ToTal Epoch: 487, LR: 0.000857, Loss: 650704.3436488, Val MAE: 689476.7169609, Test MAE: 473501.2189865, Time: 49.45847821235657\n",
      "ToTal Epoch: 488, LR: 0.000857, Loss: 648078.0604022, Val MAE: 641444.5261790, Test MAE: 473501.2189865, Time: 46.813960790634155\n",
      "ToTal Epoch: 489, LR: 0.000857, Loss: 654037.3619453, Val MAE: 563685.4917068, Test MAE: 473501.2189865, Time: 47.84123516082764\n",
      "ToTal Epoch: 490, LR: 0.000857, Loss: 642623.8566092, Val MAE: 633692.3849270, Test MAE: 473501.2189865, Time: 47.43095850944519\n",
      "ToTal Epoch: 491, LR: 0.000857, Loss: 645099.9480629, Val MAE: 660600.3442635, Test MAE: 473501.2189865, Time: 48.35705256462097\n",
      "ToTal Epoch: 492, LR: 0.000857, Loss: 648976.5698944, Val MAE: 665825.8209891, Test MAE: 473501.2189865, Time: 48.291433334350586\n",
      "ToTal Epoch: 493, LR: 0.000857, Loss: 650406.8213061, Val MAE: 562941.1535581, Test MAE: 473501.2189865, Time: 47.49401330947876\n",
      "ToTal Epoch: 494, LR: 0.000600, Loss: 646828.5544165, Val MAE: 573763.9153099, Test MAE: 473501.2189865, Time: 48.0871365070343\n",
      "ToTal Epoch: 495, LR: 0.000600, Loss: 638827.8236278, Val MAE: 658146.6648322, Test MAE: 473501.2189865, Time: 47.22754693031311\n",
      "ToTal Epoch: 496, LR: 0.000600, Loss: 645117.6593513, Val MAE: 708673.2510892, Test MAE: 473501.2189865, Time: 48.032445430755615\n",
      "ToTal Epoch: 497, LR: 0.000600, Loss: 642011.1795156, Val MAE: 673440.9539097, Test MAE: 473501.2189865, Time: 47.456674575805664\n",
      "ToTal Epoch: 498, LR: 0.000600, Loss: 639440.0103186, Val MAE: 556340.0394405, Test MAE: 473501.2189865, Time: 48.50716805458069\n",
      "ToTal Epoch: 499, LR: 0.000600, Loss: 643874.1121865, Val MAE: 573428.1488955, Test MAE: 473501.2189865, Time: 47.58597278594971\n",
      "ToTal Epoch: 500, LR: 0.000600, Loss: 634869.4308890, Val MAE: 658770.6843996, Test MAE: 473501.2189865, Time: 47.53108620643616\n",
      "ToTal Epoch: 501, LR: 0.000600, Loss: 637827.5849233, Val MAE: 525282.9143163, Test MAE: 473501.2189865, Time: 47.327049016952515\n",
      "ToTal Epoch: 502, LR: 0.000600, Loss: 645137.2387713, Val MAE: 667536.9318964, Test MAE: 473501.2189865, Time: 47.82416820526123\n",
      "ToTal Epoch: 503, LR: 0.000600, Loss: 636784.9860030, Val MAE: 528131.6260796, Test MAE: 473501.2189865, Time: 48.09396553039551\n",
      "ToTal Epoch: 504, LR: 0.000600, Loss: 635555.6482874, Val MAE: 566645.2361079, Test MAE: 473501.2189865, Time: 50.06705689430237\n",
      "ToTal Epoch: 505, LR: 0.000420, Loss: 630106.1607987, Val MAE: 621518.8308492, Test MAE: 473501.2189865, Time: 49.31051564216614\n",
      "ToTal Epoch: 506, LR: 0.000420, Loss: 625897.5616300, Val MAE: 625911.8899335, Test MAE: 473501.2189865, Time: 48.240535259246826\n",
      "ToTal Epoch: 507, LR: 0.000420, Loss: 630568.9328811, Val MAE: 642590.5055415, Test MAE: 473501.2189865, Time: 47.03459191322327\n",
      "ToTal Epoch: 508, LR: 0.000420, Loss: 627538.6223093, Val MAE: 443284.7658794, Test MAE: 457718.2016357, Time: 52.62305784225464\n",
      "ToTal Epoch: 509, LR: 0.000420, Loss: 624484.2015191, Val MAE: 540437.5504089, Test MAE: 457718.2016357, Time: 49.74270987510681\n",
      "ToTal Epoch: 510, LR: 0.000420, Loss: 632314.1445947, Val MAE: 569992.0501414, Test MAE: 457718.2016357, Time: 48.78959012031555\n",
      "ToTal Epoch: 511, LR: 0.000420, Loss: 626448.3026800, Val MAE: 638968.8065428, Test MAE: 457718.2016357, Time: 48.645161867141724\n",
      "ToTal Epoch: 512, LR: 0.000420, Loss: 629501.5109158, Val MAE: 622175.5364977, Test MAE: 457718.2016357, Time: 48.04170799255371\n",
      "ToTal Epoch: 513, LR: 0.000420, Loss: 630502.4415803, Val MAE: 595681.1349079, Test MAE: 457718.2016357, Time: 48.22946500778198\n",
      "ToTal Epoch: 514, LR: 0.000420, Loss: 628270.4223953, Val MAE: 499240.6622334, Test MAE: 457718.2016357, Time: 47.87163186073303\n",
      "ToTal Epoch: 515, LR: 0.000420, Loss: 635157.1736875, Val MAE: 498297.0731484, Test MAE: 457718.2016357, Time: 50.07510304450989\n",
      "ToTal Epoch: 516, LR: 0.000420, Loss: 631898.2149907, Val MAE: 623503.4031950, Test MAE: 457718.2016357, Time: 47.126582860946655\n",
      "ToTal Epoch: 517, LR: 0.000420, Loss: 624183.1111451, Val MAE: 496866.8274861, Test MAE: 457718.2016357, Time: 46.93321418762207\n",
      "ToTal Epoch: 518, LR: 0.000420, Loss: 626721.5173745, Val MAE: 607988.3390660, Test MAE: 457718.2016357, Time: 47.527925968170166\n",
      "ToTal Epoch: 519, LR: 0.000420, Loss: 627076.4200449, Val MAE: 538984.0042804, Test MAE: 457718.2016357, Time: 46.95278835296631\n",
      "ToTal Epoch: 520, LR: 0.000294, Loss: 623303.1882673, Val MAE: 625712.9331193, Test MAE: 457718.2016357, Time: 47.54018211364746\n",
      "ToTal Epoch: 521, LR: 0.000294, Loss: 628046.6030860, Val MAE: 593441.5971872, Test MAE: 457718.2016357, Time: 47.33579730987549\n",
      "ToTal Epoch: 522, LR: 0.000294, Loss: 627080.4648736, Val MAE: 588220.6600933, Test MAE: 457718.2016357, Time: 48.402565479278564\n",
      "ToTal Epoch: 523, LR: 0.000294, Loss: 622825.8348827, Val MAE: 510484.1574562, Test MAE: 457718.2016357, Time: 49.992475271224976\n",
      "ToTal Epoch: 524, LR: 0.000294, Loss: 627379.9681651, Val MAE: 548812.5145609, Test MAE: 457718.2016357, Time: 47.63030934333801\n",
      "ToTal Epoch: 525, LR: 0.000294, Loss: 620840.0809440, Val MAE: 595938.4190170, Test MAE: 457718.2016357, Time: 48.29033946990967\n",
      "ToTal Epoch: 526, LR: 0.000294, Loss: 621802.3052596, Val MAE: 502816.8291676, Test MAE: 457718.2016357, Time: 48.926714181900024\n",
      "ToTal Epoch: 527, LR: 0.000294, Loss: 623921.0740553, Val MAE: 639191.0858366, Test MAE: 457718.2016357, Time: 47.651514530181885\n",
      "ToTal Epoch: 528, LR: 0.000294, Loss: 626746.9156643, Val MAE: 545546.3676527, Test MAE: 457718.2016357, Time: 47.393813371658325\n",
      "ToTal Epoch: 529, LR: 0.000294, Loss: 624256.1713658, Val MAE: 565094.8204540, Test MAE: 457718.2016357, Time: 48.68486714363098\n",
      "ToTal Epoch: 530, LR: 0.000294, Loss: 624356.3147184, Val MAE: 537519.8593595, Test MAE: 457718.2016357, Time: 51.757330894470215\n",
      "ToTal Epoch: 531, LR: 0.000206, Loss: 621421.8824631, Val MAE: 707611.1252771, Test MAE: 457718.2016357, Time: 48.454320669174194\n",
      "ToTal Epoch: 532, LR: 0.000206, Loss: 619909.0825013, Val MAE: 699359.4839104, Test MAE: 457718.2016357, Time: 49.376152753829956\n",
      "ToTal Epoch: 533, LR: 0.000206, Loss: 617980.4442746, Val MAE: 594792.9679737, Test MAE: 457718.2016357, Time: 48.5027801990509\n",
      "ToTal Epoch: 534, LR: 0.000206, Loss: 625353.1981847, Val MAE: 671669.9362532, Test MAE: 457718.2016357, Time: 46.69212079048157\n",
      "ToTal Epoch: 535, LR: 0.000206, Loss: 615750.5490470, Val MAE: 503497.8625697, Test MAE: 457718.2016357, Time: 49.39222288131714\n",
      "ToTal Epoch: 536, LR: 0.000206, Loss: 651307.5337888, Val MAE: 552482.6281434, Test MAE: 457718.2016357, Time: 48.85138463973999\n",
      "ToTal Epoch: 537, LR: 0.000206, Loss: 620559.1437061, Val MAE: 601345.1966674, Test MAE: 457718.2016357, Time: 51.973336935043335\n",
      "ToTal Epoch: 538, LR: 0.000206, Loss: 619329.8223475, Val MAE: 554716.6386914, Test MAE: 457718.2016357, Time: 52.31483793258667\n",
      "ToTal Epoch: 539, LR: 0.000206, Loss: 617893.6649310, Val MAE: 622418.3835512, Test MAE: 457718.2016357, Time: 50.292038440704346\n",
      "ToTal Epoch: 540, LR: 0.000206, Loss: 616811.0055510, Val MAE: 566498.3627608, Test MAE: 457718.2016357, Time: 47.387210845947266\n",
      "ToTal Epoch: 541, LR: 0.000206, Loss: 619881.3310276, Val MAE: 623515.3515249, Test MAE: 457718.2016357, Time: 48.52307629585266\n",
      "ToTal Epoch: 542, LR: 0.000144, Loss: 617489.3565566, Val MAE: 540688.1901705, Test MAE: 457718.2016357, Time: 58.246307611465454\n",
      "ToTal Epoch: 543, LR: 0.000144, Loss: 620008.2201309, Val MAE: 553309.2783001, Test MAE: 457718.2016357, Time: 49.32897663116455\n",
      "ToTal Epoch: 544, LR: 0.000144, Loss: 614721.8861702, Val MAE: 611554.8898571, Test MAE: 457718.2016357, Time: 49.496882915496826\n",
      "ToTal Epoch: 545, LR: 0.000144, Loss: 617851.2283571, Val MAE: 565395.2653061, Test MAE: 457718.2016357, Time: 49.810688972473145\n",
      "ToTal Epoch: 546, LR: 0.000144, Loss: 618612.1276836, Val MAE: 617859.9269281, Test MAE: 457718.2016357, Time: 48.78811812400818\n",
      "ToTal Epoch: 547, LR: 0.000144, Loss: 616794.0865046, Val MAE: 582919.5004204, Test MAE: 457718.2016357, Time: 46.76695966720581\n",
      "ToTal Epoch: 548, LR: 0.000144, Loss: 619311.3879138, Val MAE: 554174.2988611, Test MAE: 457718.2016357, Time: 46.15433955192566\n",
      "ToTal Epoch: 549, LR: 0.000144, Loss: 617438.6706540, Val MAE: 607835.8064664, Test MAE: 457718.2016357, Time: 46.390899896621704\n",
      "ToTal Epoch: 550, LR: 0.000144, Loss: 616054.7667320, Val MAE: 564177.5091340, Test MAE: 457718.2016357, Time: 46.40844416618347\n",
      "ToTal Epoch: 551, LR: 0.000144, Loss: 613546.9578560, Val MAE: 563305.6785141, Test MAE: 457718.2016357, Time: 45.016111850738525\n",
      "ToTal Epoch: 552, LR: 0.000144, Loss: 614980.1320403, Val MAE: 628048.9111060, Test MAE: 457718.2016357, Time: 47.10131335258484\n",
      "ToTal Epoch: 553, LR: 0.000101, Loss: 616731.7919839, Val MAE: 578965.7381335, Test MAE: 457718.2016357, Time: 49.1755485534668\n",
      "ToTal Epoch: 554, LR: 0.000101, Loss: 613940.5148235, Val MAE: 546857.4418711, Test MAE: 457718.2016357, Time: 48.521921157836914\n",
      "ToTal Epoch: 555, LR: 0.000101, Loss: 619177.6917212, Val MAE: 566672.1339142, Test MAE: 457718.2016357, Time: 47.03996682167053\n",
      "ToTal Epoch: 556, LR: 0.000101, Loss: 614406.9127980, Val MAE: 585226.1096079, Test MAE: 457718.2016357, Time: 48.98503088951111\n",
      "ToTal Epoch: 557, LR: 0.000101, Loss: 615903.9159987, Val MAE: 572609.9732477, Test MAE: 457718.2016357, Time: 48.99648332595825\n",
      "ToTal Epoch: 558, LR: 0.000101, Loss: 619533.4921129, Val MAE: 593520.6389972, Test MAE: 457718.2016357, Time: 46.63866972923279\n",
      "ToTal Epoch: 559, LR: 0.000101, Loss: 612558.1404290, Val MAE: 595797.6482458, Test MAE: 457718.2016357, Time: 46.076093435287476\n",
      "ToTal Epoch: 560, LR: 0.000101, Loss: 615412.9992643, Val MAE: 569013.5180005, Test MAE: 457718.2016357, Time: 47.117918729782104\n",
      "ToTal Epoch: 561, LR: 0.000101, Loss: 615487.3851622, Val MAE: 619434.3535886, Test MAE: 457718.2016357, Time: 47.527953147888184\n",
      "ToTal Epoch: 562, LR: 0.000101, Loss: 612265.7975828, Val MAE: 603807.2650004, Test MAE: 457718.2016357, Time: 47.85808873176575\n",
      "ToTal Epoch: 563, LR: 0.000101, Loss: 615013.4001624, Val MAE: 559137.7317129, Test MAE: 457718.2016357, Time: 45.91650176048279\n",
      "ToTal Epoch: 564, LR: 0.000100, Loss: 617957.1224765, Val MAE: 586338.0613009, Test MAE: 457718.2016357, Time: 48.56927967071533\n",
      "ToTal Epoch: 565, LR: 0.000100, Loss: 619842.4708164, Val MAE: 546806.1141940, Test MAE: 457718.2016357, Time: 49.54302906990051\n",
      "ToTal Epoch: 566, LR: 0.000100, Loss: 611794.9490661, Val MAE: 588916.4252847, Test MAE: 457718.2016357, Time: 47.146605014801025\n",
      "ToTal Epoch: 567, LR: 0.000100, Loss: 619212.1302059, Val MAE: 629946.5773905, Test MAE: 457718.2016357, Time: 50.389580965042114\n",
      "ToTal Epoch: 568, LR: 0.000100, Loss: 615364.1931114, Val MAE: 569921.1300161, Test MAE: 457718.2016357, Time: 50.02505683898926\n",
      "ToTal Epoch: 569, LR: 0.000100, Loss: 616453.9707448, Val MAE: 615339.8015746, Test MAE: 457718.2016357, Time: 48.027079820632935\n",
      "ToTal Epoch: 570, LR: 0.000100, Loss: 614302.8761095, Val MAE: 600505.2364137, Test MAE: 457718.2016357, Time: 49.30626058578491\n",
      "ToTal Epoch: 571, LR: 0.000100, Loss: 612764.8541919, Val MAE: 588740.2766949, Test MAE: 457718.2016357, Time: 49.65602684020996\n",
      "ToTal Epoch: 572, LR: 0.000100, Loss: 614273.4689151, Val MAE: 628240.7191011, Test MAE: 457718.2016357, Time: 46.988648414611816\n",
      "ToTal Epoch: 573, LR: 0.000100, Loss: 615516.4667463, Val MAE: 640998.9182909, Test MAE: 457718.2016357, Time: 49.024410247802734\n",
      "ToTal Epoch: 574, LR: 0.000100, Loss: 613496.9783595, Val MAE: 635136.4023542, Test MAE: 457718.2016357, Time: 46.81634783744812\n",
      "ToTal Epoch: 575, LR: 0.000100, Loss: 606962.1760092, Val MAE: 647210.9143163, Test MAE: 457718.2016357, Time: 46.433095932006836\n",
      "ToTal Epoch: 576, LR: 0.000100, Loss: 611216.2633927, Val MAE: 614895.4313231, Test MAE: 457718.2016357, Time: 48.24358534812927\n",
      "ToTal Epoch: 577, LR: 0.000100, Loss: 608660.4342617, Val MAE: 627338.6899029, Test MAE: 457718.2016357, Time: 50.4026095867157\n",
      "ToTal Epoch: 578, LR: 0.000100, Loss: 612743.1797831, Val MAE: 630611.2726439, Test MAE: 457718.2016357, Time: 49.05173945426941\n",
      "ToTal Epoch: 579, LR: 0.000100, Loss: 612812.3318397, Val MAE: 634159.6025376, Test MAE: 457718.2016357, Time: 50.35835933685303\n",
      "ToTal Epoch: 580, LR: 0.000100, Loss: 612888.7758085, Val MAE: 645920.2873959, Test MAE: 457718.2016357, Time: 49.422253370285034\n",
      "ToTal Epoch: 581, LR: 0.000100, Loss: 616359.4965270, Val MAE: 514973.4635787, Test MAE: 457718.2016357, Time: 49.18583393096924\n",
      "ToTal Epoch: 582, LR: 0.000100, Loss: 616595.5562605, Val MAE: 637619.2090499, Test MAE: 457718.2016357, Time: 47.401068449020386\n",
      "ToTal Epoch: 583, LR: 0.000100, Loss: 610469.3027086, Val MAE: 556355.0873653, Test MAE: 457718.2016357, Time: 51.82263135910034\n",
      "ToTal Epoch: 584, LR: 0.000100, Loss: 614256.4048727, Val MAE: 544530.9919743, Test MAE: 457718.2016357, Time: 49.84004855155945\n",
      "ToTal Epoch: 585, LR: 0.000100, Loss: 607070.6801318, Val MAE: 632451.3472445, Test MAE: 457718.2016357, Time: 47.86766839027405\n",
      "ToTal Epoch: 586, LR: 0.000100, Loss: 610188.3782353, Val MAE: 626422.1013529, Test MAE: 457718.2016357, Time: 47.99689817428589\n",
      "ToTal Epoch: 587, LR: 0.000100, Loss: 616470.3563560, Val MAE: 562144.7239930, Test MAE: 457718.2016357, Time: 48.02379322052002\n",
      "ToTal Epoch: 588, LR: 0.000100, Loss: 620078.0095734, Val MAE: 646216.1901705, Test MAE: 457718.2016357, Time: 48.250327348709106\n",
      "ToTal Epoch: 589, LR: 0.000100, Loss: 610431.3552764, Val MAE: 570861.8763281, Test MAE: 457718.2016357, Time: 47.95596694946289\n",
      "ToTal Epoch: 590, LR: 0.000100, Loss: 616715.7527731, Val MAE: 552996.8099060, Test MAE: 457718.2016357, Time: 46.49062657356262\n",
      "ToTal Epoch: 591, LR: 0.000100, Loss: 615458.6632781, Val MAE: 531290.1817626, Test MAE: 457718.2016357, Time: 47.33184337615967\n",
      "ToTal Epoch: 592, LR: 0.000100, Loss: 611068.6773229, Val MAE: 542516.9798976, Test MAE: 457718.2016357, Time: 45.81119513511658\n",
      "ToTal Epoch: 593, LR: 0.000100, Loss: 613809.7927674, Val MAE: 531023.8251166, Test MAE: 457718.2016357, Time: 48.67465543746948\n",
      "ToTal Epoch: 594, LR: 0.000100, Loss: 613768.4883390, Val MAE: 557308.7450890, Test MAE: 457718.2016357, Time: 47.94009590148926\n",
      "ToTal Epoch: 595, LR: 0.000100, Loss: 612796.3451393, Val MAE: 600217.0829320, Test MAE: 457718.2016357, Time: 48.47732090950012\n",
      "ToTal Epoch: 596, LR: 0.000100, Loss: 610286.0063632, Val MAE: 574362.0001529, Test MAE: 457718.2016357, Time: 48.15255379676819\n",
      "ToTal Epoch: 597, LR: 0.000100, Loss: 612630.4689438, Val MAE: 606318.9617060, Test MAE: 457718.2016357, Time: 47.87902331352234\n",
      "ToTal Epoch: 598, LR: 0.000100, Loss: 609734.7205656, Val MAE: 565631.7853703, Test MAE: 457718.2016357, Time: 47.75761532783508\n",
      "ToTal Epoch: 599, LR: 0.000100, Loss: 613143.9986242, Val MAE: 563279.7804785, Test MAE: 457718.2016357, Time: 47.83175730705261\n",
      "ToTal Epoch: 600, LR: 0.000100, Loss: 612399.2653896, Val MAE: 596395.1442330, Test MAE: 457718.2016357, Time: 47.63704466819763\n",
      "ToTal Epoch: 601, LR: 0.000100, Loss: 608923.2774280, Val MAE: 547212.1959795, Test MAE: 457718.2016357, Time: 48.279892683029175\n",
      "ToTal Epoch: 602, LR: 0.000100, Loss: 612598.0574213, Val MAE: 579251.8706719, Test MAE: 457718.2016357, Time: 48.31503462791443\n",
      "ToTal Epoch: 603, LR: 0.000100, Loss: 607254.2095256, Val MAE: 586562.8360468, Test MAE: 457718.2016357, Time: 47.579131841659546\n",
      "ToTal Epoch: 604, LR: 0.000100, Loss: 609640.9159891, Val MAE: 557858.3756019, Test MAE: 457718.2016357, Time: 47.34457850456238\n",
      "ToTal Epoch: 605, LR: 0.000100, Loss: 617172.5632064, Val MAE: 581101.5815944, Test MAE: 457718.2016357, Time: 47.55017805099487\n",
      "ToTal Epoch: 606, LR: 0.000100, Loss: 610811.9717575, Val MAE: 623259.0494535, Test MAE: 457718.2016357, Time: 47.347599267959595\n",
      "ToTal Epoch: 607, LR: 0.000100, Loss: 610645.4208761, Val MAE: 570642.3315753, Test MAE: 457718.2016357, Time: 48.10142779350281\n",
      "ToTal Epoch: 608, LR: 0.000100, Loss: 614959.9811207, Val MAE: 560974.8498051, Test MAE: 457718.2016357, Time: 47.653961181640625\n",
      "ToTal Epoch: 609, LR: 0.000100, Loss: 609253.4083409, Val MAE: 616784.2176871, Test MAE: 457718.2016357, Time: 47.43328094482422\n",
      "ToTal Epoch: 610, LR: 0.000100, Loss: 611276.8710075, Val MAE: 607949.8463655, Test MAE: 457718.2016357, Time: 47.56936287879944\n",
      "ToTal Epoch: 611, LR: 0.000100, Loss: 609041.6726126, Val MAE: 544674.8018039, Test MAE: 457718.2016357, Time: 47.84999442100525\n",
      "ToTal Epoch: 612, LR: 0.000100, Loss: 614377.5571968, Val MAE: 532539.0567913, Test MAE: 457718.2016357, Time: 47.184301137924194\n",
      "ToTal Epoch: 613, LR: 0.000100, Loss: 614793.8889982, Val MAE: 520526.5746388, Test MAE: 457718.2016357, Time: 51.81983828544617\n",
      "ToTal Epoch: 614, LR: 0.000100, Loss: 615193.2758420, Val MAE: 534180.7689368, Test MAE: 457718.2016357, Time: 54.489216566085815\n",
      "ToTal Epoch: 615, LR: 0.000100, Loss: 609538.1892323, Val MAE: 550025.9579607, Test MAE: 457718.2016357, Time: 49.63497257232666\n",
      "ToTal Epoch: 616, LR: 0.000100, Loss: 609716.5317155, Val MAE: 532786.3016128, Test MAE: 457718.2016357, Time: 47.97516345977783\n",
      "ToTal Epoch: 617, LR: 0.000100, Loss: 608212.3619548, Val MAE: 593923.3496904, Test MAE: 457718.2016357, Time: 49.69619870185852\n",
      "ToTal Epoch: 618, LR: 0.000100, Loss: 608213.8826924, Val MAE: 591866.4373615, Test MAE: 457718.2016357, Time: 49.801419734954834\n",
      "ToTal Epoch: 619, LR: 0.000100, Loss: 608937.2234845, Val MAE: 593679.6257739, Test MAE: 457718.2016357, Time: 49.68202829360962\n",
      "ToTal Epoch: 620, LR: 0.000100, Loss: 609861.6468925, Val MAE: 582900.7878927, Test MAE: 457718.2016357, Time: 47.15534257888794\n",
      "ToTal Epoch: 621, LR: 0.000100, Loss: 614839.4302584, Val MAE: 533699.6835588, Test MAE: 457718.2016357, Time: 49.17693257331848\n",
      "ToTal Epoch: 622, LR: 0.000100, Loss: 611364.0418478, Val MAE: 573546.4936177, Test MAE: 457718.2016357, Time: 47.49998593330383\n",
      "ToTal Epoch: 623, LR: 0.000100, Loss: 609005.5057947, Val MAE: 606966.8926087, Test MAE: 457718.2016357, Time: 47.217865228652954\n",
      "ToTal Epoch: 624, LR: 0.000100, Loss: 607013.3651555, Val MAE: 548763.5826645, Test MAE: 457718.2016357, Time: 47.161728382110596\n",
      "ToTal Epoch: 625, LR: 0.000100, Loss: 611454.8514976, Val MAE: 596383.2209738, Test MAE: 457718.2016357, Time: 47.5254921913147\n",
      "ToTal Epoch: 626, LR: 0.000100, Loss: 610345.8237997, Val MAE: 547437.1089200, Test MAE: 457718.2016357, Time: 47.596001863479614\n",
      "ToTal Epoch: 627, LR: 0.000100, Loss: 610749.9704390, Val MAE: 539190.5514026, Test MAE: 457718.2016357, Time: 47.79269313812256\n",
      "ToTal Epoch: 628, LR: 0.000100, Loss: 609922.5792767, Val MAE: 546900.9841779, Test MAE: 457718.2016357, Time: 47.22037363052368\n",
      "ToTal Epoch: 629, LR: 0.000100, Loss: 611848.1950604, Val MAE: 561875.3203394, Test MAE: 457718.2016357, Time: 48.537785053253174\n",
      "ToTal Epoch: 630, LR: 0.000100, Loss: 607837.6900779, Val MAE: 485484.6435833, Test MAE: 457718.2016357, Time: 46.615293741226196\n",
      "ToTal Epoch: 631, LR: 0.000100, Loss: 613719.4004491, Val MAE: 512683.0103187, Test MAE: 457718.2016357, Time: 49.96260619163513\n",
      "ToTal Epoch: 632, LR: 0.000100, Loss: 606325.1386041, Val MAE: 587424.4359856, Test MAE: 457718.2016357, Time: 49.41726517677307\n",
      "ToTal Epoch: 633, LR: 0.000100, Loss: 609932.1810347, Val MAE: 508885.0508293, Test MAE: 457718.2016357, Time: 49.54083323478699\n",
      "ToTal Epoch: 634, LR: 0.000100, Loss: 609278.6984761, Val MAE: 517411.5147902, Test MAE: 457718.2016357, Time: 48.4668345451355\n",
      "ToTal Epoch: 635, LR: 0.000100, Loss: 613493.0821956, Val MAE: 572952.6108691, Test MAE: 457718.2016357, Time: 49.298962116241455\n",
      "ToTal Epoch: 636, LR: 0.000100, Loss: 611695.0373095, Val MAE: 573561.3177406, Test MAE: 457718.2016357, Time: 49.23818802833557\n",
      "ToTal Epoch: 637, LR: 0.000100, Loss: 617218.0807720, Val MAE: 595406.4028128, Test MAE: 457718.2016357, Time: 49.6332151889801\n",
      "ToTal Epoch: 638, LR: 0.000100, Loss: 611858.5869202, Val MAE: 584866.3499197, Test MAE: 457718.2016357, Time: 48.4530234336853\n",
      "ToTal Epoch: 639, LR: 0.000100, Loss: 608859.2415038, Val MAE: 595568.0244592, Test MAE: 457718.2016357, Time: 48.0487699508667\n",
      "ToTal Epoch: 640, LR: 0.000100, Loss: 607638.2807624, Val MAE: 578046.5159367, Test MAE: 457718.2016357, Time: 49.526246309280396\n",
      "ToTal Epoch: 641, LR: 0.000100, Loss: 613401.7455310, Val MAE: 564171.9409921, Test MAE: 457718.2016357, Time: 50.51164674758911\n",
      "ToTal Epoch: 642, LR: 0.000100, Loss: 610828.9514929, Val MAE: 571700.5096690, Test MAE: 457718.2016357, Time: 49.53924369812012\n",
      "ToTal Epoch: 643, LR: 0.000100, Loss: 609216.3014570, Val MAE: 556312.1871130, Test MAE: 457718.2016357, Time: 47.66338515281677\n",
      "ToTal Epoch: 644, LR: 0.000100, Loss: 610536.2461950, Val MAE: 595999.7841474, Test MAE: 457718.2016357, Time: 46.87342810630798\n",
      "ToTal Epoch: 645, LR: 0.000100, Loss: 617268.2369082, Val MAE: 584244.2345028, Test MAE: 457718.2016357, Time: 47.63261795043945\n",
      "ToTal Epoch: 646, LR: 0.000100, Loss: 612656.3911909, Val MAE: 573897.8839716, Test MAE: 457718.2016357, Time: 47.13477420806885\n",
      "ToTal Epoch: 647, LR: 0.000100, Loss: 608903.7784169, Val MAE: 583036.8319193, Test MAE: 457718.2016357, Time: 48.44013786315918\n",
      "ToTal Epoch: 648, LR: 0.000100, Loss: 626697.1246549, Val MAE: 622727.7242223, Test MAE: 457718.2016357, Time: 49.332794189453125\n",
      "ToTal Epoch: 649, LR: 0.000100, Loss: 614337.9109349, Val MAE: 549595.8627226, Test MAE: 457718.2016357, Time: 49.56463646888733\n",
      "ToTal Epoch: 650, LR: 0.000100, Loss: 607216.1504228, Val MAE: 600194.5706642, Test MAE: 457718.2016357, Time: 49.21305823326111\n",
      "ToTal Epoch: 651, LR: 0.000100, Loss: 610763.1194382, Val MAE: 541975.4086983, Test MAE: 457718.2016357, Time: 49.216792821884155\n",
      "ToTal Epoch: 652, LR: 0.000100, Loss: 609822.7113171, Val MAE: 563623.7541848, Test MAE: 457718.2016357, Time: 47.98713517189026\n",
      "ToTal Epoch: 653, LR: 0.000100, Loss: 612420.9750346, Val MAE: 516875.6798899, Test MAE: 457718.2016357, Time: 50.122812271118164\n",
      "ToTal Epoch: 654, LR: 0.000100, Loss: 605809.4626475, Val MAE: 618802.8005809, Test MAE: 457718.2016357, Time: 50.10972714424133\n",
      "ToTal Epoch: 655, LR: 0.000100, Loss: 610691.0647112, Val MAE: 541283.8584423, Test MAE: 457718.2016357, Time: 46.84443926811218\n",
      "ToTal Epoch: 656, LR: 0.000100, Loss: 607731.6068600, Val MAE: 602335.0497592, Test MAE: 457718.2016357, Time: 50.00197505950928\n",
      "ToTal Epoch: 657, LR: 0.000100, Loss: 605696.2461186, Val MAE: 573359.1598257, Test MAE: 457718.2016357, Time: 49.316402435302734\n",
      "ToTal Epoch: 658, LR: 0.000100, Loss: 606358.7311136, Val MAE: 567284.6417488, Test MAE: 457718.2016357, Time: 48.86709499359131\n",
      "ToTal Epoch: 659, LR: 0.000100, Loss: 610718.4151340, Val MAE: 594670.3123137, Test MAE: 457718.2016357, Time: 48.22984480857849\n",
      "ToTal Epoch: 660, LR: 0.000100, Loss: 603787.3628816, Val MAE: 498426.8079187, Test MAE: 457718.2016357, Time: 48.836694955825806\n",
      "ToTal Epoch: 661, LR: 0.000100, Loss: 605710.0878422, Val MAE: 506745.2669877, Test MAE: 457718.2016357, Time: 48.44178557395935\n",
      "ToTal Epoch: 662, LR: 0.000100, Loss: 610929.7976592, Val MAE: 604514.9485592, Test MAE: 457718.2016357, Time: 49.331244707107544\n",
      "ToTal Epoch: 663, LR: 0.000100, Loss: 603758.1541872, Val MAE: 544988.6319651, Test MAE: 457718.2016357, Time: 47.6431884765625\n",
      "ToTal Epoch: 664, LR: 0.000100, Loss: 606499.7522381, Val MAE: 591121.9286096, Test MAE: 457718.2016357, Time: 47.540947675704956\n",
      "ToTal Epoch: 665, LR: 0.000100, Loss: 609062.1657288, Val MAE: 563308.4008255, Test MAE: 457718.2016357, Time: 47.90108942985535\n",
      "ToTal Epoch: 666, LR: 0.000100, Loss: 607605.1690250, Val MAE: 579023.5462814, Test MAE: 457718.2016357, Time: 47.12981104850769\n",
      "ToTal Epoch: 667, LR: 0.000100, Loss: 605820.6740362, Val MAE: 566656.8499580, Test MAE: 457718.2016357, Time: 47.02237606048584\n",
      "ToTal Epoch: 668, LR: 0.000100, Loss: 613092.5721492, Val MAE: 605508.6007796, Test MAE: 457718.2016357, Time: 47.73034381866455\n",
      "ToTal Epoch: 669, LR: 0.000100, Loss: 612250.4803707, Val MAE: 578552.8297791, Test MAE: 457718.2016357, Time: 47.56476616859436\n",
      "ToTal Epoch: 670, LR: 0.000100, Loss: 608722.7725792, Val MAE: 561897.3183521, Test MAE: 457718.2016357, Time: 47.60105633735657\n",
      "ToTal Epoch: 671, LR: 0.000100, Loss: 610873.8413796, Val MAE: 498143.0833907, Test MAE: 457718.2016357, Time: 47.407530784606934\n",
      "ToTal Epoch: 672, LR: 0.000100, Loss: 607381.5131324, Val MAE: 608608.0807154, Test MAE: 457718.2016357, Time: 47.62709283828735\n",
      "ToTal Epoch: 673, LR: 0.000100, Loss: 605009.5279224, Val MAE: 602567.6483987, Test MAE: 457718.2016357, Time: 47.6018762588501\n",
      "ToTal Epoch: 674, LR: 0.000100, Loss: 609842.7489610, Val MAE: 510293.7356875, Test MAE: 457718.2016357, Time: 47.35139799118042\n",
      "ToTal Epoch: 675, LR: 0.000100, Loss: 608363.2132996, Val MAE: 556994.8910800, Test MAE: 457718.2016357, Time: 47.78034710884094\n",
      "ToTal Epoch: 676, LR: 0.000100, Loss: 610635.8678832, Val MAE: 558835.4481388, Test MAE: 457718.2016357, Time: 47.28618574142456\n",
      "ToTal Epoch: 677, LR: 0.000100, Loss: 606617.9358525, Val MAE: 526529.0413514, Test MAE: 457718.2016357, Time: 47.88457155227661\n",
      "ToTal Epoch: 678, LR: 0.000100, Loss: 611579.6819949, Val MAE: 547404.1806925, Test MAE: 457718.2016357, Time: 47.6657452583313\n",
      "ToTal Epoch: 679, LR: 0.000100, Loss: 608597.4869154, Val MAE: 571973.6029963, Test MAE: 457718.2016357, Time: 47.235820055007935\n",
      "ToTal Epoch: 680, LR: 0.000100, Loss: 610068.6476664, Val MAE: 526287.5034778, Test MAE: 457718.2016357, Time: 47.55215072631836\n",
      "ToTal Epoch: 681, LR: 0.000100, Loss: 615852.7393111, Val MAE: 481665.2731025, Test MAE: 457718.2016357, Time: 47.89283013343811\n",
      "ToTal Epoch: 682, LR: 0.000100, Loss: 611043.1926623, Val MAE: 523677.5467400, Test MAE: 457718.2016357, Time: 47.50326156616211\n",
      "ToTal Epoch: 683, LR: 0.000100, Loss: 606449.3799455, Val MAE: 564471.7132156, Test MAE: 457718.2016357, Time: 47.8321270942688\n",
      "ToTal Epoch: 684, LR: 0.000100, Loss: 610280.1867291, Val MAE: 595305.0658106, Test MAE: 457718.2016357, Time: 48.246535778045654\n",
      "ToTal Epoch: 685, LR: 0.000100, Loss: 606938.2522906, Val MAE: 570934.0181915, Test MAE: 457718.2016357, Time: 47.88552498817444\n",
      "ToTal Epoch: 686, LR: 0.000100, Loss: 607567.3112502, Val MAE: 524793.5091340, Test MAE: 457718.2016357, Time: 47.6249303817749\n",
      "ToTal Epoch: 687, LR: 0.000100, Loss: 656422.5241294, Val MAE: 553349.8451426, Test MAE: 457718.2016357, Time: 47.786487102508545\n",
      "ToTal Epoch: 688, LR: 0.000100, Loss: 619801.1486552, Val MAE: 555607.7156615, Test MAE: 457718.2016357, Time: 48.68906545639038\n",
      "ToTal Epoch: 689, LR: 0.000100, Loss: 609557.1347824, Val MAE: 557700.4222273, Test MAE: 457718.2016357, Time: 49.40722846984863\n",
      "ToTal Epoch: 690, LR: 0.000100, Loss: 608564.4153824, Val MAE: 582584.4170297, Test MAE: 457718.2016357, Time: 48.293652057647705\n",
      "ToTal Epoch: 691, LR: 0.000100, Loss: 606518.4734534, Val MAE: 618242.7369869, Test MAE: 457718.2016357, Time: 48.3750364780426\n",
      "ToTal Epoch: 692, LR: 0.000100, Loss: 604735.9327378, Val MAE: 535773.1731254, Test MAE: 457718.2016357, Time: 47.40093398094177\n",
      "ToTal Epoch: 693, LR: 0.000100, Loss: 608722.6523480, Val MAE: 581191.5652373, Test MAE: 457718.2016357, Time: 48.12434720993042\n",
      "ToTal Epoch: 694, LR: 0.000100, Loss: 610134.4761286, Val MAE: 578863.9235649, Test MAE: 457718.2016357, Time: 47.17497730255127\n",
      "ToTal Epoch: 695, LR: 0.000100, Loss: 604718.7417379, Val MAE: 651124.0908049, Test MAE: 457718.2016357, Time: 47.9090416431427\n",
      "ToTal Epoch: 696, LR: 0.000100, Loss: 627574.7880571, Val MAE: 684121.8937553, Test MAE: 457718.2016357, Time: 48.525952100753784\n",
      "ToTal Epoch: 697, LR: 0.000100, Loss: 632531.2212487, Val MAE: 630223.9963311, Test MAE: 457718.2016357, Time: 47.70641565322876\n",
      "ToTal Epoch: 698, LR: 0.000100, Loss: 610030.1231548, Val MAE: 601986.8299320, Test MAE: 457718.2016357, Time: 47.37843322753906\n",
      "ToTal Epoch: 699, LR: 0.000100, Loss: 612917.6155544, Val MAE: 579204.3720859, Test MAE: 457718.2016357, Time: 47.81809711456299\n",
      "ToTal Epoch: 700, LR: 0.000100, Loss: 612896.1082310, Val MAE: 535442.0215547, Test MAE: 457718.2016357, Time: 47.78421497344971\n",
      "ToTal Epoch: 701, LR: 0.000100, Loss: 606866.5248555, Val MAE: 486596.3506841, Test MAE: 457718.2016357, Time: 48.03088116645813\n",
      "ToTal Epoch: 702, LR: 0.000100, Loss: 606120.0819376, Val MAE: 535550.3618436, Test MAE: 457718.2016357, Time: 47.432151794433594\n",
      "ToTal Epoch: 703, LR: 0.000100, Loss: 606267.9645727, Val MAE: 543309.6201177, Test MAE: 457718.2016357, Time: 47.91088914871216\n",
      "ToTal Epoch: 704, LR: 0.000100, Loss: 609808.6349400, Val MAE: 489330.8201483, Test MAE: 457718.2016357, Time: 48.51081967353821\n",
      "ToTal Epoch: 705, LR: 0.000100, Loss: 605950.5929967, Val MAE: 584659.7520446, Test MAE: 457718.2016357, Time: 47.61153697967529\n",
      "ToTal Epoch: 706, LR: 0.000100, Loss: 607037.9952802, Val MAE: 559113.9408393, Test MAE: 457718.2016357, Time: 48.10210585594177\n",
      "ToTal Epoch: 707, LR: 0.000100, Loss: 607326.4432618, Val MAE: 551459.2017121, Test MAE: 457718.2016357, Time: 47.51990795135498\n",
      "ToTal Epoch: 708, LR: 0.000100, Loss: 606285.2268093, Val MAE: 522465.2510892, Test MAE: 457718.2016357, Time: 48.22961735725403\n",
      "ToTal Epoch: 709, LR: 0.000100, Loss: 608214.0806573, Val MAE: 519432.5332110, Test MAE: 457718.2016357, Time: 48.156757831573486\n",
      "ToTal Epoch: 710, LR: 0.000100, Loss: 605622.9896145, Val MAE: 575786.0502943, Test MAE: 457718.2016357, Time: 47.579033613204956\n",
      "ToTal Epoch: 711, LR: 0.000100, Loss: 609176.0396694, Val MAE: 559659.1778644, Test MAE: 457718.2016357, Time: 47.02808976173401\n",
      "ToTal Epoch: 712, LR: 0.000100, Loss: 608748.1784360, Val MAE: 543449.5274784, Test MAE: 457718.2016357, Time: 46.86331653594971\n",
      "ToTal Epoch: 713, LR: 0.000100, Loss: 606810.5909712, Val MAE: 538971.8217534, Test MAE: 457718.2016357, Time: 49.09716081619263\n",
      "ToTal Epoch: 714, LR: 0.000100, Loss: 604341.3307600, Val MAE: 559716.4014370, Test MAE: 457718.2016357, Time: 47.673823833465576\n",
      "ToTal Epoch: 715, LR: 0.000100, Loss: 606277.3361104, Val MAE: 533795.4493618, Test MAE: 457718.2016357, Time: 47.87608075141907\n",
      "ToTal Epoch: 716, LR: 0.000100, Loss: 609785.4066976, Val MAE: 500936.6928075, Test MAE: 457718.2016357, Time: 47.65491724014282\n",
      "Done. Total Time: 14518.24613070488\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# tensorboard writer\n",
    "tb_writer = SummaryWriter('run/GAT_Net/run_09')\n",
    "    ## logdir=./python/run/GAT_Net/run_02\n",
    "\n",
    "#input parameters\n",
    "total_num_epoch = 416 #the total number of epoch that have run\n",
    "running_num_epoch = 300 #the number of epoch that run in this time\n",
    "\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    \n",
    "    tb_writer.add_scalar('loss in train', loss, total_num_epoch) #tensorboard\n",
    "    tb_writer.add_scalar('validation MAE', validation_error, total_num_epoch) #tensorboard\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "    \n",
    "    epoch_time_end = time.time() # to measure time\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "  \n",
    "    tb_writer.add_scalar('test MAE', test_error, total_num_epoch) #tensorboard\n",
    "    tb_writer.add_scalar('learning rate', lr, total_num_epoch) #tensorboard\n",
    "total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total Time: {total_time_finish - total_time_start}')\n",
    "tb_writer.add_hparams({'num_conv': 5, 'hidden_channels' : 11}, {'hparam/total_epoch' : total_num_epoch, 'hparam/total_time' : total_time_finish - total_time_start}) #tensorboard\n",
    "tb_writer.close() #tensorboard : if close() is not declared, the writer does not save any valeus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cabfe12b-a697-46e0-9c07-fbdb4c7341de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': total_num_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model/GAT_gate_epoch716_run09_20211215')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67989575-cf8d-4c4e-bc67-3c897624992c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/GAT_gate_epoch716_run09_20211215'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53843/415108051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model/GAT_gate_epoch716_run09_20211215'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/GAT_gate_epoch716_run09_20211215'"
     ]
    }
   ],
   "source": [
    "model = GAT_gate_Net(in_channels= 11, hidden_channels= 140, FC_channels= 128, out_channels= 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.7, patience=10, min_lr=0.0025)\n",
    "PATH = 'model/GAT_gate_epoch716_run09_20211215'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acaeb41-459e-420e-9ee2-490acd9ff6a8",
   "metadata": {},
   "source": [
    "## Gated graph sequence neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb54081d-c1f6-43f4-bd88-68a5c8c6b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GatedGraphConv, GraphConv\n",
    "from torch_geometric.nn import SAGPooling, global_add_pool\n",
    "\n",
    "class GGS_NN_Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.in_channels = in_channels # dim(node features)\n",
    "        self.out_channels = out_channels # \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "       # model structure\n",
    "        self.conv1 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv2 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv3 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv4 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv5 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv6 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv7 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        self.conv8 = GatedGraphConv(out_channels= self.in_channels, num_layers= self.num_layers)\n",
    "        #self.pool1 = SAGPooling(self.in_channels, min_score=0.001, GNN=GraphConv)\n",
    "        self.lin1 = Linear(self.in_channels * 2, self.in_channels)\n",
    "        self.lin2 = Linear(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "     \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.conv6(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv7(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv8(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.cat((x, data.x), dim=1) #skip connect\n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "406d1bba-7905-47a5-a6c7-2d3daff6fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU --> GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GGS_NN_Net(in_channels= 11, out_channels= 1, num_layers=20).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.9, patience=10, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f607fabb-3a1c-4d7f-8b21-4ae5a18a29a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGS_NN_Net(\n",
      "  (conv1): GatedGraphConv(11, num_layers=20)\n",
      "  (conv2): GatedGraphConv(11, num_layers=20)\n",
      "  (conv3): GatedGraphConv(11, num_layers=20)\n",
      "  (conv4): GatedGraphConv(11, num_layers=20)\n",
      "  (conv5): GatedGraphConv(11, num_layers=20)\n",
      "  (conv6): GatedGraphConv(11, num_layers=20)\n",
      "  (conv7): GatedGraphConv(11, num_layers=20)\n",
      "  (conv8): GatedGraphConv(11, num_layers=20)\n",
      "  (lin1): Linear(in_features=22, out_features=11, bias=True)\n",
      "  (lin2): Linear(in_features=11, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55da9129-5839-4b3f-a276-66f556278776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|       Modules       | Parameters |\n",
      "+---------------------+------------+\n",
      "|     conv1.weight    |    2420    |\n",
      "| conv1.rnn.weight_ih |    363     |\n",
      "| conv1.rnn.weight_hh |    363     |\n",
      "|  conv1.rnn.bias_ih  |     33     |\n",
      "|  conv1.rnn.bias_hh  |     33     |\n",
      "|     conv2.weight    |    2420    |\n",
      "| conv2.rnn.weight_ih |    363     |\n",
      "| conv2.rnn.weight_hh |    363     |\n",
      "|  conv2.rnn.bias_ih  |     33     |\n",
      "|  conv2.rnn.bias_hh  |     33     |\n",
      "|     conv3.weight    |    2420    |\n",
      "| conv3.rnn.weight_ih |    363     |\n",
      "| conv3.rnn.weight_hh |    363     |\n",
      "|  conv3.rnn.bias_ih  |     33     |\n",
      "|  conv3.rnn.bias_hh  |     33     |\n",
      "|     conv4.weight    |    2420    |\n",
      "| conv4.rnn.weight_ih |    363     |\n",
      "| conv4.rnn.weight_hh |    363     |\n",
      "|  conv4.rnn.bias_ih  |     33     |\n",
      "|  conv4.rnn.bias_hh  |     33     |\n",
      "|     conv5.weight    |    2420    |\n",
      "| conv5.rnn.weight_ih |    363     |\n",
      "| conv5.rnn.weight_hh |    363     |\n",
      "|  conv5.rnn.bias_ih  |     33     |\n",
      "|  conv5.rnn.bias_hh  |     33     |\n",
      "|     conv6.weight    |    2420    |\n",
      "| conv6.rnn.weight_ih |    363     |\n",
      "| conv6.rnn.weight_hh |    363     |\n",
      "|  conv6.rnn.bias_ih  |     33     |\n",
      "|  conv6.rnn.bias_hh  |     33     |\n",
      "|     conv7.weight    |    2420    |\n",
      "| conv7.rnn.weight_ih |    363     |\n",
      "| conv7.rnn.weight_hh |    363     |\n",
      "|  conv7.rnn.bias_ih  |     33     |\n",
      "|  conv7.rnn.bias_hh  |     33     |\n",
      "|     conv8.weight    |    2420    |\n",
      "| conv8.rnn.weight_ih |    363     |\n",
      "| conv8.rnn.weight_hh |    363     |\n",
      "|  conv8.rnn.bias_ih  |     33     |\n",
      "|  conv8.rnn.bias_hh  |     33     |\n",
      "|     lin1.weight     |    242     |\n",
      "|      lin1.bias      |     11     |\n",
      "|     lin2.weight     |     11     |\n",
      "|      lin2.bias      |     1      |\n",
      "+---------------------+------------+\n",
      "Total Trainable Params: 25961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25961"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d608ff5f-aee2-47f2-9c40-ca32c16ca091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10))\n",
    "        optimizer.zero_grad() #initialization\n",
    "        loss = F.mse_loss(model(batch), y) # (predicted value, true value)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()  # to update the parameters\n",
    "        #if idx%500 == 0:\n",
    "        #    print(f\"IDX: {idx:5d}\\tLoss: {loss:.4f}\")\n",
    "            \n",
    "    return loss_all / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5467ea57-74e9-422a-a407-c940721fce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0.0\n",
    "    out_all = []\n",
    "    true = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        #batch = data.to(device) #it trigger error!\n",
    "        out = model(batch)\n",
    "        y = torch.index_select(batch.y, 1, torch.tensor(10))\n",
    "        tmp = (out - y)**2\n",
    "        error += tmp.sum().item()\n",
    "        \n",
    "        out_all.extend([x.item() for x in out])\n",
    "        true.extend([x.item() for x in y])\n",
    "        \n",
    "        #error += (model(batch) * std - y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "910fc496-8aa2-4679-9555-63ff7fa68bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125720592.65183826"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da81aa-06c0-436b-9f7d-adb6dbe94f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters\n",
    "total_num_epoch = 0 #the total number of epoch that have run\n",
    "running_num_epoch = 300\n",
    "\n",
    "#running code\n",
    "import time\n",
    "total_time_start = time.time() # to measure time\n",
    "best_validation_error = None\n",
    "for epoch in range(1, running_num_epoch+1):\n",
    "    epoch_time_start = time.time() # to measure time\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    validation_error = test(validation_loader)\n",
    "    scheduler.step(validation_error)\n",
    "\n",
    "    if best_validation_error is None or validation_error <= best_validation_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_validation_error = validation_error\n",
    "    \n",
    "    total_num_epoch = total_num_epoch + 1\n",
    "    epoch_time_end = time.time() # to measure time\n",
    "    print(f'ToTal Epoch: {total_num_epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {validation_error:.7f}, Test MAE: {test_error:.7f}, Time: {epoch_time_end - epoch_time_start}')\n",
    "\n",
    "total_time_finish = time.time() # to measure time\n",
    "print(f'Done. Total Time: {total_time_finish - total_time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f62f0-df20-432d-9cbd-8c4e0ebefd6e",
   "metadata": {},
   "source": [
    "## Directed_MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ce35a8d-3e18-444d-bc27-fbb9ac87b021",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    neighboor = torch.index_select(edge_attr, 0, boolian)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "\n",
    "\n",
    "class Directed_MPNN(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, atom_fdim: int, bond_fdim: int):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.atom_fdim = atom_fdim\n",
    "        self.bond_fdim = bond_fdim\n",
    "        self.dropout = 0.1\n",
    "        self.adjacency_amtrix = to_dense_adj(data.edge_index)\n",
    "        \n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.Wi = torch.nn.Linear(self.in_channels, self.hidden_channels) #\n",
    "        w_h_input_size = self.hidden_channels + self.bond_fdim #\n",
    "        self.Wh = torch.nn.Linear(w_h_input_size, self.hidden_channels) # [E, E_attr + N_attr]\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        \n",
    "    def forward(self, node_attr, edge_index, edge_attr):\n",
    "        # x (node and its feature) has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        # edge_attr has shape [E, the number of edge attributes]\n",
    "        \n",
    "        # to convert a node representation into an edge representation\n",
    "        #knn_graph(x, k=1, batch=None, loop=False, flow=\"source_to_target\")\n",
    "        source = torch.index_select(edge_index, 0, torch.tensor([0])) # select source node per each edge\n",
    "        node_feature_for_concat = torch.index_select(node_attr, 0, source.squeeze()) # select the node feature to concat with edge_attr\n",
    "        concat = torch.cat((node_feature_for_concat, edge_attr), dim=1) # [E, E_attr + N_attr]\n",
    "        msg_input = ReLU(self.Wi(concat)))  \n",
    "            #self.Wi : (E_attr + N_attr, hidden_channels)\n",
    "        h_vw = msg_input # [E, hidden_channels]\n",
    "        \n",
    "        \n",
    "        # edge-based message\n",
    "        for depth in range(self.depth-1) :\n",
    "            #message = h_vw.sum(dim=1) # [hidden_channels]\n",
    "    \n",
    "            # select the neigborhood edge\n",
    "            for iteration in range(node_attr.size(dim=0)) :\n",
    "                boolian=torch.isin(torch.index_select(node_attr, 1, source.squeeze(), torch.tensor[iteration])\n",
    "                                   boolian --\n",
    "                neighboor = torch.index_select(edge_attr, 0, boolian)                   \n",
    "                msg_neighboor = torch.where(x == iteration, , 0, dtype=torch.int)\n",
    "            msg_update = ReLU(msg_input + self.Wh(message)) # [, hidden_channels]\n",
    "            msg_update = self.dropout_layer(mes_update)\n",
    "\n",
    "            # msg_update --> message for memory\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ###########\n",
    "#         # Step 1: Add self-loops to the adjacency matrix.\n",
    "#         edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "\n",
    "#         # Step 2: Linearly transform node feature matrix.\n",
    "#         x = self.lin(x)\n",
    "\n",
    "#         # Step 3: Compute normalization.\n",
    "#         row, col = edge_index\n",
    "#         deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "#         deg_inv_sqrt = deg.pow(-0.5)\n",
    "#         deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "#         norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "#         # Step 4-5: Start propagating messages.\n",
    "#         return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6fa5b61-2af7-4b1a-a1ee-5f39879eb382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Directed_MPNN(\n",
       "  (Wi): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (Wh): Linear(in_features=24, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Directed_MPNN(10, 20, 30,14 ,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2980862-eeac-4c26-aa0b-f1c34f0d5993",
   "metadata": {},
   "source": [
    "Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0baac41f-df2d-4950-bf0b-a9c2e50abc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, depth):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.in_channels = in_channels # dim(node features)\n",
    "        self.hidden_channels = hidden_channels # dim(node embedding)\n",
    "        self.depth = depth # the number of message passing\n",
    "        \n",
    "        # model structure\n",
    "        self.conv1 = GCNConv(self.in_channels, self.hidden_channels)\n",
    "        self.conv2 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv3 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv4 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv5 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.lin1 = Linear(self.hidden_channels, self.hidden_channels)\n",
    "        self.lin2 = Linear(self.hidden_channels, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "#         x = self.conv4(x, edge_index)\n",
    "#         x = F.elu(x)\n",
    "        \n",
    "#         x = self.conv5(x, edge_index)\n",
    "#         x = F.elu(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch) # [batch_size, hidden_channels]\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "            #[32, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18859e5d-8f0b-4128-abba-827dbd9e2eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Directed_MPNN(\n",
       "  (conv1): GCNConv(10, 20)\n",
       "  (conv2): GCNConv(20, 20)\n",
       "  (conv3): GCNConv(20, 20)\n",
       "  (conv4): GCNConv(20, 20)\n",
       "  (conv5): GCNConv(20, 20)\n",
       "  (lin1): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (lin2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Directed_MPNN(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede2697-0513-456b-bc63-37ee4dfe7214",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ref\n",
    "- https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_nn_conv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5ca01-42a4-4a23-a059-b3cd30504643",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mutag_gin.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
